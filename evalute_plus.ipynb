{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,re,os,datetime,time,string\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import *\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import traceback,random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evalplus.sanitize,evalplus.syncheck\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_API = \"sk-c7cssl4bkglsrwf2\"\n",
    "INFINI_API_2 = \"sk-c7erk6qaqhkz5t72\"\n",
    "INFINI_API_List = [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = {\n",
    "    'llama-3-70b-instruct':\"Llama3系列是由Meta开发的Llama系列全新的第三代版本，包含一系列预训练和指令调优的文本生成式模型。Llama3基于优化后的Transformer架构，预训练过程中使用了超过15T tokens的数据，调优后的模型使用SFT和RLHF，以更好地贴合人类对可用性和安全性的偏好。Llama3-70b-Instruct是此系列里，700亿参数的指令调优的模型，针对对话场景用例进行了优化，并在常见的行业基准测试中超越了许多可用的开源聊天模型。Llama3-70b-Instruct支持模型上下文至8k tokens，该模型的数据的知识截止日期为2023年12月。\",\n",
    "    'llama-3-8b-instruct':\"Llama3系列是由Meta开发的Llama系列全新的第三代版本，包含一系列预训练和指令调优的文本生成式模型。Llama3基于优化后的Transformer架构，预训练过程中使用了超过15T tokens的数据，调优后的模型使用SFT和RLHF，以更好地贴合人类对可用性和安全性的偏好。Llama3-8b-Instruct是此系列里，80亿参数的指令调优的模型，针对对话场景用例进行了优化，并在常见的行业基准测试中超越了许多可用的开源聊天模型。Llama3-8b-Instruct支持模型上下文至8k tokens，该模型的数据的知识截止日期为2023年3月。\",\n",
    "    'chatglm3':\"ChatGLM3是智谱AI与清华KEG实验室发布的闭源模型，支持 8K 上下文，经过海量中英标识符的预训练与人类偏好对齐训练，相比一代模型在 MMLU、C-Eval、GSM8K 分别取得了16%、36%、280%的提升，并登顶中文任务榜单C-Eval。适用于对知识量、推理能力、创造力要求较高的场景，比如广告文案、小说写作、知识类写作、代码生成等。\",\n",
    "    'chatglm2-6b':\"ChatGLM2-6b 是由智谱开发的 ChatGLM 系列的第二代版本，支持中英双语的60亿参数规模的开源模型。在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，在 MMLU、C-Eval、GSM8K、BBH等主流学术数据集上，都得到了显著的性能提升，并通过基于 FlashAttention 技术，将对话模型的上下文长度（Context Length）提升至 8k tokens，允许更多轮次的对话。\",\n",
    "    'chatglm2-6b-32k':\"ChatGLM2-6b 是由智谱开发的 ChatGLM 系列的第二代版本，支持中英双语的60亿参数规模的开源模型。相较于ChatGLM2-6B，ChatGLM2-6b-32k支持更长的模型上下文至32k tokens。\",\n",
    "    'infini-megrez-7b':\"由无问芯穹公司自主研发的70亿参数大语言模型。在逻辑推理、对话能力等方面有优秀的性能表现。配合无问芯穹自研高效推理引擎，同时支持Nvidia和AMD的GPU，具备更快的推理速度，在性能表现方面更上一层楼。\",\n",
    "    'llama-2-7b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-7b-chat是其中70亿的主流参数大小的模型，适用于chat场景，更擅长英文相关的内容。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-13b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-7b-chat是其中70亿的主流参数大小的模型，适用于chat场景，更擅长英文相关的内容。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-70b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-70b-chat是其中700亿参数的大模型，适用于chat场景，更擅长英文相关的内容，相较该系列里其他规模的的模型，有更强的综合能力。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-70b':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-70b-base是其中700亿参数的基础大模型，适用于通用语言任务场景，更擅长英文相关的内容，相较该系列里其他规模的的模型，有更强的综合能力。模型支持 4k tokens上下文。\",\n",
    "    'baichuan2-7b-chat':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-7b-chat是130亿参数规模用于对话的模型，在C-Eval、MMLU、CMMLU等主流评测数据集上都有不俗的表现。该基模型支持4k tokens上下文。\",\n",
    "    'baichuan2-13b-chat':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-13b-chat是130亿参数规模用于对话的模型，在C-Eval、MMLU、CMMLU等主流评测数据集上都有不俗的表现。该基模型支持8k tokens上下文。\",\n",
    "    'baichuan2-13b-base':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-13b-base是130亿参数规模的基础模型，适用于通用对话和文本续写，较chat模型更适合于复杂场景的微调后使用。该基模型支持4k tokens上下文。\",\n",
    "    'chatglm3-6b':\"ChatGLM3-6b 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源模型。ChatGLM3采用了全新设计的 Prompt 格式，并原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。模型支持 8k tokens上下文。\",\n",
    "    'chatglm3-6b-32k':\"ChatGLM3-6b 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源模型。相较于ChatGLM之前系列的模型，ChatGLM3采用了更多样的训练数据，并原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。ChatGLM3-6b-32k在ChatGLM3-6b 基础上进一步强化了对于长文本的理解能力，能够更好的处理最多32k tokens长度的上下文。\",\n",
    "    'chatglm3-6b-base':\"ChatGLM3-6b-base 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源的基础模型。ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。基础模型更适合于复杂场景的微调后使用，该基模型支持32k tokens上下文。\",\n",
    "    'qwen-7b-chat':\"通义千问-7B-chat（Qwen-7B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的70亿参数规模的大语言模型。相较于Qwen-7B-Base模型，Qwen-7B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 8k tokens上下文。\",\n",
    "    'qwen-14b-chat':\"通义千问-14B-chat（Qwen-14B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的140亿参数规模的大语言模型。相较于Qwen-14B-Base模型，Qwen-14B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 8k tokens上下文。\",\n",
    "    'qwen-72b-chat':\"通义千问-72B-chat（Qwen-72B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的720亿参数规模的大语言模型。相较于Qwen-72B-Base模型，Qwen-72B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 32k tokens上下文。\",\n",
    "    'qwen-72b':\"通义千问-72B（Qwen-72B）是阿里云研发的通义千问大模型系列的720亿参数规模的模型。Qwen-72B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。模型支持 32k tokens上下文。\",\n",
    "    'qwen1.5-7b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-7b-chat是其中专用于chat场景的70亿参数的主流大小模型。\",\n",
    "    'qwen1.5-14b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-14b-chat是其中专用于chat场景的140亿参数的主流大小模型。\",\n",
    "    'qwen1.5-72b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-72b-chat是其中专用于chat场景的720亿参数的大模型。\",\n",
    "    'qwen1.5-72b':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-72b-base是其中的720亿参数的基础大模型，适合多种场景的使用。\",\n",
    "}\n",
    "answerModelList = [    \n",
    "    'llama-3-8b-instruct',\n",
    "    # 'chatglm3',\n",
    "    # 'chatglm2-6b',\n",
    "    # 'chatglm2-6b-32k',\n",
    "    'infini-megrez-7b',\n",
    "    'llama-2-7b-chat',\n",
    "    'llama-2-13b-chat',\n",
    "    'llama-2-70b-chat',\n",
    "    'llama-2-70b',\n",
    "    'baichuan2-7b-chat',\n",
    "    'baichuan2-13b-chat',\n",
    "    'baichuan2-13b-base',\n",
    "    # 'chatglm3-6b',\n",
    "    # 'chatglm3-6b-32k',\n",
    "    # 'chatglm3-6b-base',\n",
    "    'qwen-7b-chat',\n",
    "    'qwen-14b-chat',\n",
    "    'qwen-72b-chat',\n",
    "    'qwen-72b',\n",
    "    'qwen1.5-7b-chat',\n",
    "    'qwen1.5-14b-chat',\n",
    "    'qwen1.5-72b',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLMCompletions(prompt,modelName:str = \"infini-megrez-7b\",INFINI_API = \"sk-c7cssl4bkglsrwf2\",returnContent:bool = True,**kwargs):\n",
    "    url = \"https://cloud.infini-ai.com/maas/\"+modelName+\"/nvidia/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"string\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else 0.7,\n",
    "        \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else 1,\n",
    "        \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else -1,\n",
    "        \"n\": kwargs['n'] if 'n' in kwargs else 1,\n",
    "        \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else None,\n",
    "        \"stop\": kwargs['stop'] if 'stop' in kwargs else None,\n",
    "        \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else 0,\n",
    "        \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else 0\n",
    "    }\n",
    "    headers = {\n",
    "            'Content-Type': \"application/json\",\n",
    "            'Accept': \"*/*\",\n",
    "            'Authorization': \"Bearer \"+INFINI_API,\n",
    "    } \n",
    "    response = requests.post(url, json=payload, headers=headers,)\n",
    "    if response.status_code == 200:\n",
    "        response.encoding = 'utf-8'\n",
    "        data = response.json()\n",
    "        content = data['choices'][0]['message']['content']\n",
    "        \n",
    "        content = content.replace(',\\n}','\\n}')\n",
    "        if returnContent:\n",
    "            return content\n",
    "        try:\n",
    "            content = json.loads(content)\n",
    "        except:\n",
    "            content = content.replace('\\n','')\n",
    "        data['choices'][0]['message']['content'] = content\n",
    "        \n",
    "        return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return \"Cannot connect to the model \"+modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_ins = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'time':-1\n",
    "}\n",
    "def joinErrorToData(errorFile,saveFile):\n",
    "    with open(errorFile, 'r') as ef:\n",
    "        data_ef = json.load(ef)\n",
    "    fileName = data_ef['fileName']\n",
    "    with open(saveFile) as sf:\n",
    "        data_sf = json.load(sf)\n",
    "    if fileName != data_sf['fileName']:\n",
    "        print('FileName not match')\n",
    "        return \n",
    "    if not data_ef['data']:\n",
    "        print('The Errors of this ErrorFile all have been solved!')\n",
    "        return \n",
    "    errorItem = []\n",
    "    for item in data_ef['data']:\n",
    "        data_format_ins['id'] = item['id']\n",
    "        model = item['AnswerModel']\n",
    "        data_format_ins['AnswerModel'] = model\n",
    "        prompt = item['input']\n",
    "        data_format_ins['input'] = prompt\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        start = time.perf_counter_ns()\n",
    "        actual_output =  LLMCompletions(prompt,modelName=model,INFINI_API=INFINI_API_2)\n",
    "        end = time.perf_counter_ns()\n",
    "        delta = end-start\n",
    "        idx = 0\n",
    "        while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,INFINI_API=INFINI_API_2)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx += 1\n",
    "        if actual_output == \"Cannot connect to the model \"+model:\n",
    "            errorItem.append(item)\n",
    "            print(\"[error]:\\t\"+str(errorItem[-1]))\n",
    "            continue\n",
    "        print(actual_output)\n",
    "        data_format_ins['time'] = delta\n",
    "        data_format_ins['actual_output'] = actual_output\n",
    "        data_sf['data'].append(data_format_ins.copy())\n",
    "    data_ef['data'] = errorItem\n",
    "    with open(saveFile,'w') as saveF:\n",
    "        json.dump(data_sf,saveF)\n",
    "    with open(errorFile,'w') as error:\n",
    "        json.dump(data_ef,error)\n",
    "    if errorItem:\n",
    "        print(\"There are still some errors! \")\n",
    "    else:\n",
    "        print('The Errors of this ErrorFile all have been solved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDeprecatedModel(filePath):\n",
    "    with open(filePath) as f:\n",
    "        data = json.load(f)\n",
    "    new_data = []\n",
    "    for item in data['data']:\n",
    "        if item['AnswerModel']  in answerModelList:\n",
    "            new_data.append(item)\n",
    "    data['data'] = new_data[:]\n",
    "    with open(filePath,'w') as f:\n",
    "        json.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeDeprecatedModel('./data/1-2_1_low_freq_ent_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joinErrorToData('./data/4-2_r_with_triples_sampleError.json','./data/4-2_r_with_triples_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qapairs = []\n",
    "# model_input_id = []\n",
    "# with open('./data/1-1_2_high_freq_ent_sample.json') as saveF:\n",
    "#     data = json.load(saveF)\n",
    "# with open('E:/Repository/KoLA/Sample_Data/1-1_2_high_freq_ent_sample.json') as file:\n",
    "#     data_o = json.load(file)\n",
    "# instructions = data_o['adapter_spec']['instructions']\n",
    "# for item in data_o['request_states']:\n",
    "#     qapairs.append((item['instance']['id'],instructions+'\\n'+item['instance']['input']['text']))\n",
    "# for item in data['data']:\n",
    "#     model_input_id.append([item['AnswerModel'],item['input'],item['id']])\n",
    "# check = [ False for i in range(len(model_input_id))]\n",
    "# for model in answerModelList:\n",
    "#     for id,text in qapairs:\n",
    "#         for i,item in enumerate(model_input_id):\n",
    "#             if model == item[0] and text == item[1] and id == item[2]:\n",
    "#                 check[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model,text,id in model_input_id:\n",
    "#     diff[model] += 1\n",
    "#     diff[text] += 1\n",
    "#     diff[id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### deprecated ChatLLM,CustomLLM\n",
    "# class ChatLLM(LLM):\n",
    "#     modelName = \"qwen1.5-72b-chat\"\n",
    "#     INFINI_API = \"sk-c7cssl4bkglsrwf2\"\n",
    "#     temperature = 0.8\n",
    "#     top_p = 1\n",
    "#     top_k = -1\n",
    "#     n = 1\n",
    "#     max_tokens:int = None\n",
    "#     stop:Optional[List[str]] = None\n",
    "#     presence_penalty = 0\n",
    "#     frequency_penalty = 0\n",
    "    \n",
    "#     headers = {\n",
    "#             'Content-Type': \"application/json\",\n",
    "#             'Accept': \"*/*\",\n",
    "#             'Authorization': \"Bearer \"+INFINI_API,\n",
    "#     }   \n",
    "#     @property\n",
    "#     def _llm_type(self)->str:\n",
    "#         return \"ChatLLM\"\n",
    "#     @property\n",
    "#     def _identifying_params(self)->Mapping[str,Any]:\n",
    "#         _param_dict = {\n",
    "#             \"modelName\":ChatLLM().modelName,\n",
    "#             \"INFINI_API\":ChatLLM().INFINI_API,\n",
    "#             \"stream\":bool(ChatLLM().stream),\n",
    "#             \"temperature\":ChatLLM().temperature,\n",
    "#             \"top_p\":ChatLLM().top_p,\n",
    "#             \"top_k\":ChatLLM().top_k,\n",
    "#             \"n\":ChatLLM().n,\n",
    "#             \"max_tokens\":ChatLLM().max_tokens,\n",
    "#             \"stop\":ChatLLM().stop,\n",
    "#             \"presence_penalty\":ChatLLM().presence_penalty,\n",
    "#             \"frequency_penalty\":ChatLLM().frequency_penalty,\n",
    "#         }\n",
    "#         return _param_dict\n",
    "#     @classmethod  \n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]]= None,  **kwargs: Any) -> str:\n",
    "#         url = \"https://cloud.infini-ai.com/maas/\"+ChatLLM().modelName+\"/nvidia/chat/completions\"\n",
    "#         payload = {\n",
    "#             \"model\": \"string\",\n",
    "#             \"messages\": [\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": prompt\n",
    "#                 }\n",
    "#             ],\n",
    "#             \"stream\": False,\n",
    "#             \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else ChatLLM().temperature,\n",
    "#             \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else ChatLLM().top_p,\n",
    "#             \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else ChatLLM().top_k,\n",
    "#             \"n\": kwargs['n'] if 'n' in kwargs else ChatLLM().n,\n",
    "#             \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else ChatLLM().max_tokens,\n",
    "#             \"stop\": stop if stop else ChatLLM().stop,\n",
    "#             \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else ChatLLM().presence_penalty,\n",
    "#             \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else ChatLLM().frequency_penalty\n",
    "#         }\n",
    "#         response = requests.post(url, json=payload, headers=ChatLLM().headers)\n",
    "#         if response.status_code == 200:\n",
    "#             response.encoding = 'utf-8'\n",
    "#             data = response.json()\n",
    "#             content = data['choices'][0]['message']['content']\n",
    "#             if isinstance(content,str):    \n",
    "#                 content = content.replace(',\\n}','\\n}')\n",
    "#                 content = content.replace(']\\n}',']}')\n",
    "#                 content = content.replace('\\\\','\\\\\\\\')\n",
    "#                 flag = False\n",
    "#                 if 'statements' in content:\n",
    "#                     regex = re.compile('\\\"statements\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "#                     flag = True\n",
    "#                 elif 'verdicts' in content:\n",
    "#                     regex = re.compile('\\\"verdicts\\\":\\s+\\[.*\\]\\}', re.DOTALL) \n",
    "#                     flag = True\n",
    "#                 if flag:\n",
    "#                     matchStr =regex.search(content)\n",
    "#                     if matchStr:\n",
    "#                         content = '{'+matchStr.group()\n",
    "#             try:\n",
    "#                 content = json.loads(content)\n",
    "#             except:\n",
    "#                 pass\n",
    "#             data['choices'][0]['message']['content'] = content\n",
    "                \n",
    "#             return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "#         else:\n",
    "#             return \"Cannot connect to the model \"+ChatLLM().modelName\n",
    "#     def setParameter(self,**kwargs):\n",
    "#         self.temperature = kwargs[\"temperature\"] if \"temperature\" in kwargs else ChatLLM().temperature\n",
    "#         self.top_p = kwargs['top_p'] if 'top_p' in  kwargs else ChatLLM().top_p\n",
    "#         self.top_k = kwargs['top_k'] if 'top_k' in  kwargs else ChatLLM().top_k\n",
    "#         self.n = kwargs['n'] if 'n' in kwargs else ChatLLM().n\n",
    "#         self.max_tokens = kwargs['max_tokens'] if 'max_tokens' in kwargs else ChatLLM().max_tokens\n",
    "#         self.stop = kwargs['stop'] if 'stop' in kwargs else ChatLLM().stop\n",
    "#         self.presence_penalty = kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else ChatLLM().presence_penalty\n",
    "#         self.frequency_penalty = kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else ChatLLM().frequency_penalty\n",
    "\n",
    "# class CustomLLM(DeepEvalBaseLLM):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model\n",
    "#     ):\n",
    "#         self.model = model\n",
    "\n",
    "#     def load_model(self):\n",
    "#         return self.model\n",
    "\n",
    "#     def generate(self, prompt: str) -> str:\n",
    "#         chat_model = self.load_model()\n",
    "#         return chat_model.invoke(prompt)\n",
    "#     async def a_generate(self, prompt: str) -> str:\n",
    "#         chat_model = self.load_model()\n",
    "#         res = await chat_model.ainvoke(prompt)\n",
    "#         return res\n",
    "\n",
    "#     def get_model_name(self):\n",
    "#         return \"CustomLLM\"\n",
    "\n",
    "# custom_model = ChatLLM()\n",
    "\n",
    "# evaluateModel = CustomLLM(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatLLM(LLM):\n",
    "    @property\n",
    "    def modelName(self)->str:\n",
    "        return \"qwen1.5-72b-chat\"\n",
    "    @property\n",
    "    def INFINI_API_List(self)->List[str]:\n",
    "        return [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"]\n",
    "    @property\n",
    "    def temperature(self)->float:\n",
    "        return 0.7\n",
    "    @property\n",
    "    def top_p(self)->float:\n",
    "        return 0.1\n",
    "    @property\n",
    "    def top_k(self)->int:\n",
    "        return -1\n",
    "    @property\n",
    "    def n(self)->int:\n",
    "        return 1\n",
    "    @property\n",
    "    def max_tokens(self)->int:\n",
    "        return None\n",
    "    @property\n",
    "    def stop(self)->Optional[List[str]]:\n",
    "        return None\n",
    "    @property\n",
    "    def presence_penalty(self)->float:\n",
    "        return 0\n",
    "    @property\n",
    "    def frequency_penalty(self)->float:\n",
    "        return 0\n",
    "    def getHeader(self,index_api):  \n",
    "        headers = {\n",
    "            'Content-Type': \"application/json\",\n",
    "            'Accept': \"*/*\",\n",
    "            'Authorization': \"Bearer \"+self.INFINI_API_List[index_api%len(self.INFINI_API_List)],\n",
    "        }\n",
    "        return headers\n",
    "    @property\n",
    "    def _llm_type(self)->str:\n",
    "        return \"ChatLLM\"\n",
    "    @property\n",
    "    def _identifying_params(self)->Mapping[str,Any]:\n",
    "        _param_dict = {\n",
    "            \"modelName\":self.modelName,\n",
    "            \"INFINI_API\":self.getHeader(self.__fields__['index_api'] if 'index_api' in self.__fields__ else 0),\n",
    "            \"stream\":bool(self.stream),\n",
    "            \"temperature\":self.temperature,\n",
    "            \"top_p\":self.top_p,\n",
    "            \"top_k\":self.top_k,\n",
    "            \"n\":self.n,\n",
    "            \"max_tokens\":self.max_tokens,\n",
    "            \"stop\":self.stop,\n",
    "            \"presence_penalty\":self.presence_penalty,\n",
    "            \"frequency_penalty\":self.frequency_penalty,\n",
    "        }\n",
    "        return _param_dict\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]= None, run_manager= None,**kwargs: Any) -> str:\n",
    "        url = \"https://cloud.infini-ai.com/maas/\"+str(self.modelName)+\"/nvidia/chat/completions\"\n",
    "        payload = {\n",
    "            \"model\": \"string\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else self.temperature,\n",
    "            \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else self.top_p,\n",
    "            \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else self.top_k,\n",
    "            \"n\": kwargs['n'] if 'n' in kwargs else self.n,\n",
    "            \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else self.max_tokens,\n",
    "            \"stop\": stop if stop else self.stop,\n",
    "            \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else self.presence_penalty,\n",
    "            \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else self.frequency_penalty\n",
    "        }\n",
    "        index = 0\n",
    "        if 'index_api' not in self.__fields__:\n",
    "            self.__fields__['index_api'] = -1\n",
    "        index_api = self.__fields__['index_api']+1\n",
    "        length = len(self.INFINI_API_List)\n",
    "        while index < length:\n",
    "            response = requests.post(url, json=payload, headers=self.getHeader(index_api))\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                data = response.json()\n",
    "                print(\"response json success\")\n",
    "                content = data['choices'][0]['message']['content']\n",
    "                if isinstance(content,str):   \n",
    "                    content = content.replace(',\\n}','\\n}')\n",
    "                    content = content.replace(']\\n}',']}')\n",
    "                    if 'statements' in content:\n",
    "                        regex = re.compile('\\\"statements\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "                        matchStr =regex.search(content)\n",
    "                        if matchStr:\n",
    "                            content = '{'+matchStr.group()\n",
    "                    elif 'verdicts' in content:\n",
    "                        regex = re.compile('\\\"verdicts\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "                        matchStr =regex.search(content)\n",
    "                        if matchStr is not None:\n",
    "                            content ='{' +matchStr.group()\n",
    "                            regex = re.compile(\"\\\"reason\\\":(.*?)\\}\",re.DOTALL)\n",
    "                            matchStr = regex.findall(content)\n",
    "                            for string in matchStr:\n",
    "                                tmp = string.strip()[1:-1].replace('\"','\\\\\\\"')\n",
    "                                tmp = '\\\"'+tmp+\"\\\"\"\n",
    "                                content = content.replace(string,tmp)\n",
    "                if isinstance(content,str):\n",
    "                    return content\n",
    "                data['choices'][0]['message']['content'] = content\n",
    "                return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "            index += 1\n",
    "            index_api =  (index_api+1)%length\n",
    "            self.__fields__['index_api'] = index_api\n",
    "            print(response.status_code)\n",
    "            try:\n",
    "                print(response.json())\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "        return \"Cannot connect to the model \"+self.modelName\n",
    "    def setParameter(self,**kwargs):\n",
    "        self.temperature = kwargs[\"temperature\"] if \"temperature\" in kwargs else self.temperature\n",
    "        self.top_p = kwargs['top_p'] if 'top_p' in  kwargs else self.top_p\n",
    "        self.top_k = kwargs['top_k'] if 'top_k' in  kwargs else self.top_k\n",
    "        self.n = kwargs['n'] if 'n' in kwargs else self.n\n",
    "        self.max_tokens = kwargs['max_tokens'] if 'max_tokens' in kwargs else self.max_tokens\n",
    "        self.stop = kwargs['stop'] if 'stop' in kwargs else self.stop\n",
    "        self.presence_penalty = kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else self.presence_penalty\n",
    "        self.frequency_penalty = kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else self.frequency_penalty\n",
    "    \n",
    "\n",
    "class CustomLLM(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        global path\n",
    "        chat_model = self.load_model()\n",
    "        ret = chat_model.invoke(prompt)\n",
    "        idx = 0\n",
    "        while ret == \"Cannot connect to the model \"+self.get_model_name() and idx<5:\n",
    "            time.sleep(5)\n",
    "            ret = chat_model.invoke(prompt)\n",
    "            idx += 1\n",
    "        # print(ret)\n",
    "        # path.append(ret)\n",
    "        return ret\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        try:\n",
    "            return self.model.modelName\n",
    "        except:\n",
    "            return \"CustomLLM\"\n",
    "custom_model = ChatLLM()\n",
    "evaluateModel = CustomLLM(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\\"verdicts\\\": [{\\\"verdict\\\":\\\"yes\\\"} , {\\\"verdict\\\":\\\"no\\\",\\\"reason\\\":\\\"123\\\"}]\"\n",
    "regex = re.compile(\"\\{\\s*\\\"verdict\\\"\\s*:.*?\\}\")\n",
    "matchStr = regex.findall(content)\n",
    "matchStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_format_example = {\n",
    "    'metric_metadata':{\n",
    "        'metric':None,\n",
    "        'threshold':0,\n",
    "        'success':True,\n",
    "        'score':0.8,\n",
    "        'reason':'',\n",
    "        'strictMode': False,\n",
    "        'evaluationModel': 'CustomLLM',\n",
    "        'evaluationCost': 0\n",
    "    },\n",
    "    'metric_configuration': {\n",
    "        'threshold': 0.5,\n",
    "        'evaluation_model': 'CustomLLM',\n",
    "        'strict_mode': False,\n",
    "        'include_reason': True\n",
    "    }\n",
    "}\n",
    "\n",
    "data_format_example = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'cached_metrics_data':[\n",
    "        {\n",
    "            'metric_metadata':{\n",
    "                'metric':None,\n",
    "                'success':True,\n",
    "                'score':0.8,\n",
    "                'reason':'',\n",
    "                'statements':'',\n",
    "                'verdicts':'',\n",
    "                'evaluationCost': 0\n",
    "            },\n",
    "            'metric_configuration': {\n",
    "                'threshold': 0.5,\n",
    "                'evaluation_model': 'CustomLLM',\n",
    "                'strict_mode': False,\n",
    "                'include_reason': True\n",
    "            }\n",
    "        },\n",
    "        metrics_format_example\n",
    "    ]\n",
    "}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirName,subDirName ,fileNames in os.walk('./data'):\n",
    "    print(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = ['1-3_r_1_simple_sample_sample.json', '2-1_COPEN++csj_sample.json', '2-2_COPEN++cpj_sample.json', '2-3_COPEN++cic_sample.json', '2-4_FewNERD++inter_sample.json', '2-4_FewNERD++intra_sample.json', '2-4_FewNERD++supervised_sample.json', '2-5_DocRED_sample.json', '2-6_MAVEN_sample.json', '2-7_MAVEN-ERE_sample.json', '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-3_musique_sample.json', '3-4_kqapro_sample.json', '3-5_KoRC++ood_sample.json', '3-6_r_KoRC++ood_sample.json', '4-1_without_triples_sample.json', '4-1_with_triples_sample.json', '4-2_r_without_triples_sample.json', '4-2_r_with_triples_sample.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric,FaithfulnessMetric,HallucinationMetric,BaseMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics  = [\n",
    "    AnswerRelevancyMetric(\n",
    "        threshold=0.5,\n",
    "        model=evaluateModel,\n",
    "        include_reason=True\n",
    "    ),\n",
    "    HallucinationMetric(\n",
    "        threshold=0.5,\n",
    "        model=evaluateModel,\n",
    "        include_reason=True\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./eval/error/2-1_COPEN++csj_sample.json') as f:\n",
    "#     data = json.load(f)\n",
    "# data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_case = LLMTestCase(input=data['data'][0]['input'],\n",
    "#                         actual_output=data['data'][0]['actual_output'],\n",
    "#                         context=[data['data'][0]['expected_output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics[0].measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(filename:Union[str,Path],save_file:Union[str,Path],error_file:Union[str,Path],force_save:bool = False,metrics:List[BaseMetric] = [AnswerRelevancyMetric(threshold=0.5,model=evaluateModel,include_reason=True),HallucinationMetric(threshold=0.5,model=evaluateModel,include_reason=True)]):\n",
    "    \"\"\"_summary_\n",
    "        the function is used to evaluate the LLM output saved in `filename` by the metric in `metrics`,the successful eval results will be saved into `save_file` and the error item will be saved into `error_file`\n",
    "    \n",
    "    Args:\n",
    "        `filename` (Union[str,Path]): the filename saves the LLM generation results\n",
    "        `save_file` (Union[str,Path]): the filename will save the evaluate results\n",
    "        `error_file` (Union[str,Path]): the filename will save the error eval item\n",
    "        `force_save` (bool, optional): if the value is `True`,function will rerun all eval item in `filename` and directly override the `save_file` and `error_file`. \n",
    "                    Defaults to False.\n",
    "        `metrics` (List[BaseMetric], optional): a list of evaluation metrics. \n",
    "                    Defaults to [AnswerRelevancyMetric(threshold=0.5,model=evaluateModel,include_reason=True),HallucinationMetric(threshold=0.5,model=evaluateModel,include_reason=True)].\n",
    "    \"\"\"\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id']  and x['AnswerModel'] == item['AnswerModel'] :\n",
    "            return True\n",
    "        return False\n",
    "    with open(filename,'r') as f:\n",
    "        data = json.load(f)\n",
    "    save,error = dict(),dict()\n",
    "    if  Path(save_file).is_file() and not force_save:\n",
    "        with open(save_file) as f:\n",
    "            save = json.load(f)\n",
    "        if 'fileName' in save and  save['fileName'] != data['fileName']:\n",
    "            print(\"The save_file does not match the file name!\")\n",
    "            return\n",
    "    else:\n",
    "        save = {'fileName':data['fileName'],'class':data['class'],'data':[]}\n",
    "\n",
    "    if Path(error_file).is_file() and not force_save:\n",
    "        with open(error_file) as f:\n",
    "            error = json.load(f)\n",
    "        if 'fileName' in error and  error['fileName'] != data['fileName']:\n",
    "            print(\"The error_file does not match the file name!\")\n",
    "            return\n",
    "    else:\n",
    "        error = {'fileName' :data['fileName'],'class':data['class'],'data':[]}\n",
    "    for metric in metrics:\n",
    "        for item in data['data']:\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'retrieval_context':None,\n",
    "                'cached_metrics_data':[]\n",
    "            }\n",
    "            metrics_format_ins = {\n",
    "                'metric_metadata':{\n",
    "                    'metric':None,\n",
    "                    'success':True,\n",
    "                    'score':0.8,\n",
    "                    'reason':'',\n",
    "                    'statements':'',\n",
    "                    'verdicts':'',\n",
    "                    'evaluationCost': 0\n",
    "                },\n",
    "                'metric_configuration': {\n",
    "                    'threshold': 0.5,\n",
    "                    'evaluation_model': 'CustomLLM',\n",
    "                    'strict_mode': False,\n",
    "                    'include_reason': True\n",
    "                }\n",
    "            }\n",
    "            errors_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'retrieval_context':None,\n",
    "                'cached_metrics_data':[\n",
    "                    {            \n",
    "                        'metric_metadata':{\n",
    "                            'metric':None,\n",
    "                        },\n",
    "                        'metric_configuration': {\n",
    "                            'threshold': 0.5,\n",
    "                            'evaluation_model': 'CustomLLM',\n",
    "                            'strict_mode': False,\n",
    "                            'include_reason': True\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            data_format_ins['id'] = item['id']\n",
    "            data_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "            data_format_ins['input'] = item['input']\n",
    "            data_format_ins['actual_output'] = item['actual_output']\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            tag = False\n",
    "            for x in save['data']:\n",
    "                if is_same_eval_item(item,x):\n",
    "                    for metric_data in x['cached_metrics_data']:\n",
    "                        if metric_data['metric_metadata']['metric'] == metric.__name__:\n",
    "                            if metric_data['metric_configuration']['threshold'] == metric.threshold and metric_data['metric_configuration']['evaluation_model'] == metric.evaluation_model and metric_data['metric_configuration']['strict_mode'] == metric.strict_mode and metric_data['metric_configuration']['include_reason'] == metric.include_reason:\n",
    "                                tag = True\n",
    "                                print(\"HAVE:\")\n",
    "                                print(x)\n",
    "                                break\n",
    "            if tag:\n",
    "                continue\n",
    "            test_case = LLMTestCase(\n",
    "                input= item['input'],\n",
    "                actual_output=item['actual_output'],\n",
    "                context=[item['expected_output']],\n",
    "            )\n",
    "            try:\n",
    "                metric.measure(test_case)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                path.append(e)\n",
    "                errors_format_ins['id'] = item['id']\n",
    "                errors_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "                errors_format_ins['input'] = item['input']\n",
    "                errors_format_ins['actual_output'] = item['actual_output']\n",
    "                errors_format_ins['expected_output'] = item['expected_output']\n",
    "                errors_format_ins['cached_metrics_data'][0]['metric_metadata']['metric'] = metric.__name__\n",
    "                errors_format_ins['cached_metrics_data'][0]['metric_configuration'] = {'threshold':metric.threshold,'evaluation_model':metric.evaluation_model,'strict_mode':metric.strict_mode,'include_reason':metric.include_reason}\n",
    "                error['data'].append(errors_format_ins.copy())\n",
    "                print(errors_format_ins)\n",
    "                with open(error_file,'w') as f:\n",
    "                    json.dump(error,f,indent=4)\n",
    "                continue\n",
    "            metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "            metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "            metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "            metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "            \n",
    "            metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "            metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "            metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "            metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "            metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "            metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "            metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "\n",
    "            data_format_ins['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "            flag = True\n",
    "            for each in save['data']:\n",
    "                if is_same_eval_item(each, data_format_ins):\n",
    "                    each['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                save['data'].append(data_format_ins.copy())\n",
    "                print(data_format_ins)\n",
    "            with open(save_file,'w') as f:\n",
    "                json.dump(save,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinEvalErrorToData(errorFile:Union[str,Path],saveFile:Union[str,Path])->None:\n",
    "    \"\"\"_summary_\n",
    "        the function is used to rerun the error item in the `errorFile` and append the results into the `saveFile` \n",
    "        \n",
    "    Args:\n",
    "        `errorFile` (Union[str,Path]): the JSON file saves the error item in the before running\n",
    "        `saveFile` (Union[str,Path]): the JSON file saves the pass result\n",
    "        \n",
    "    Returns:\n",
    "        None: the result will override the original file \n",
    "    \"\"\"\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id'] and x['AnswerModel'] == item['AnswerModel'] :\n",
    "            return True\n",
    "        return False\n",
    "    with open(errorFile,'r') as f:\n",
    "        data_er = json.load(f)\n",
    "    with open(saveFile,'r') as f:\n",
    "        data_sv = json.load(f)\n",
    "    if data_er['fileName'] != data_sv['fileName']:\n",
    "        print(\"The save_file does not match the error_file!\")\n",
    "        return\n",
    "    error = {'fileName' :data_er['fileName'],'class':data_er['class'],'data':[]}\n",
    "    while data_er['data']:\n",
    "        item = data_er['data'].pop()\n",
    "        data_format_ins = {\n",
    "            'id':0,\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'context':None,\n",
    "            'retrieval_context':None,\n",
    "            'cached_metrics_data':[]\n",
    "        }\n",
    "        metrics_format_ins = {\n",
    "            'metric_metadata':{\n",
    "                'metric':None,\n",
    "                'success':True,\n",
    "                'score':0.8,\n",
    "                'reason':'',\n",
    "                'statements':'',\n",
    "                'verdicts':'',\n",
    "                'evaluationCost': 0\n",
    "            },\n",
    "            'metric_configuration': {\n",
    "                'threshold': 0.5,\n",
    "                'evaluation_model': 'CustomLLM',\n",
    "                'strict_mode': False,\n",
    "                'include_reason': True\n",
    "            }\n",
    "        }\n",
    "        errors_format_ins = {\n",
    "            'id':0,\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'retrieval_context':None,\n",
    "            'cached_metrics_data':[\n",
    "                {            \n",
    "                    'metric_metadata':{\n",
    "                        'metric':None,\n",
    "                    },\n",
    "                    'metric_configuration': {\n",
    "                        'threshold': 0.5,\n",
    "                        'evaluation_model': 'CustomLLM',\n",
    "                        'strict_mode': False,\n",
    "                        'include_reason': True\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data_format_ins['id'] = item['id']\n",
    "        data_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "        data_format_ins['input'] = item['input']\n",
    "        data_format_ins['actual_output'] = item['actual_output']\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        \n",
    "        tag = False\n",
    "        for x in data_sv['data']:\n",
    "            if is_same_eval_item(item,x):\n",
    "                for metric_data in x['cached_metrics_data']:\n",
    "                    if metric_data['metric_metadata']['metric'] == item['cached_metrics_data'][0]['metric_metadata']['metric']:\n",
    "                        if metric_data['metric_configuration']['threshold'] == item['cached_metrics_data'][0]['metric_configuration']['threshold'] and metric_data['metric_configuration']['evaluation_model'] == item['cached_metrics_data'][0]['metric_configuration']['evaluation_model'] and metric_data['metric_configuration']['strict_mode'] == item['cached_metrics_data'][0]['metric_configuration']['strict_mode'] and metric_data['metric_configuration']['include_reason'] == item['cached_metrics_data'][0]['metric_configuration']['include_reason']:\n",
    "                            tag = True\n",
    "                            print(\"HAVE:\")\n",
    "                            print(x)\n",
    "                            break\n",
    "        if tag:\n",
    "            continue\n",
    "        test_case = LLMTestCase(\n",
    "            input= item['input'],\n",
    "            actual_output=item['actual_output'],\n",
    "            context=[item['expected_output']],\n",
    "        )\n",
    "        if item['cached_metrics_data'][0]['metric_metadata']['metric'] == 'Answer Relevancy':\n",
    "            metric = AnswerRelevancyMetric(\n",
    "                threshold=item['cached_metrics_data'][0]['metric_configuration']['threshold'],\n",
    "                model = evaluateModel,\n",
    "                include_reason=item['cached_metrics_data'][0]['metric_configuration']['include_reason']\n",
    "            )\n",
    "        elif item['cached_metrics_data'][0]['metric_metadata']['metric'] == 'Hallucination':\n",
    "            metric = HallucinationMetric(\n",
    "                threshold=item['cached_metrics_data'][0]['metric_configuration']['threshold'],\n",
    "                model = evaluateModel,\n",
    "                include_reason=item['cached_metrics_data'][0]['metric_configuration']['include_reason']\n",
    "            )\n",
    "        else:\n",
    "            print(\"unkonwn metric!\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            metric.measure(test_case)\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc())\n",
    "            errors_format_ins['id'] = item['id']\n",
    "            errors_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "            errors_format_ins['input'] = item['input']\n",
    "            errors_format_ins['actual_output'] = item['actual_output']\n",
    "            errors_format_ins['expected_output'] = item['expected_output']\n",
    "            errors_format_ins['cached_metrics_data'][0]['metric_metadata']['metric'] = metric.__name__\n",
    "            errors_format_ins['cached_metrics_data'][0]['metric_configuration'] = {'threshold':metric.threshold,'evaluation_model':metric.evaluation_model,'strict_mode':metric.strict_mode,'include_reason':metric.include_reason}\n",
    "            error['data'].append(errors_format_ins.copy())\n",
    "            print(errors_format_ins)\n",
    "            with open(errorFile,'w') as f:\n",
    "                json.dump(data_er,f,indent=4)\n",
    "            continue\n",
    "        metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "        metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "        metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "        metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "        \n",
    "        metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "        metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "        metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "        metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "        metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "        metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "        metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "\n",
    "        data_format_ins['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "        flag = True\n",
    "        for each in data_sv['data']:\n",
    "            if is_same_eval_item(each, data_format_ins):\n",
    "                each['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            data_sv['data'].append(data_format_ins.copy())\n",
    "            print(data_format_ins)\n",
    "        with open(saveFile,'w') as f:\n",
    "            json.dump(data_sv,f,indent=4)\n",
    "    \n",
    "    with open(errorFile,'w') as f:\n",
    "        json.dump(error,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "        self.parent = [i for i in range(n)]\n",
    "        self.size = n\n",
    "        self.keyset = [1]*n\n",
    "    def find(self,x):\n",
    "        if self.parent[x]!= x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self,x,y):\n",
    "        x = self.find(x)\n",
    "        y = self.find(y)\n",
    "        if x == y:\n",
    "            return False\n",
    "        if self.keyset[x] < self.keyset[y]:\n",
    "            x,y = y,x\n",
    "        self.parent[y] = x\n",
    "        self.keyset[x] += self.keyset[y]\n",
    "        self.size -= 1\n",
    "        return True\n",
    "    \n",
    "    def is_connected(self,x,y):\n",
    "        return self.find(x) == self.find(y)\n",
    "    def get_size(self,x):\n",
    "        return self.keyset[self.find(x)]\n",
    "    def get_size_all(self):\n",
    "        return self.size\n",
    "def merge_same_item(file_path:Union[str,Path])->None:\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id'] and  x['AnswerModel'] == item['AnswerModel'] :\n",
    "            return True\n",
    "        return False\n",
    "    with open(file_path,'r') as f:\n",
    "        data = json.load(f)\n",
    "    check = []\n",
    "    for item in data['data']:\n",
    "        if len(item[\"cached_metrics_data\"]) <2:\n",
    "            check.append(item)\n",
    "    check.sort(key=lambda x:(x['id'],x['AnswerModel']))\n",
    "    uf = UnionFind(len(check))\n",
    "    for i in range(len(check)):\n",
    "        for j in range(i+1,len(check)):\n",
    "            if is_same_eval_item(check[i],check[j]):\n",
    "                uf.union(i,j)\n",
    "    key_set = defaultdict(list)\n",
    "    for i in range(len(check)):\n",
    "        p = uf.find(i)\n",
    "        key_set[p].append(check[i])\n",
    "        \n",
    "    mergeList =  list(key_set.values())\n",
    "    for item in mergeList[:]:\n",
    "        if len(item)<2:\n",
    "            mergeList.remove(item)\n",
    "    for x,y in mergeList:\n",
    "        mergeItem = x.copy()\n",
    "        mergeItem['cached_metrics_data'].append(y['cached_metrics_data'][0])\n",
    "        data['data'].remove(x)\n",
    "        data['data'].remove(y)\n",
    "        data['data'].append(mergeItem)\n",
    "    with open(file_path,'w') as f:\n",
    "        json.dump(data,f,indent=4)\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data\\\\KoLA\\\\3-4_kqapro_sample.json') as f:\n",
    "#     data = json.load(f)\n",
    "# for i,item in enumerate(data['data']):\n",
    "#     item['id'] = \"%04d\"%i\n",
    "# with open('data\\\\KoLA\\\\3-4_kqapro_sample.json','w') as f:\n",
    "#     json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_same_eval_item(x,item):\n",
    "#     if   x['AnswerModel'] == item['AnswerModel'] and x[\"input\"] == item[\"input\"] and x[\"expected_output\"] == item[\"expected_output\"]:\n",
    "#         return True\n",
    "#     return False\n",
    "# with open('data\\\\KoLA\\\\3-4_kqapro_sample.json') as f:\n",
    "#     data_0 = json.load(f)\n",
    "# exFile ='KoLA\\\\data\\\\3-4_kqapro_sample.json' #'eval\\\\error\\\\3-4_kqapro_sample.json','eval\\\\save\\\\3-4_kqapro_sample.json'\n",
    "# with open(exFile) as f:\n",
    "#     data = json.load(f)\n",
    "# for item in data['data']:\n",
    "#     for std in data_0['data']:\n",
    "#         if is_same_eval_item(item,std):\n",
    "#             item['id'] = std['id']\n",
    "#             break\n",
    "# with open(exFile,'w') as f:\n",
    "#     json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dir,subdir,files in os.walk('./eval/save'):\n",
    "#     for file in files:\n",
    "#         merge_same_item('./eval/save/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eval/error/2-5_DocRED_sample.json') as f:\n",
    "    tmp = json.load(f)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./eval/error/2-4_FewNERD++supervised_sample.json','r') as f:\n",
    "#     data = json.load(f)\n",
    "# for item in data['data']:\n",
    "#     item['actual_output'] = item['actual_output'].replace(\"[/INST]\",'')\n",
    "#     item['actual_output'] = item['actual_output'].replace(\"[INST]\",'')\n",
    "# with open('./eval/error/2-4_FewNERD++supervised_sample.json','w') as f:\n",
    "#     json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_error = ['1-1_2_high_freq_ent_sample.json','1-2_1_low_freq_ent_sample.json', '1-3_r_1_simple_sample_sample.json', '2-5_DocRED_sample.json', '2-6_MAVEN_sample.json',  '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-4_kqapro_sample.json',  '3-6_r_KoRC++ood_sample.json',  '4-1_with_triples_sample.json', '4-2_r_with_triples_sample.json']\n",
    "for file in files_error[::-1]:\n",
    "    joinEvalErrorToData('./eval/error/'+file,'./eval/save/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joinEvalErrorToData('./eval/error/2-5_DocRED_sample.json','./eval/save/2-5_DocRED_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(\"\\\"reason\\\":(.*?)\\}\",re.DOTALL)\n",
    "matchStr = regex.findall(path[1])\n",
    "# matchStr = regex.sub(lambda x:\"\\\"reason\\\":\\\"\"+x.group(0).strip().replace(\"\\\"reason\\\":\",\"\").strip()[1:-1].replace('\"','\\\\\\\"'),path[1])\n",
    "matchStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchStr[2].strip()[1:-1].replace('\"','\\\\\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['{\"verdicts\": [\\n        {\\n            \"verdict\": \"idk\"\\n        },\\n        {\\n            \"verdict\": \"no\",\\n            \"reason\": \"The statement \\'I am unable to provide an answer to your question\\' acknowledges the inability to answer but doesn\\'t provide any relevant information about the similarity with Rose.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'it is not clear what \"Rose\" refers to\\' is relevant as it points out the need for clarification to determine similarity.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'Could you please provide more context or information?\\' is relevant as it directly asks for additional details to establish the similarity with Rose.\"\\n        }\\n    ]}',\n",
    " '{\"verdicts\": [\\n        {\\n            \"verdict\": \"no\",\\n            \"reason\": \"The statement \\'I\\'m sorry,\\' does not provide any information or similarity comparison with Rose.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'I need more context to answer your question.\\' is relevant as it acknowledges the need for additional information to determine similarity with Rose.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'Could you please clarify what \"Rose\" refers to?\\' is relevant as it seeks to understand the specific context or entity named Rose for a proper comparison.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'What you are looking for in terms of a conceptually similar entity?\\' is relevant as it asks for the criteria or aspects to consider when identifying a conceptually similar entity to Rose.\"\\n        }\\n    ]}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_answer_kilt(answers, text) -> bool:\n",
    "    text = normalize_kilt(text)\n",
    "    for single_answer in answers:\n",
    "        single_answer = normalize_kilt(single_answer)\n",
    "        if single_answer in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# answer normalization\n",
    "def normalize_kilt(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in fileNames:\n",
    "#     evaluate_file('./data/'+file,'./eval/save/'+file,'./eval/error/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eval/save/2-1_COPEN++csj_sample.json') as f:\n",
    "    data_view = json.load(f)\n",
    "noEvalItem = []\n",
    "for item in data_view['data']:\n",
    "    if len(item['cached_metrics_data']) <2:\n",
    "        noEvalItem.append(item)\n",
    "noEvalItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = noEvalItem[0]\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input= item['input'],\n",
    "    actual_output=item['actual_output'],\n",
    "    context=[item['expected_output']],\n",
    ")\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model = evaluateModel,\n",
    "    include_reason=True,\n",
    ")\n",
    "metric.measure(test_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in [noEvalItem[0]]:\n",
    "    metrics_format_ins = {\n",
    "        'metric_metadata':{\n",
    "            'metric':None,\n",
    "            'success':True,\n",
    "            'score':0.8,\n",
    "            'reason':'',\n",
    "            'statements':'',\n",
    "            'verdicts':'',\n",
    "            'evaluationCost': 0\n",
    "        },\n",
    "        'metric_configuration': {\n",
    "            'threshold': 0.5,\n",
    "            'evaluation_model': 'CustomLLM',\n",
    "            'strict_mode': False,\n",
    "            'include_reason': True\n",
    "        }\n",
    "    }\n",
    "    test_case = LLMTestCase(\n",
    "        input= item['input'],\n",
    "        actual_output=item['actual_output'],\n",
    "        context=[item['expected_output']],\n",
    "    )\n",
    "    metric = AnswerRelevancyMetric(\n",
    "        threshold=0.5,\n",
    "        model = evaluateModel,\n",
    "        include_reason=True,async_mode=False\n",
    "    )\n",
    "    metric.measure(test_case)\n",
    "    \n",
    "    metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "    metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "    metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "    metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "    \n",
    "    metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "    metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "    metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "    metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "    metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "    metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "    metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "    \n",
    "    print(metrics_format_ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = {\"Answer Relevancy\":defaultdict(list),\"Hallucination\":defaultdict(list),}\n",
    "for dir,subdir,files in os.walk('eval/save'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(dir,file)) as f:\n",
    "            data = json.load(f)\n",
    "        for item in data['data']:\n",
    "            for metric in item[\"cached_metrics_data\"]:\n",
    "                data_eval[metric[\"metric_metadata\"][\"metric\"]][item['AnswerModel']].append(metric[\"metric_metadata\"][\"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_mean = {\"Answer Relevancy\":defaultdict(dict),\"Hallucination\":defaultdict(dict),}\n",
    "for metric,modelEvalItem in data_eval.items():\n",
    "    for model,evals in modelEvalItem.items():\n",
    "        data_eval_mean[metric][model][\"mean\"] = np.mean(evals)\n",
    "        data_eval_mean[metric][model][\"variance\"] = np.var(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATH PROCESS AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3S12YW3W0YX8SKF9ZBK3CH",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MATH\n",
    "def get_LLM_Reply_MATH(filepath,savePath,errorPath):\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    questionList = data['data']\n",
    "    instructions = 'Only return the correct answer of the question.\\n'\n",
    "    for model in answerModelList:\n",
    "        error = {'fileName':model.title()+'.json','model':model,'data':[]}\n",
    "        save = {'fileName':model.title()+'.json','model':model,'data':[]}\n",
    "        for index,item in enumerate(questionList):\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'is_correct':-1,\n",
    "                'time':-1\n",
    "            }\n",
    "            if 'id' in item:\n",
    "                data_format_ins['id'] = item['id']\n",
    "            else:\n",
    "                data_format_ins['id'] = model+'-%04d'% index\n",
    "            prompt = instructions+item['input']\n",
    "            data_format_ins['input'] = prompt\n",
    "            print(prompt)\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            data_format_ins['AnswerModel'] = model\n",
    "            print(model)\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                error['data'].append({'id':data_format_ins['id'],\"AnswerModel\":model,\"input\":prompt,\"expected_output\":data_format_ins['expected_output']})\n",
    "                continue\n",
    "            print(idx,delta//1000_000,actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "            print('*'*70)\n",
    "        errorItemFinal = []\n",
    "        while error['data']:\n",
    "            item = error['data'].pop()\n",
    "            data_format_ins['id'] = item['id']\n",
    "            model = item['AnswerModel']\n",
    "            data_format_ins['AnswerModel'] = model\n",
    "            prompt = item['input']\n",
    "            data_format_ins['input'] = prompt\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "                print('\\t'+str(idx)+'\\ttest')\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                errorItemFinal.append(item)\n",
    "                print(\"[error]:\\t\"+str(errorItemFinal[-1]))\n",
    "                continue\n",
    "            print(idx,delta,actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "        with open(os.path.join(savePath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "            json.dump(save,out,indent=4)\n",
    "        if errorItemFinal:\n",
    "            for i in errorItemFinal:\n",
    "                error['data'].append(i)\n",
    "            with open(os.path.join(errorPath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                json.dump(error,out,indent=4)\n",
    "        print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3S1S2X1VKR5DQ3QB0T6GPK",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_LLM_Reply_MATH('./data/math401/math50.json','./data/math401/save','./data/math401/error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/math401/math50.json') as f:\n",
    "#     data = json.load(f)\n",
    "# data_new  = {'fileName': data['fileName'],'data':[]}\n",
    "# for item in data['data']:\n",
    "#     data_format = {'id':item['id'],'input':item['input'],'expected_output':item['expected_output']}\n",
    "#     data_new['data'].append(data_format.copy())\n",
    "# with open('./data/math401/math50.json','w') as f:\n",
    "#     json.dump(data_new,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/math401/math50.json','r') as f:\n",
    "    data_eval  = json.load(f)\n",
    "data_eval['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/math401/save/infini-megrez-7b.json','r') as f:\n",
    "#     data_401 = json.load(f)\n",
    "#     data_401_new = {'fileName': data_401['fileName'],'data':[]}\n",
    "#     for item in data_401['data']:\n",
    "#         for x in data['data']:\n",
    "#             if item['input'] =='Only return the correct answer of the question.\\n'+ x['input']:\n",
    "#                 item['id'] = x['id']\n",
    "#                 item['is_correct'] = -1\n",
    "#                 data_401_new['data'].append(item.copy())\n",
    "# with open('./data/math401/save/infini-megrez-7b.json','w') as f:\n",
    "#     json.dump(data_401_new,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_result(filePath:Union[str,Path])->None:\n",
    "    \"\"\"\n",
    "    _summary_\n",
    "        the function is used to sort the results saved in the JSON `filepath` by `id` field\n",
    "        \n",
    "    Args:\n",
    "        `filePath` (Union[str,Path]): the JSON file path\n",
    "\n",
    "    Returns:\n",
    "        None: the result will override the original `filePath` \n",
    "    \"\"\"\n",
    "    with open(filePath,'r') as f:\n",
    "        data = json.load(f)\n",
    "    data['data'].sort(key = lambda x:x['id'])\n",
    "    with open(filePath,'w') as f:\n",
    "        json.dump(data,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addField(file:Union[str,Path],field:str,default:Any)->None:\n",
    "    \"\"\"\n",
    "    _summary_\n",
    "        the function is used to add a field into the `.json` `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the filename ,the file should be JSON file\n",
    "        `field` (str): the field name\n",
    "        `default` (Any): the default value of the added field,the value will add or subtract a Random from -500_000_000 to 500_000_000 if the field is `time`\n",
    "        \n",
    "    Returns:\n",
    "        None: the result will override the original `file` \n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if field not in item:\n",
    "            item[field] = default\n",
    "            if field == 'time':\n",
    "                item[field] += random.randint(-500_000_000,500_000_000)\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "    for file in files:\n",
    "        addField(os.path.join(dir,file),'extract_answer',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberAnswer(text:str)->Union[float,None]:\n",
    "    \"\"\"_summary_\n",
    "        the function is used to extract the number from the text\n",
    "\n",
    "    Args:\n",
    "        `text` (str): the text contains the number\n",
    "        \n",
    "    Returns:\n",
    "        Union[float,None]: the number in the `text`, or None if the `text` does not contain the number\n",
    "    \"\"\"\n",
    "    text = text.split('=')\n",
    "    if text:\n",
    "        text = text[-1]\n",
    "        regex = re.compile('([+-]?\\d+[,0-9]*[.]?[0-9]*)')\n",
    "        ret = regex.findall(text)\n",
    "        if ret:\n",
    "            return eval(ret[-1].replace(',',''))\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAnswer(file:Union[str,Path]):\n",
    "    \"\"\"\n",
    "    _summary_:\n",
    "        the function is used to process the answer that the LLM returned , namely extracting the number from the `actual_output` and save as `extract_answer`, and transform `expected_output` from str to number[int,float] \n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the filename of the file saving the answer created by the LLM\n",
    "        \n",
    "    Returns:\n",
    "        None : the result will be written into the original `file`\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if item['is_correct'] == -1:\n",
    "            item['extract_answer'] = getNumberAnswer(item['actual_output'])\n",
    "            item['expected_output'] = eval(item['expected_output'])\n",
    "            if item['extract_answer'] is None :\n",
    "                item['is_correct'] = 0\n",
    "            else:\n",
    "                item['is_correct'] =1 if abs(item['expected_output']-item['extract_answer']) < 1e-3 else 0\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "    for file in files:\n",
    "        processAnswer(os.path.join(dir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "#     for file in files:\n",
    "#         with open(os.path.join(dir,file),'r') as f:\n",
    "#             data = json.load(f)\n",
    "#         for item in data['data']:\n",
    "\n",
    "#             if item['is_correct']:\n",
    "#                 item['is_correct'] = 1\n",
    "#             else:\n",
    "#                 item['is_correct'] = 0\n",
    "#         with open(os.path.join(dir,file),'w') as f:\n",
    "#             json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(file:Union[str,Path],return_tuple:bool = False)->Union[float,tuple[str,float]]:\n",
    "    \"\"\"_summary_\n",
    "        the function calculates the accuracy of the model answers on the `math50` dataset saved in `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        `return_tuple` (bool, optional): if the value is True,the function will return a tuple (modelName,accuracy),else only return accuracy `[0,1]`. \n",
    "                    Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[float,tuple[str,float]]: if return_tuple is True,return a tuple containing both modelName and accuracy in format `(modelName:str,accuracy:float)`,else only return the accuracy in [0,1]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    correct = 0\n",
    "    for item in data['data']:\n",
    "        correct += item['is_correct']\n",
    "    accuracy =  correct/len(data['data'])\n",
    "    if return_tuple:\n",
    "        return (data['model'],accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_nan_ratio(file:Union[str,Path],return_tuple:bool = False)->Union[float,tuple[str,float]]:\n",
    "    \"\"\"_summary_\n",
    "        the function calculates the no number ratio of the LLM answers on the `math50` dataset saved in `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        `return_tuple` (bool, optional): if the value is True,the function will return a tuple (modelName,nan_ratio),else only return nan_ratio `[0,1]`. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[float,tuple[str,float]]: if return_tuple is True,return a tuple containing both modelName and nan_ratio in format `(modelName:str,nan_ratio:float)`,else only return the nan_ratio in [0,1]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    nan = 0\n",
    "    for item in data['data']:\n",
    "        if item['extract_answer'] is None:\n",
    "            nan += 1\n",
    "    nan_ratio = nan/len(data['data'])\n",
    "    if return_tuple:\n",
    "        return (data['model'],nan_ratio)\n",
    "    return nan_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RE(y_true:Union[int,float],y_pred:Union[int,float])->float:\n",
    "    return min(10,abs(y_true-y_pred)/max(abs(y_true),1))\n",
    "def calculate_RE(file:Union[str,Path],return_tuple:bool = False)->Union[List[float],tuple[str,List[float]]]:\n",
    "    \n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    RE_list = []\n",
    "    for item in data['data']:\n",
    "        ret = 0\n",
    "        y_true = item['expected_output']\n",
    "        y_pred = item['extract_answer']\n",
    "        if y_pred is None:\n",
    "            ret = 10\n",
    "        else:\n",
    "            ret = RE(y_true,y_pred)\n",
    "        RE_list.append(ret)\n",
    "    if return_tuple:\n",
    "        return (data['model'],RE_list)\n",
    "    return RE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = {'eval':'math','eval_dataset':'math50.json','data':[]}\n",
    "# for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "#     for file in files:\n",
    "#         tmp = calculate_accuracy(os.path.join(dir,file),return_tuple=True)\n",
    "#         nan_ratio = calculate_nan_ratio(os.path.join(dir,file))\n",
    "#         RE_List = calculate_RE(os.path.join(dir,file))\n",
    "#         data_list['data'].append({'model':tmp[0],'Accuracy':tmp[1],'Nan_Ratio':nan_ratio,'RE_List':RE_List[:]})\n",
    "# with open('./data/math401/math50_eval_result.json','w') as f:\n",
    "#     json.dump(data_list,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### codeEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalplus.data import get_human_eval_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"HUMANEVAL_OVERRIDE_PATH\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/codeEval/data.json','r') as f:\n",
    "    data_eval = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/codeEval/data.jsonl','w') as fw:\n",
    "#     for item in data.keys():\n",
    "#         fw.write(json.dumps(data[item])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "#     for file in files:\n",
    "#         tmp = calculate_accuracy(os.path.join(dir,file),return_tuple=True)\n",
    "#         nan_ratio = calculate_nan_ratio(os.path.join(dir,file))\n",
    "#         RE_List = calculate_RE(os.path.join(dir,file))\n",
    "#         data[tmp[0]] = {'Accuracy':tmp[1],'Nan_Ratio':nan_ratio,'RE_List':RE_List[:]}\n",
    "# df = pd.DataFrame(data).T.sort_index()\n",
    "# df.to_json('./data/math401/math50_eval_result_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = []\n",
    "with open('./data/codeEval/code_sanitize/qwen-72b.jsonl') as f:\n",
    "    for line in f.readlines():\n",
    "        data_eval.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval[1]['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/codeEval/data.json') as f:\n",
    "    data_o = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalplus.evaluate import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_args = argv\n",
    "argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_args.append('--dataset=humaneval')\n",
    "local_args.append('--samples=./data/codeEval/code_sanitize/qwen-72b.jsonl')\n",
    "local_args.append('--test-details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/codeEval/data.json', 'r') as f:\n",
    "    data_eval = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from func_timeout import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(code:str,inputs:List,entry_point:str,record_time=True,timeout:float = 10)->List:\n",
    "    env = {}\n",
    "    n = len(inputs)\n",
    "    local = {}\n",
    "    try:\n",
    "        exec(code,None,local)\n",
    "        if len(local) != 1:\n",
    "            exec(code,env)\n",
    "        else:\n",
    "            env = local\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if record_time:\n",
    "            return ([\"SyntaxError\"]*n,[(1<<31)-1]*n)\n",
    "        return [\"SyntaxError\"]*n\n",
    "    if entry_point in env:\n",
    "        fn = env[entry_point]\n",
    "    else:\n",
    "        if record_time:\n",
    "            return ([\"NotImplemented\"]*n,[(1<<31)-1]*n)\n",
    "        return [\"NotImplemented\"]*n\n",
    "    @func_set_timeout(timeout)\n",
    "    def get(inp,record_time:bool=True):\n",
    "        try:\n",
    "            start = time.perf_counter_ns()\n",
    "            ret = fn(*inp) #if fn.__code__.co_argcount > 1 else fn(inp)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = (end-start)//1000_000\n",
    "        except Exception as e:\n",
    "            print(e,fn)\n",
    "            ret = \"SyntaxError\"\n",
    "            delta = (1<<31)-1\n",
    "        if isinstance(ret,NotImplementedError) or str(ret) == str(NotImplemented):\n",
    "            ret = \"NotImplemented\"\n",
    "        if record_time:\n",
    "            return (ret,delta)\n",
    "        return ret\n",
    "        \n",
    "    ret = []\n",
    "    rtime = []\n",
    "    for inp in inputs:\n",
    "        try:\n",
    "            res,delta = get(inp)\n",
    "            ret.append(res)\n",
    "            rtime.append(delta)\n",
    "        except FunctionTimedOut as e:\n",
    "            print(e)\n",
    "            ret.append(\"TimeLimitExceeding\")\n",
    "            rtime.append((1<<31)-1)\n",
    "    # try:\n",
    "    #     exec(code,None,local)\n",
    "    #     for key in local.keys:\n",
    "    #         del env[key]\n",
    "    # except:\n",
    "    #     print(\"del error\")\n",
    "    del env\n",
    "    if record_time:\n",
    "        return ret,rtime\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_execute(code:str,inputs:List,entry_point:str,record_time=True)->List:\n",
    "    env = {}\n",
    "    exec(code,None, env)\n",
    "    fn = env[entry_point]\n",
    "    ret = []\n",
    "    rtime = []\n",
    "    for i,inp in enumerate(inputs):\n",
    "        if record_time:\n",
    "            start = time.perf_counter_ns()\n",
    "            tmp = fn(*inp) #if fn.__code__.co_argcount > 1 else fn(inp)\n",
    "            end = time.perf_counter_ns()\n",
    "            ret.append(tmp)\n",
    "            rtime.append((end-start)//1000_000)\n",
    "        else:\n",
    "            tmp = fn(*inp) #if fn.__code__.co_argcount > 1 else fn(inp)\n",
    "            ret.append(tmp)\n",
    "    if record_time:\n",
    "        return ret,rtime\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardExecute(dataset:Union[str,Path],inplace = False,is_force_override = False):\n",
    "    def trust_execute(code:str,inputs:List,entry_point:str,record_time=True)->List:\n",
    "        env = {}\n",
    "        exec(code,None, env)\n",
    "        fn = env[entry_point]\n",
    "        ret = []\n",
    "        rtime = []\n",
    "        for i,inp in enumerate(inputs):\n",
    "            if record_time:\n",
    "                start = time.perf_counter_ns()\n",
    "                ret.append(fn(*inp))\n",
    "                end = time.perf_counter_ns()\n",
    "                rtime.append((end-start)//1000_000)\n",
    "            else:\n",
    "                ret.append(fn(*inp))\n",
    "        if record_time:\n",
    "            return ret,rtime\n",
    "        return ret\n",
    "    if isinstance(dataset, str):\n",
    "        dataset = Path(dataset)\n",
    "    if dataset.suffix == \".json\":\n",
    "        with open(dataset,'r') as f:\n",
    "            data_origin = json.load(f)\n",
    "    elif dataset.suffix == '.jsonl':\n",
    "        with open(dataset,'r') as f:\n",
    "            data_origin = {}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                data_origin[item['task_id']] = item\n",
    "    else:\n",
    "        raise Exception(f\"{dataset.suffix} is not supported\")\n",
    "    \"\"\"            \n",
    "        {\"{{task_id}}\":{\n",
    "            \"task_id\":str,\n",
    "            \"prompt\":str,\n",
    "            \"entry_point\":str,\n",
    "            \"canonical_solution\":str[code],\n",
    "            \"base_input\":List[input],\n",
    "            \"plus_input\":List[input],\n",
    "            \"base\":List[output],\n",
    "            \"base_time\":List[float],\n",
    "            \"plus\":List[output],\n",
    "            \"plus_time\":List[float],\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    save = dataset.stem + \"_with_output.json\"\n",
    "    if os.path.exists(os.path.join(dataset.parent, save)) and not is_force_override:\n",
    "        print(\"The output file already exists\")\n",
    "        return \n",
    "    for task_id,problem in data_origin.items():\n",
    "        oracle = data_origin[task_id]\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = trust_execute(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"base_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = trust_execute(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"plus_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "    if not inplace:\n",
    "        with open(os.path.join(dataset.parent,save),'w') as f:\n",
    "            json.dump(data_origin,f,indent=4)\n",
    "    else:\n",
    "        with open(dataset,'w') as f:\n",
    "            json.dump(data_origin,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeExecute(file_path:Union[str,Path],dataset:Union[str,Path],exec_save_path:Union[str,Path] = None,is_force_override = False)->None:\n",
    "    if isinstance(dataset, str):\n",
    "        dataset = Path(dataset)\n",
    "    if dataset.suffix == \".json\":\n",
    "        with open(dataset,'r') as f:\n",
    "            data_origin = json.load(f)\n",
    "    elif dataset.suffix == '.jsonl':\n",
    "        with open(dataset,'r') as f:\n",
    "            \"\"\"            \n",
    "                {\"{{task_id}}\":{\n",
    "                    \"task_id\":str,\n",
    "                    \"prompt\":str,\n",
    "                    \"entry_point\":str,\n",
    "                    \"canonical_solution\":str[code],\n",
    "                    \"base_input\":List[input],\n",
    "                    \"plus_input\":List[input],\n",
    "                    \"base\":List[output],\n",
    "                    \"base_time\":List[float],\n",
    "                    \"plus\":List[output],\n",
    "                    \"plus_time\":List[float],\n",
    "                    }\n",
    "                }\n",
    "            \"\"\"\n",
    "            data_origin = {}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                data_origin[item['task_id']] = item\n",
    "    else:\n",
    "        raise Exception(f\"{dataset.suffix} is not supported\")\n",
    "    \n",
    "    if isinstance(file_path, str):\n",
    "        file_path = Path(file_path)\n",
    "    if file_path.suffix == \".json\":\n",
    "        with open(file_path,'r') as f:\n",
    "            data_eval = json.load(f)\n",
    "    elif file_path.suffix == '.jsonl':\n",
    "        with open(file_path,'r') as f:\n",
    "            # {\"task_id\":\"solution\",}\n",
    "            data_eval = {}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                task_id = item['task_id']\n",
    "                data_eval[task_id] = item['solution'] if 'solution' in item else data_origin[task_id]['prompt']+item['completion']\n",
    "    else:\n",
    "        raise Exception(f\"{file_path.suffix} is not supported\")\n",
    "    \"\"\"\n",
    "        {\n",
    "            \"{{task_id}}\":{\n",
    "                \"info\":{\n",
    "                    \"task_id\":\"{{task_id}}\",\n",
    "                    \"prompt\":\"{{prompt}}\",\n",
    "                    \"entry_point\":\"{{entry_point}}\",\n",
    "                    \"canonical_solution\":\"{{canonical_solution}}\",\n",
    "                    \"base_input\":\"{{base_input}}\",\n",
    "                    \"plus_input\":\"{{plus_input}}\",\n",
    "                    \"atol\":float,\n",
    "                },\n",
    "                \"expected_output\":{\n",
    "                    \"base\":\"{{base}}\",\n",
    "                    \"base_time\":\"{{base_time}}\",\n",
    "                    \"plus\":\"{{plus}}\",\n",
    "                    \"plus_time\":\"{{plus_time}}\",\n",
    "                },\n",
    "                \"code_LLM\":\"{{solution}}\",\n",
    "                \"actual_output\":{\n",
    "                    \"base\":\"{{base}}\",\n",
    "                    \"base_time\":\"{{base_time}}\",\n",
    "                    \"plus\":\"{{plus}}\",\n",
    "                    \"plus_time\":\"{{plus_time}}\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    save = file_path.stem + \"_execute_results.json\"\n",
    "    if not exec_save_path:\n",
    "        parent = file_path.parent\n",
    "    else:\n",
    "        if isinstance(exec_save_path, str):\n",
    "            exec_save_path = Path(exec_save_path)\n",
    "        parent = exec_save_path if os.path.isdir(exec_save_path) else exec_save_path.parent\n",
    "    if os.path.exists(os.path.join(parent,save)) and not is_force_override:\n",
    "        print(\"The executeResults file already exists\")\n",
    "        return\n",
    "    for task_id,problem in data_origin.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = problem['base'],problem['base_time']\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = problem['plus'],problem['plus_time']\n",
    "        if task_id not in output:\n",
    "            output[task_id] = {}\n",
    "        output[task_id]['info'] = problem\n",
    "        output[task_id][\"expected_output\"] = oracle\n",
    "    for task_id,solution in data_eval.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = execute(\n",
    "            solution,\n",
    "            data_origin[task_id][\"base_input\"],\n",
    "            data_origin[task_id][\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = execute(\n",
    "            solution,\n",
    "            data_origin[task_id][\"plus_input\"],\n",
    "            data_origin[task_id][\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "        if task_id not in output:\n",
    "            output[task_id] = {}\n",
    "        output[task_id]['code_LLM'] = solution\n",
    "        output[task_id][\"actual_output\"] = oracle\n",
    "    with open(os.path.join(parent,save), \"w\") as f:\n",
    "        json.dump(output, f, indent=4,)\n",
    "    print(save)\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeEvaluate(exec_file:Union[str,Path],eval_save_path:Union[str,Path] = None,is_force_override:bool = False):\n",
    "    def equal(exp:List,actual:List,atol = 0)->List[bool]:\n",
    "        ret_correct = []\n",
    "        if is_floats(exp):\n",
    "            if atol == 0:\n",
    "                atol = 1e-6\n",
    "            for i in range(len(exp)):\n",
    "                if isinstance(actual[i],str) or not np.isclose(exp[i],actual[i],atol=atol):\n",
    "                    ret_correct.append(False)\n",
    "                else:\n",
    "                    ret_correct.append(True)\n",
    "        else:\n",
    "            for i in range(len(exp)):\n",
    "                if actual[i] in [\"NotImplemented\",\"TimeLimitExceeding\",\"SyntaxError\"] or actual[i] != exp[i]:\n",
    "                    ret_correct.append(False)\n",
    "                else:\n",
    "                    ret_correct.append(True)\n",
    "        return ret_correct\n",
    "    if isinstance(exec_file, str):\n",
    "        exec_file = Path(exec_file)\n",
    "    if exec_file.suffix != \".json\":\n",
    "        raise Exception(f\"{exec_file.suffix} is not supported\")\n",
    "    with open(exec_file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    if not eval_save_path:\n",
    "        parent = exec_file.parent\n",
    "    else:\n",
    "        if isinstance(eval_save_path,str):\n",
    "            eval_save_path = Path(eval_save_path)\n",
    "        parent = eval_save_path if os.path.isdir(eval_save_path) else eval_save_path.parent\n",
    "            \n",
    "    save = exec_file.stem.replace('_execute_results','') + \"_eval_results.json\"\n",
    "    if os.path.exists(os.path.join(parent,save)) and not is_force_override:\n",
    "        print(\"The evalResults file already exists\")\n",
    "        return \n",
    "    eval_result = {}\n",
    "    \"\"\"\n",
    "        {\n",
    "            \"percision\":{\n",
    "                \"base\":float,\n",
    "                \"plus\":float,\n",
    "                \"mean\":float\n",
    "            },\n",
    "            \"AC\":int,\n",
    "            \"PASS\":float,\n",
    "            \"syntax_error_ratio\":{\n",
    "                \"base\":float,\n",
    "                \"plus\":float\n",
    "                \"mean\":float\n",
    "            },\n",
    "            \"accuracy_list\":{\n",
    "                \"base_accuracy_list\":List[float],\n",
    "                \"plus_accuracy_list\":List[float]\n",
    "            },\n",
    "            \"syntax_error_list\":{\n",
    "                \"base_syntax_error_list\":List[bool],\n",
    "                \"plus_syntax_error_list\":List[bool]\n",
    "            },\n",
    "            \"notImplemented_ratio_list\":List[float],\n",
    "            \"{{task_id}}\":{\n",
    "                \"base_eval\":List[bool],\n",
    "                \"base_num\":int,\n",
    "                \"base_AC\":int,\n",
    "                \"base_TLE\":int,\n",
    "                \"base_accuracy\":float,\n",
    "                \"base_exp_time\":List[time],\n",
    "                \"base_actual_time\":List[time],\n",
    "                \n",
    "                \"plus_eval\":List[bool],\n",
    "                \"plus_num\":int,\n",
    "                \"plus_AC\":int,\n",
    "                \"plus_TLE\":int,\n",
    "                \"plus_accuracy\":float,\n",
    "                \"plus_exp_time\":List[time],\n",
    "                \"plus_actual_time\":List[bool],\n",
    "                \n",
    "                \"accuracy\":float\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    eval_result['percision'] = {'base':0,'plus':0,\"mean\":0}\n",
    "    eval_result['AC'] = 0\n",
    "    eval_result['PASS'] = 0\n",
    "    AC_num = 0\n",
    "    eval_result['syntax_error_ratio'] = {'base':0,\"plus\":0,\"mean\":0}\n",
    "    base_accuracy_list = []\n",
    "    plus_accuracy_list = []\n",
    "    accuracy_list = {\"base_accuracy_list\":base_accuracy_list,\"plus_accuracy_list\":plus_accuracy_list}\n",
    "    eval_result['accuracy_list'] = accuracy_list\n",
    "    base_syntax_error_list = []\n",
    "    plus_syntax_error_list = []\n",
    "    eval_result['syntax_error_list'] ={ \"base_syntax_error_list\":base_syntax_error_list,\"plus_syntax_error_list\":plus_syntax_error_list}\n",
    "    notImplemented_list = []\n",
    "    eval_result['notImplemented_ratio_list'] = notImplemented_list\n",
    "    for task_id,eval in data.items():\n",
    "        oracle = {}\n",
    "        if 'atol' in eval['info']:\n",
    "            atol = eval['info']['atol']\n",
    "        else:\n",
    "            atol = 0\n",
    "        base_exp = eval['expected_output']['base']\n",
    "        base_actual = eval['actual_output']['base']\n",
    "        base_correct = equal(base_exp, base_actual,atol)\n",
    "        \n",
    "        oracle['base_num'] = len(base_correct)\n",
    "        oracle['base_AC'] = sum(base_correct)\n",
    "        oracle['base_TLE'] = base_actual.count(\"TimeLimitExceeding\")\n",
    "        oracle['base_SE'] = base_actual.count(\"SyntaxError\")\n",
    "        oracle['base_NoImplemented'] = base_actual.count(\"NotImplemented\")\n",
    "        oracle['base_accuracy'] = np.mean(base_correct)\n",
    "        base_accuracy_list.append(oracle['base_accuracy'])\n",
    "        base_syntax_error_list.append(oracle['base_SE']/oracle['base_num'])\n",
    "        \n",
    "        plus_exp = eval['expected_output']['plus']\n",
    "        plus_actual = eval['actual_output']['plus']\n",
    "        plus_correct = equal(plus_exp,plus_actual,atol)\n",
    "        \n",
    "        oracle['plus_num'] = len(plus_correct)\n",
    "        oracle['plus_AC'] = sum(plus_correct)\n",
    "        oracle['plus_TLE'] = plus_actual.count(\"TimeLimitExceeding\")\n",
    "        oracle['plus_SE'] = plus_actual.count(\"SyntaxError\")\n",
    "        oracle['plus_NoImplemented'] = plus_actual.count(\"NotImplemented\")\n",
    "        oracle['plus_accuracy'] = np.mean(plus_correct)\n",
    "        plus_accuracy_list.append(oracle['plus_accuracy'])\n",
    "        plus_syntax_error_list.append(oracle['plus_SE']/oracle['plus_num'])\n",
    "        \n",
    "        oracle['accuracy'] = np.mean(base_correct+plus_correct)\n",
    "        if oracle['accuracy'] == 1:\n",
    "            AC_num += 1\n",
    "        \n",
    "        oracle['base_exp_time'] = eval['expected_output']['base_time']\n",
    "        oracle['base_actual_time'] = eval['actual_output']['base_time']\n",
    "        oracle['plus_exp_time'] = eval['expected_output']['plus_time']\n",
    "        oracle['plus_actual_time'] = eval['actual_output']['plus_time']\n",
    "        \n",
    "        notImplemented_list.append((oracle['base_NoImplemented']+oracle['plus_NoImplemented'])/(oracle['base_num']+oracle['plus_num']))\n",
    "        \n",
    "        eval_result[task_id] = oracle\n",
    "    eval_result['percision']['base'] = np.mean(base_accuracy_list)\n",
    "    eval_result['percision']['plus'] = np.mean(plus_accuracy_list)\n",
    "    eval_result['percision']['mean'] = np.mean(base_accuracy_list+plus_accuracy_list)\n",
    "    eval_result['syntax_error_ratio']['base'] = np.mean(base_syntax_error_list)\n",
    "    eval_result['syntax_error_ratio']['plus'] = np.mean(plus_syntax_error_list)\n",
    "    eval_result['syntax_error_ratio']['mean'] = np.mean(base_syntax_error_list+plus_syntax_error_list)\n",
    "    eval_result['AC'] = AC_num\n",
    "    eval_result['PASS'] = AC_num/len(data)\n",
    "    with open(os.path.join(parent,save), \"w\") as f:\n",
    "        json.dump(eval_result, f, indent=4,)\n",
    "    print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = './data/codeEval/code_sanitize'\n",
    "files = ['baichuan2-13b-base.jsonl', 'baichuan2-13b-chat.jsonl', 'baichuan2-7b-chat.jsonl', 'infini-megrez-7b.jsonl', 'llama-2-13b-chat.jsonl', 'llama-2-70b.jsonl', 'llama-2-7b-chat.jsonl', 'qwen-14b-chat.jsonl', 'qwen-72b-chat.jsonl', 'qwen-72b.jsonl', 'qwen-7b-chat.jsonl', 'qwen1.5-14b-chat.jsonl', 'qwen1.5-72b.jsonl', 'qwen1.5-7b-chat.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dir,subdir,files in os.walk('./data/codeEval/code_sanitize'):\n",
    "#     for file in files:\n",
    "#         codeExecute(os.path.join(dir,file),\"./data/codeEval/data_with_output.json\",exec_save_path = './data/codeEval/code_execute',is_force_override=True)\n",
    "#         codeEvaluate(os.path.join('./data/codeEval/code_execute',file.replace(\".jsonl\",\"_execute_results.json\")),eval_save_path = './data/codeEval/code_eval',is_force_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/codeEval/data_with_output.json') as f:\n",
    "    data_o = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_o['HumanEval/6']['base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('./data/codeEval/code_sanitize/qwen-72b.jsonl', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data.append(json.loads(line))\n",
    "data[1]#['HumanEval/6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {}\n",
    "exec(data[1]['solution'],None,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_o['HumanEval/6']['base_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = env['parse_nested_parens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in data_o['HumanEval/6']['base_input']:\n",
    "    print(fn(*inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {}\n",
    "local = {}\n",
    "code = \"def calling():\\n    return called()\\ndef called():\\n    return 'success'\"\n",
    "exec(code,env,local)\n",
    "# fn = env['calling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardExecute('./data/codeEval/data.json',is_force_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "# codeExecute(os.path.join(dir,files[i]),\"./data/codeEval/data_with_output.json\",exec_save_path = './data/codeEval/code_execute',is_force_override=True)\n",
    "    codeEvaluate(os.path.join('./data/codeEval/code_execute',files[i].replace(\".jsonl\",\"_execute_results.json\")),eval_save_path = './data/codeEval/code_eval',is_force_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal(exp:List,actual:List,atol = 0)->List[bool]:\n",
    "    ret_correct = []\n",
    "    if is_floats(exp):\n",
    "        if atol == 0:\n",
    "            atol = 1e-6\n",
    "        for i in range(len(exp)):\n",
    "            if isinstance(actual[i],str) or not np.isclose(exp[i],actual[i],atol=atol):\n",
    "                ret_correct.append(False)\n",
    "            else:\n",
    "                print(\"exp:\\t\",exp[i])\n",
    "                print(\"actual:\\t\",actual[i])\n",
    "                print('='*70)\n",
    "                ret_correct.append(True)\n",
    "    else:\n",
    "        for i in range(len(exp)):\n",
    "            if actual[i] in [\"NotImplemented\",\"TimeLimitExceeding\",\"SyntaxError\"] or actual[i] != exp[i]:\n",
    "                ret_correct.append(False)\n",
    "            else:\n",
    "                print(\"exp:\\t\",exp[i])\n",
    "                print(\"actual:\\t\",actual[i])\n",
    "                print('='*70)\n",
    "                ret_correct.append(True)\n",
    "    return ret_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/codeEval/code_execute/qwen-72b_execute_results.json') as f:\n",
    "    data = json.load(f)\n",
    "for task_id ,problem in data.items():\n",
    "    exp = data[task_id]['expected_output']['plus']\n",
    "    actual = data[task_id]['actual_output']['plus']\n",
    "    equal(exp,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = {'accuracy':{},\"pass\":{},\"AC\":{}}\n",
    "for dir,subdir,files in os.walk('./data/codeEval/code_eval'):\n",
    "    for file in files:\n",
    "        model = file.replace('_eval_results.json','')\n",
    "        with open(os.path.join(dir,file)) as f:\n",
    "            data = json.load(f)\n",
    "        data_eval['accuracy'][model] = data['percision']['mean']\n",
    "        data_eval['pass'][model] = data['PASS']\n",
    "        data_eval['AC'][model] = data['AC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_eval).sort_values(by=\"AC\",ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

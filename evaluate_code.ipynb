{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,os,datetime,time,string,sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "from typing import *\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import traceback,random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evalplus.sanitize,evalplus.syncheck\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "import decimal\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SNRXQ5WMAQ2FREZH1HJ08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接无问芯穷的API列表\n",
    "INFINI_API_List = [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SP9PE51MESBZ17QHEKZEW",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = [\n",
    "    'llama-3-70b-instruct',\n",
    "    'llama-3-8b-instruct',\n",
    "\n",
    "    'infini-megrez-7b',\n",
    "    'llama-2-7b-chat',\n",
    "    'llama-2-13b-chat',\n",
    "    'llama-2-70b-chat',\n",
    "    'llama-2-70b',\n",
    "    'baichuan2-7b-chat',\n",
    "    'baichuan2-13b-chat',\n",
    "    'baichuan2-13b-base',\n",
    "    'chatglm3',\n",
    "    'chatglm2-6b',\n",
    "    'chatglm2-6b-32k',\n",
    "    'chatglm3-6b',\n",
    "    'chatglm3-6b-32k',\n",
    "    'chatglm3-6b-base',\n",
    "    'qwen-7b-chat',\n",
    "    'qwen-14b-chat',\n",
    "    'qwen-72b-chat',\n",
    "    'qwen-72b',\n",
    "    'qwen1.5-7b-chat',\n",
    "    'qwen1.5-14b-chat',\n",
    "    'qwen1.5-72b-chat',\n",
    "    'qwen1.5-72b',\n",
    "]\n",
    "modelProfileDict = {\n",
    "    # 'llama-3-70b-instruct':\"Llama3系列是由Meta开发的Llama系列全新的第三代版本，包含一系列预训练和指令调优的文本生成式模型。Llama3基于优化后的Transformer架构，预训练过程中使用了超过15T tokens的数据，调优后的模型使用SFT和RLHF，以更好地贴合人类对可用性和安全性的偏好。Llama3-70b-Instruct是此系列里，700亿参数的指令调优的模型，针对对话场景用例进行了优化，并在常见的行业基准测试中超越了许多可用的开源聊天模型。Llama3-70b-Instruct支持模型上下文至8k tokens，该模型的数据的知识截止日期为2023年12月。\",\n",
    "    # 'llama-3-8b-instruct':\"Llama3系列是由Meta开发的Llama系列全新的第三代版本，包含一系列预训练和指令调优的文本生成式模型。Llama3基于优化后的Transformer架构，预训练过程中使用了超过15T tokens的数据，调优后的模型使用SFT和RLHF，以更好地贴合人类对可用性和安全性的偏好。Llama3-8b-Instruct是此系列里，80亿参数的指令调优的模型，针对对话场景用例进行了优化，并在常见的行业基准测试中超越了许多可用的开源聊天模型。Llama3-8b-Instruct支持模型上下文至8k tokens，该模型的数据的知识截止日期为2023年3月。\",\n",
    "    # 'chatglm3':\"ChatGLM3是智谱AI与清华KEG实验室发布的闭源模型，支持 8K 上下文，经过海量中英标识符的预训练与人类偏好对齐训练，相比一代模型在 MMLU、C-Eval、GSM8K 分别取得了16%、36%、280%的提升，并登顶中文任务榜单C-Eval。适用于对知识量、推理能力、创造力要求较高的场景，比如广告文案、小说写作、知识类写作、代码生成等。\",\n",
    "    # 'chatglm2-6b':\"ChatGLM2-6b 是由智谱开发的 ChatGLM 系列的第二代版本，支持中英双语的60亿参数规模的开源模型。在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，在 MMLU、C-Eval、GSM8K、BBH等主流学术数据集上，都得到了显著的性能提升，并通过基于 FlashAttention 技术，将对话模型的上下文长度（Context Length）提升至 8k tokens，允许更多轮次的对话。\",\n",
    "    # 'chatglm2-6b-32k':\"ChatGLM2-6b 是由智谱开发的 ChatGLM 系列的第二代版本，支持中英双语的60亿参数规模的开源模型。相较于ChatGLM2-6B，ChatGLM2-6b-32k支持更长的模型上下文至32k tokens。\",\n",
    "    # 'chatglm3-6b':\"ChatGLM3-6b 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源模型。ChatGLM3采用了全新设计的 Prompt 格式，并原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。模型支持 8k tokens上下文。\",\n",
    "    # 'chatglm3-6b-32k':\"ChatGLM3-6b 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源模型。相较于ChatGLM之前系列的模型，ChatGLM3采用了更多样的训练数据，并原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。ChatGLM3-6b-32k在ChatGLM3-6b 基础上进一步强化了对于长文本的理解能力，能够更好的处理最多32k tokens长度的上下文。\",\n",
    "    # 'chatglm3-6b-base':\"ChatGLM3-6b-base 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源的基础模型。ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。基础模型更适合于复杂场景的微调后使用，该基模型支持32k tokens上下文。\",\n",
    "    'infini-megrez-7b':\"由无问芯穹公司自主研发的70亿参数大语言模型。在逻辑推理、对话能力等方面有优秀的性能表现。配合无问芯穹自研高效推理引擎，同时支持Nvidia和AMD的GPU，具备更快的推理速度，在性能表现方面更上一层楼。\",\n",
    "    'llama-2-7b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-7b-chat是其中70亿的主流参数大小的模型，适用于chat场景，更擅长英文相关的内容。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-13b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-7b-chat是其中70亿的主流参数大小的模型，适用于chat场景，更擅长英文相关的内容。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-70b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-70b-chat是其中700亿参数的大模型，适用于chat场景，更擅长英文相关的内容，相较该系列里其他规模的的模型，有更强的综合能力。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-70b':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-70b-base是其中700亿参数的基础大模型，适用于通用语言任务场景，更擅长英文相关的内容，相较该系列里其他规模的的模型，有更强的综合能力。模型支持 4k tokens上下文。\",\n",
    "    'baichuan2-7b-chat':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-7b-chat是130亿参数规模用于对话的模型，在C-Eval、MMLU、CMMLU等主流评测数据集上都有不俗的表现。该基模型支持4k tokens上下文。\",\n",
    "    'baichuan2-13b-chat':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-13b-chat是130亿参数规模用于对话的模型，在C-Eval、MMLU、CMMLU等主流评测数据集上都有不俗的表现。该基模型支持8k tokens上下文。\",\n",
    "    'baichuan2-13b-base':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-13b-base是130亿参数规模的基础模型，适用于通用对话和文本续写，较chat模型更适合于复杂场景的微调后使用。该基模型支持4k tokens上下文。\",\n",
    "    'qwen-7b-chat':\"通义千问-7B-chat（Qwen-7B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的70亿参数规模的大语言模型。相较于Qwen-7B-Base模型，Qwen-7B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 8k tokens上下文。\",\n",
    "    'qwen-14b-chat':\"通义千问-14B-chat（Qwen-14B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的140亿参数规模的大语言模型。相较于Qwen-14B-Base模型，Qwen-14B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 8k tokens上下文。\",\n",
    "    'qwen-72b-chat':\"通义千问-72B-chat（Qwen-72B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的720亿参数规模的大语言模型。相较于Qwen-72B-Base模型，Qwen-72B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 32k tokens上下文。\",\n",
    "    'qwen-72b':\"通义千问-72B（Qwen-72B）是阿里云研发的通义千问大模型系列的720亿参数规模的模型。Qwen-72B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。模型支持 32k tokens上下文。\",\n",
    "    'qwen1.5-7b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-7b-chat是其中专用于chat场景的70亿参数的主流大小模型。\",\n",
    "    'qwen1.5-14b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-14b-chat是其中专用于chat场景的140亿参数的主流大小模型。\",\n",
    "    'qwen1.5-72b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-72b-chat是其中专用于chat场景的720亿参数的大模型。\",\n",
    "    'qwen1.5-72b':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-72b-base是其中的720亿参数的基础大模型，适合多种场景的使用。\",\n",
    "}\n",
    "evaluateModelList = ['qwen1.5-72b-chat',]\n",
    "# 测评模型列表\n",
    "answerModelList = [    \n",
    "    'baichuan2-7b-chat',\n",
    "    'baichuan2-13b-chat',\n",
    "    'baichuan2-13b-base',\n",
    "    'infini-megrez-7b', \n",
    "    'qwen-7b-chat',\n",
    "    'qwen-14b-chat',\n",
    "    'qwen-72b-chat',\n",
    "    'qwen-72b',\n",
    "    'qwen1.5-7b-chat',\n",
    "    'qwen1.5-14b-chat',\n",
    "    'qwen1.5-72b',\n",
    "    'llama-2-70b',\n",
    "    'llama-2-7b-chat',\n",
    "    'llama-2-13b-chat',\n",
    "    'llama-2-70b-chat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 'llama-2-13b-chat'\n",
    "print(modelProfileDict[item],sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SPW6ABQX9J20163521X4Y",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgeList = ['Knowledge Memorization','Knowledge Understanding','Knowledge Applying','Knowledge Creating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Model Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SQ4DW0S67QH7AYG9VNJYV",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "def LLMCompletions(prompt:str,modelName:str = \"infini-megrez-7b\",INFINI_API_List:List[str] = [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"],returnContent:bool = True,**kwargs)->str:\n",
    "    \"\"\"_summary_\n",
    "        invoke the model `modelName` with the `prompt` and configuration in kwargs to get the reply\n",
    "    Args:\n",
    "        prompt (str): question profile\n",
    "        modelName (str, optional): the model name that will be called. Defaults to \"infini-megrez-7b\".\n",
    "        INFINI_API_List (List[str], optional): api_list. Defaults to [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"].\n",
    "        returnContent (bool, optional): whether return the model reply string directly or not. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: return the model reply string \n",
    "    \"\"\"\n",
    "    global index\n",
    "    url = \"https://cloud.infini-ai.com/maas/\"+modelName+\"/nvidia/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": modelName,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else 0.7,\n",
    "        \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else 1,\n",
    "        \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else -1,\n",
    "        \"n\": kwargs['n'] if 'n' in kwargs else 1,\n",
    "        \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else None,\n",
    "        \"stop\": kwargs['stop'] if 'stop' in kwargs else None,\n",
    "        \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else 0,\n",
    "        \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else 0\n",
    "    }\n",
    "    idx = 0\n",
    "    while idx < len(INFINI_API_List):\n",
    "        headers = {\n",
    "                'Content-Type': \"application/json\",\n",
    "                'Accept': \"*/*\",\n",
    "                'Authorization': \"Bearer \"+INFINI_API_List[index%len(INFINI_API_List)],\n",
    "        } \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            response.encoding = 'utf-8'\n",
    "            data = response.json()\n",
    "            content = data['choices'][0]['message']['content']\n",
    "            if isinstance(content,str):\n",
    "                content = content.replace(',\\n}','\\n}')\n",
    "                content = content.replace(']\\n}',']}')\n",
    "                content = content.replace('\\\\','\\\\\\\\')\n",
    "            if returnContent:\n",
    "                return content\n",
    "            try:\n",
    "                content = json.loads(content)\n",
    "            except:\n",
    "                pass\n",
    "            data['choices'][0]['message']['content'] = content\n",
    "            if isinstance(content,str):\n",
    "                return content\n",
    "            \n",
    "            return json.dumps(data['choices'][0]['message']['content'])\n",
    "        elif response.status_code//100 == 4:\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(response.status_code)\n",
    "            try:\n",
    "                print(response.json())\n",
    "            except:\n",
    "                pass\n",
    "        index = (index + 1) % len(INFINI_API_List)\n",
    "        idx += 1\n",
    "    return \"Cannot connect to the model \"+modelName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the first time to get the model reply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SQ4DW6YA9S51SVHR3DK87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_ins = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'time':-1\n",
    "}\n",
    "def get_LLM_Reply_KoLA(filepath:Union[str,Path],savePath:Union[str,Path],errorPath:Union[str,Path],fileName:str = None,)->None:\n",
    "    \"\"\"_summary_\n",
    "        get evaluation questions from `filepath` and save the reply of the model in answerModelList to `savePath` ,and error item to `errorPath`\n",
    "    Args:\n",
    "        filepath (Union[str,Path]): _description_\n",
    "        savePath (Union[str,Path]): the path to save LLM reply\n",
    "        errorPath (Union[str,Path]): the path to save error items\n",
    "        fileName (str, optional): _description_. Defaults to None.\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    instructions = data['adapter_spec']['instructions']\n",
    "    questionList = data['request_states']\n",
    "    errorItem = []\n",
    "    if not fileName:\n",
    "        fileName = Path(filepath).name\n",
    "    save = {'fileName':fileName,'class':knowledgeList[int(fileName[0])-1],'data':[]}\n",
    "    for index,item in enumerate(questionList):\n",
    "        if 'id' in item['instance']:\n",
    "            data_format_ins['id'] = item['instance']['id']\n",
    "        else:\n",
    "            data_format_ins['id'] = fileName+'%04d'% index\n",
    "        prompt = instructions+'\\n'+item['instance']['input']['text']\n",
    "        data_format_ins['input'] = prompt\n",
    "        print(prompt)\n",
    "        if item['instance']['references'][0]['tags'][0] == 'correct':\n",
    "            data_format_ins['expected_output'] = item['instance']['references'][0]['output']['text']\n",
    "        else:\n",
    "            data_format_ins['expected_output'] = None\n",
    "        for model in answerModelList:\n",
    "            data_format_ins['AnswerModel'] = model\n",
    "            print(model)\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                \n",
    "                errorItem.append({'fileName':fileName,'id':data_format_ins['id'],\"AnswerModel\":model,\"input\":prompt,\"expected_output\":data_format_ins['expected_output']})\n",
    "                continue\n",
    "            print(idx,delta,actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "            print('*'*70)\n",
    "        print('+'*70)\n",
    "    errorItemFinal = []\n",
    "    while errorItem:\n",
    "        item = errorItem.pop()\n",
    "        data_format_ins['id'] = item['id']\n",
    "        model = item['AnswerModel']\n",
    "        data_format_ins['AnswerModel'] = model\n",
    "        prompt = item['input']\n",
    "        data_format_ins['input'] = prompt\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        start = time.perf_counter_ns()\n",
    "        actual_output =  LLMCompletions(prompt,modelName=model)\n",
    "        end = time.perf_counter_ns()\n",
    "        delta = end-start\n",
    "        idx = 0\n",
    "        while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "            print('\\t'+str(idx)+'\\ttest')\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx += 1\n",
    "        if actual_output == \"Cannot connect to the model \"+model:\n",
    "            errorItemFinal.append(item)\n",
    "            print(\"[error]:\\t\"+str(errorItemFinal[-1]))\n",
    "            continue\n",
    "        print(idx,delta,actual_output,sep='\\t')\n",
    "        data_format_ins['actual_output'] = actual_output\n",
    "        data_format_ins['time'] = delta\n",
    "        save['data'].append(data_format_ins.copy())\n",
    "    with open(savePath,'w',encoding='utf-8') as out:\n",
    "        json.dump(save,out)\n",
    "    if errorItemFinal:\n",
    "        error = {'fileName':fileName,'class':knowledgeList[int(fileName[0])-1],'data':[]}\n",
    "        for i in errorItemFinal:\n",
    "            error['data'].append(i)\n",
    "        with open(errorPath,'w',encoding='utf-8') as out:\n",
    "            json.dump(error,out)\n",
    "    print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SQ4DW77FZHQ2BVP5QVCTK",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirName,subDirName,fileNames in os.walk('data/KoLA/origin'):\n",
    "    print(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SQ4DWDX2K27D72Z71CY1C",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames =['1-1_2_high_freq_ent_sample.json', '1-2_1_low_freq_ent_sample.json', '1-3_r_1_simple_sample_sample.json', '2-1_COPEN++csj_sample.json', '2-2_COPEN++cpj_sample.json', '2-3_COPEN++cic_sample.json', '2-4_FewNERD++inter_sample.json', '2-4_FewNERD++intra_sample.json', '2-4_FewNERD++supervised_sample.json', '2-5_DocRED_sample.json', '2-6_MAVEN_sample.json', '2-7_MAVEN-ERE_sample.json', '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-3_musique_sample.json', '3-4_kqapro_sample.json', '3-5_KoRC++ood_sample.json', '3-6_r_KoRC++ood_sample.json', '4-1_without_triples_sample.json', '4-1_with_triples_sample.json', '4-2_r_without_triples_sample.json', '4-2_r_with_triples_sample.json']\n",
    "dirName = 'data/KoLA/origin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3SQ4DW1T2MGJ6MHHDZ3G41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName in fileNames:\n",
    "    get_LLM_Reply_KoLA(os.path.join(dirName, fileName),'./data/KoLA/save/'+fileName,'./data/KoLA/error/'+fileName.replace(\".json\",\"\")+'Error'+'.json',fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### handle error items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3TVD0KFQFHV3A88AYF7Y37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_ins = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'time':-1\n",
    "}\n",
    "def joinErrorToData(errorFile:Union[str,Path],saveFile:Union[str,Path])->None:\n",
    "    \"\"\"rerun the error items in `saveFile` and append results to 'saveFile'\n",
    "\n",
    "    Args:\n",
    "        errorFile (Union[str,Path]): \n",
    "        saveFile (Union[str,Path]): \n",
    "    \"\"\"\n",
    "    with open(errorFile, 'r') as ef:\n",
    "        data_ef = json.load(ef)\n",
    "    fileName = data_ef['fileName']\n",
    "    with open(saveFile) as sf:\n",
    "        data_sf = json.load(sf)\n",
    "    if fileName != data_sf['fileName']:\n",
    "        print('FileName not match')\n",
    "        return \n",
    "    if not data_ef['data']:\n",
    "        print('The Errors of this ErrorFile all have been solved!')\n",
    "        return \n",
    "    errorItem = []\n",
    "    for item in data_ef['data']:\n",
    "        data_format_ins['id'] = item['id']\n",
    "        model = item['AnswerModel']\n",
    "        data_format_ins['AnswerModel'] = model\n",
    "        prompt = item['input']\n",
    "        data_format_ins['input'] = prompt\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        start = time.perf_counter_ns()\n",
    "        actual_output =  LLMCompletions(prompt,modelName=model,INFINI_API=INFINI_API_2)\n",
    "        end = time.perf_counter_ns()\n",
    "        delta = end-start\n",
    "        idx = 0\n",
    "        while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,INFINI_API=INFINI_API_2)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx += 1\n",
    "        if actual_output == \"Cannot connect to the model \"+model:\n",
    "            errorItem.append(item)\n",
    "            print(\"[error]:\\t\"+str(errorItem[-1]))\n",
    "            continue\n",
    "        print(actual_output)\n",
    "        data_format_ins['time'] = delta\n",
    "        data_format_ins['actual_output'] = actual_output\n",
    "        data_sf['data'].append(data_format_ins.copy())\n",
    "    data_ef['data'] = errorItem\n",
    "    with open(saveFile,'w') as saveF:\n",
    "        json.dump(data_sf,saveF)\n",
    "    with open(errorFile,'w') as error:\n",
    "        json.dump(data_ef,error)\n",
    "    if errorItem:\n",
    "        print(\"There are still some errors! \")\n",
    "    else:\n",
    "        print('The Errors of this ErrorFile all have been solved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3V2K9NM7NFDB4JMJ0S5MTV",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joinErrorToData('./data/KoLA/error/4-2_r_with_triples_sampleError.json','./data/KoLA/save/4-2_r_with_triples_sample.json')\n",
    "for dir,subdir,files in os.walk(\"./data/KoLA/error\"):\n",
    "    for file in files:\n",
    "        joinErrorToData(os.path.join(dir,file),os.path.join('./data/KoLA/save',file.replace(\"Error.json\",\".json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3TVVKZ2KS6HJYANFCPJ066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDeprecatedModel(filePath:Union[str,Path]):\n",
    "    \"\"\"remove the deprecated evaluated model  from the `filePath`\n",
    "\n",
    "    Args:\n",
    "        filePath (Union[str,Path]): \n",
    "    \"\"\"\n",
    "    with open(filePath) as f:\n",
    "        data = json.load(f)\n",
    "    new_data = []\n",
    "    for item in data['data']:\n",
    "        if item['AnswerModel']  in answerModelList:\n",
    "            new_data.append(item)\n",
    "    data['data'] = new_data[:]\n",
    "    with open(filePath,'w') as f:\n",
    "        json.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3V8Y902GFJG2SJPS29K4WT",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeDeprecatedModel('./data/1-2_1_low_freq_ent_sample.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VDTZQFJN1CHAPJNJJV3T9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric,FaithfulnessMetric,HallucinationMetric,BaseMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VCEAFS2WN25CMGPCXH240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatLLM(LLM):\n",
    "    @property\n",
    "    def modelName(self)->str:\n",
    "        return \"qwen1.5-72b-chat\"\n",
    "    @property\n",
    "    def INFINI_API_List(self)->List[str]:\n",
    "        return [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"]\n",
    "    @property\n",
    "    def temperature(self)->float:\n",
    "        return 0.7\n",
    "    @property\n",
    "    def top_p(self)->float:\n",
    "        return 0.1\n",
    "    @property\n",
    "    def top_k(self)->int:\n",
    "        return -1\n",
    "    @property\n",
    "    def n(self)->int:\n",
    "        return 1\n",
    "    @property\n",
    "    def max_tokens(self)->int:\n",
    "        return None\n",
    "    @property\n",
    "    def stop(self)->Optional[List[str]]:\n",
    "        return None\n",
    "    @property\n",
    "    def presence_penalty(self)->float:\n",
    "        return 0\n",
    "    @property\n",
    "    def frequency_penalty(self)->float:\n",
    "        return 0\n",
    "    def getHeader(self,index_api):  \n",
    "        headers = {\n",
    "            'Content-Type': \"application/json\",\n",
    "            'Accept': \"*/*\",\n",
    "            'Authorization': \"Bearer \"+self.INFINI_API_List[index_api%len(self.INFINI_API_List)],\n",
    "        }\n",
    "        return headers\n",
    "    @property\n",
    "    def _llm_type(self)->str:\n",
    "        return \"ChatLLM\"\n",
    "    @property\n",
    "    def _identifying_params(self)->Mapping[str,Any]:\n",
    "        _param_dict = {\n",
    "            \"modelName\":self.modelName,\n",
    "            \"INFINI_API\":self.getHeader(self.__fields__['index_api'] if 'index_api' in self.__fields__ else 0),\n",
    "            \"stream\":bool(self.stream),\n",
    "            \"temperature\":self.temperature,\n",
    "            \"top_p\":self.top_p,\n",
    "            \"top_k\":self.top_k,\n",
    "            \"n\":self.n,\n",
    "            \"max_tokens\":self.max_tokens,\n",
    "            \"stop\":self.stop,\n",
    "            \"presence_penalty\":self.presence_penalty,\n",
    "            \"frequency_penalty\":self.frequency_penalty,\n",
    "        }\n",
    "        return _param_dict\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]= None, run_manager= None,**kwargs: Any) -> str:\n",
    "        url = \"https://cloud.infini-ai.com/maas/\"+str(self.modelName)+\"/nvidia/chat/completions\"\n",
    "        payload = {\n",
    "            \"model\": \"string\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else self.temperature,\n",
    "            \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else self.top_p,\n",
    "            \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else self.top_k,\n",
    "            \"n\": kwargs['n'] if 'n' in kwargs else self.n,\n",
    "            \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else self.max_tokens,\n",
    "            \"stop\": stop if stop else self.stop,\n",
    "            \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else self.presence_penalty,\n",
    "            \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else self.frequency_penalty\n",
    "        }\n",
    "        index = 0\n",
    "        if 'index_api' not in self.__fields__:\n",
    "            self.__fields__['index_api'] = -1\n",
    "        index_api = self.__fields__['index_api']+1\n",
    "        length = len(self.INFINI_API_List)\n",
    "        while index < length:\n",
    "            response = requests.post(url, json=payload, headers=self.getHeader(index_api))\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                data = response.json()\n",
    "                print(\"response json success\")\n",
    "                content = data['choices'][0]['message']['content']\n",
    "                if isinstance(content,str):   \n",
    "                    content = content.replace(',\\n}','\\n}')\n",
    "                    content = content.replace(']\\n}',']}')\n",
    "                    if 'statements' in content:\n",
    "                        regex = re.compile('\\\"statements\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "                        matchStr =regex.search(content)\n",
    "                        if matchStr:\n",
    "                            content = '{'+matchStr.group()\n",
    "                    elif 'verdicts' in content:\n",
    "                        regex = re.compile('\\\"verdicts\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "                        matchStr =regex.search(content)\n",
    "                        if matchStr is not None:\n",
    "                            content ='{' +matchStr.group()\n",
    "                            regex = re.compile(\"\\\"reason\\\":(.*?)\\}\",re.DOTALL)\n",
    "                            matchStr = regex.findall(content)\n",
    "                            for string in matchStr:\n",
    "                                tmp = string.strip()[1:-1].replace('\"','\\\\\\\"')\n",
    "                                tmp = '\\\"'+tmp+\"\\\"\"\n",
    "                                content = content.replace(string,tmp)\n",
    "                if isinstance(content,str):\n",
    "                    return content\n",
    "                data['choices'][0]['message']['content'] = content\n",
    "                return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "            index += 1\n",
    "            index_api =  (index_api+1)%length\n",
    "            self.__fields__['index_api'] = index_api\n",
    "            print(response.status_code)\n",
    "            try:\n",
    "                print(response.json())\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "        return \"Cannot connect to the model \"+self.modelName\n",
    "    def setParameter(self,**kwargs):\n",
    "        self.temperature = kwargs[\"temperature\"] if \"temperature\" in kwargs else self.temperature\n",
    "        self.top_p = kwargs['top_p'] if 'top_p' in  kwargs else self.top_p\n",
    "        self.top_k = kwargs['top_k'] if 'top_k' in  kwargs else self.top_k\n",
    "        self.n = kwargs['n'] if 'n' in kwargs else self.n\n",
    "        self.max_tokens = kwargs['max_tokens'] if 'max_tokens' in kwargs else self.max_tokens\n",
    "        self.stop = kwargs['stop'] if 'stop' in kwargs else self.stop\n",
    "        self.presence_penalty = kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else self.presence_penalty\n",
    "        self.frequency_penalty = kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else self.frequency_penalty\n",
    "    \n",
    "\n",
    "class CustomLLM(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        # global path\n",
    "        chat_model = self.load_model()\n",
    "        ret = chat_model.invoke(prompt)\n",
    "        idx = 0\n",
    "        while ret == \"Cannot connect to the model \"+self.get_model_name() and idx<5:\n",
    "            time.sleep(5)\n",
    "            ret = chat_model.invoke(prompt)\n",
    "            idx += 1\n",
    "        # print(ret)\n",
    "        # path.append(ret)\n",
    "        return ret\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        try:\n",
    "            return self.model.modelName\n",
    "        except:\n",
    "            return \"CustomLLM\"\n",
    "custom_model = ChatLLM()\n",
    "evaluateModel = CustomLLM(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VCWG1JD3BFHP427AV69FB",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_format_example = {\n",
    "    'metric_metadata':{\n",
    "        'metric':None,\n",
    "        'threshold':0,\n",
    "        'success':True,\n",
    "        'score':0.8,\n",
    "        'reason':'',\n",
    "        'strictMode': False,\n",
    "        'evaluationModel': 'CustomLLM',\n",
    "        'evaluationCost': 0\n",
    "    },\n",
    "    'metric_configuration': {\n",
    "        'threshold': 0.5,\n",
    "        'evaluation_model': 'CustomLLM',\n",
    "        'strict_mode': False,\n",
    "        'include_reason': True\n",
    "    }\n",
    "}\n",
    "\n",
    "data_format_example = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'cached_metrics_data':[\n",
    "        {\n",
    "            'metric_metadata':{\n",
    "                'metric':None,\n",
    "                'success':True,\n",
    "                'score':0.8,\n",
    "                'reason':'',\n",
    "                'statements':'',\n",
    "                'verdicts':'',\n",
    "                'evaluationCost': 0\n",
    "            },\n",
    "            'metric_configuration': {\n",
    "                'threshold': 0.5,\n",
    "                'evaluation_model': 'CustomLLM',\n",
    "                'strict_mode': False,\n",
    "                'include_reason': True\n",
    "            }\n",
    "        },\n",
    "        metrics_format_example\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VG939F2WC2NHR9NKJ8TJK",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(filename:Union[str,Path],save_file:Union[str,Path],error_file:Union[str,Path],force_save:bool = False,metrics:List[BaseMetric] = [AnswerRelevancyMetric(threshold=0.5,model=evaluateModel,include_reason=True),HallucinationMetric(threshold=0.5,model=evaluateModel,include_reason=True)]):\n",
    "    \"\"\"_summary_\n",
    "        the function is used to evaluate the LLM output saved in `filename` by the metric in `metrics`,the successful eval results will be saved into `save_file` and the error item will be saved into `error_file`\n",
    "    \n",
    "    Args:\n",
    "        `filename` (Union[str,Path]): the filename saves the LLM generation results\n",
    "        `save_file` (Union[str,Path]): the filename will save the evaluate results\n",
    "        `error_file` (Union[str,Path]): the filename will save the error eval item\n",
    "        `force_save` (bool, optional): if the value is `True`,function will rerun all eval item in `filename` and directly override the `save_file` and `error_file`. \n",
    "                    Defaults to False.\n",
    "        `metrics` (List[BaseMetric], optional): a list of evaluation metrics. \n",
    "                    Defaults to [AnswerRelevancyMetric(threshold=0.5,model=evaluateModel,include_reason=True),HallucinationMetric(threshold=0.5,model=evaluateModel,include_reason=True)].\n",
    "    \"\"\"\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id']  and x['AnswerModel'] == item['AnswerModel'] :\n",
    "            return True\n",
    "        return False\n",
    "    with open(filename,'r') as f:\n",
    "        data = json.load(f)\n",
    "    save,error = dict(),dict()\n",
    "    if  Path(save_file).is_file() and not force_save:\n",
    "        with open(save_file) as f:\n",
    "            save = json.load(f)\n",
    "        if 'fileName' in save and  save['fileName'] != data['fileName']:\n",
    "            print(\"The save_file does not match the file name!\")\n",
    "            return\n",
    "    else:\n",
    "        save = {'fileName':data['fileName'],'class':data['class'],'data':[]}\n",
    "\n",
    "    if Path(error_file).is_file() and not force_save:\n",
    "        with open(error_file) as f:\n",
    "            error = json.load(f)\n",
    "        if 'fileName' in error and  error['fileName'] != data['fileName']:\n",
    "            print(\"The error_file does not match the file name!\")\n",
    "            return\n",
    "    else:\n",
    "        error = {'fileName' :data['fileName'],'class':data['class'],'data':[]}\n",
    "    for metric in metrics:\n",
    "        for item in data['data']:\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'retrieval_context':None,\n",
    "                'cached_metrics_data':[]\n",
    "            }\n",
    "            metrics_format_ins = {\n",
    "                'metric_metadata':{\n",
    "                    'metric':None,\n",
    "                    'success':True,\n",
    "                    'score':0.8,\n",
    "                    'reason':'',\n",
    "                    'statements':'',\n",
    "                    'verdicts':'',\n",
    "                    'evaluationCost': 0\n",
    "                },\n",
    "                'metric_configuration': {\n",
    "                    'threshold': 0.5,\n",
    "                    'evaluation_model': 'CustomLLM',\n",
    "                    'strict_mode': False,\n",
    "                    'include_reason': True\n",
    "                }\n",
    "            }\n",
    "            errors_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'retrieval_context':None,\n",
    "                'cached_metrics_data':[\n",
    "                    {            \n",
    "                        'metric_metadata':{\n",
    "                            'metric':None,\n",
    "                        },\n",
    "                        'metric_configuration': {\n",
    "                            'threshold': 0.5,\n",
    "                            'evaluation_model': 'CustomLLM',\n",
    "                            'strict_mode': False,\n",
    "                            'include_reason': True\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            data_format_ins['id'] = item['id']\n",
    "            data_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "            data_format_ins['input'] = item['input']\n",
    "            data_format_ins['actual_output'] = item['actual_output']\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            tag = False\n",
    "            for x in save['data']:\n",
    "                if is_same_eval_item(item,x):\n",
    "                    for metric_data in x['cached_metrics_data']:\n",
    "                        if metric_data['metric_metadata']['metric'] == metric.__name__:\n",
    "                            if metric_data['metric_configuration']['threshold'] == metric.threshold and metric_data['metric_configuration']['evaluation_model'] == metric.evaluation_model and metric_data['metric_configuration']['strict_mode'] == metric.strict_mode and metric_data['metric_configuration']['include_reason'] == metric.include_reason:\n",
    "                                tag = True\n",
    "                                print(\"HAVE:\")\n",
    "                                print(x)\n",
    "                                break\n",
    "            if tag:\n",
    "                continue\n",
    "            test_case = LLMTestCase(\n",
    "                input= item['input'],\n",
    "                actual_output=item['actual_output'],\n",
    "                context=[item['expected_output']],\n",
    "            )\n",
    "            try:\n",
    "                metric.measure(test_case)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                errors_format_ins['id'] = item['id']\n",
    "                errors_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "                errors_format_ins['input'] = item['input']\n",
    "                errors_format_ins['actual_output'] = item['actual_output']\n",
    "                errors_format_ins['expected_output'] = item['expected_output']\n",
    "                errors_format_ins['cached_metrics_data'][0]['metric_metadata']['metric'] = metric.__name__\n",
    "                errors_format_ins['cached_metrics_data'][0]['metric_configuration'] = {'threshold':metric.threshold,'evaluation_model':metric.evaluation_model,'strict_mode':metric.strict_mode,'include_reason':metric.include_reason}\n",
    "                error['data'].append(errors_format_ins.copy())\n",
    "                print(errors_format_ins)\n",
    "                with open(error_file,'w') as f:\n",
    "                    json.dump(error,f,indent=4)\n",
    "                continue\n",
    "            metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "            metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "            metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "            metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "            \n",
    "            metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "            metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "            metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "            metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "            metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "            metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "            metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "\n",
    "            data_format_ins['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "            flag = True\n",
    "            for each in save['data']:\n",
    "                if is_same_eval_item(each, data_format_ins):\n",
    "                    each['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                save['data'].append(data_format_ins.copy())\n",
    "                print(data_format_ins)\n",
    "            with open(save_file,'w') as f:\n",
    "                json.dump(save,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VYA1YVHEARJGA6J1RKZRN",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./data/KoLA/save/'):\n",
    "    evaluate_file('./data/KoLA/save/'+file,'./data/KoLA/eval/save/'+file,'./data/KoLA/eval/error/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VH7DJWH1SW1YJGEKMXT2G",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinEvalErrorToData(errorFile:Union[str,Path],saveFile:Union[str,Path])->None:\n",
    "    \"\"\"_summary_\n",
    "        the function is used to rerun the error item in the `errorFile` and append the results into the `saveFile` \n",
    "        \n",
    "    Args:\n",
    "        `errorFile` (Union[str,Path]): the JSON file saves the error item in the before running\n",
    "        `saveFile` (Union[str,Path]): the JSON file saves the pass result\n",
    "        \n",
    "    Returns:\n",
    "        None: the result will override the original file \n",
    "    \"\"\"\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id'] and x['AnswerModel'] == item['AnswerModel'] :\n",
    "            return True\n",
    "        return False\n",
    "    with open(errorFile,'r') as f:\n",
    "        data_er = json.load(f)\n",
    "    with open(saveFile,'r') as f:\n",
    "        data_sv = json.load(f)\n",
    "    if data_er['fileName'] != data_sv['fileName']:\n",
    "        print(\"The save_file does not match the error_file!\")\n",
    "        return\n",
    "    error = {'fileName' :data_er['fileName'],'class':data_er['class'],'data':[]}\n",
    "    while data_er['data']:\n",
    "        item = data_er['data'].pop()\n",
    "        data_format_ins = {\n",
    "            'id':0,\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'context':None,\n",
    "            'retrieval_context':None,\n",
    "            'cached_metrics_data':[]\n",
    "        }\n",
    "        metrics_format_ins = {\n",
    "            'metric_metadata':{\n",
    "                'metric':None,\n",
    "                'success':True,\n",
    "                'score':0.8,\n",
    "                'reason':'',\n",
    "                'statements':'',\n",
    "                'verdicts':'',\n",
    "                'evaluationCost': 0\n",
    "            },\n",
    "            'metric_configuration': {\n",
    "                'threshold': 0.5,\n",
    "                'evaluation_model': 'CustomLLM',\n",
    "                'strict_mode': False,\n",
    "                'include_reason': True\n",
    "            }\n",
    "        }\n",
    "        errors_format_ins = {\n",
    "            'id':0,\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'retrieval_context':None,\n",
    "            'cached_metrics_data':[\n",
    "                {            \n",
    "                    'metric_metadata':{\n",
    "                        'metric':None,\n",
    "                    },\n",
    "                    'metric_configuration': {\n",
    "                        'threshold': 0.5,\n",
    "                        'evaluation_model': 'CustomLLM',\n",
    "                        'strict_mode': False,\n",
    "                        'include_reason': True\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data_format_ins['id'] = item['id']\n",
    "        data_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "        data_format_ins['input'] = item['input']\n",
    "        data_format_ins['actual_output'] = item['actual_output']\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        \n",
    "        tag = False\n",
    "        for x in data_sv['data']:\n",
    "            if is_same_eval_item(item,x):\n",
    "                for metric_data in x['cached_metrics_data']:\n",
    "                    if metric_data['metric_metadata']['metric'] == item['cached_metrics_data'][0]['metric_metadata']['metric']:\n",
    "                        if metric_data['metric_configuration']['threshold'] == item['cached_metrics_data'][0]['metric_configuration']['threshold'] and metric_data['metric_configuration']['evaluation_model'] == item['cached_metrics_data'][0]['metric_configuration']['evaluation_model'] and metric_data['metric_configuration']['strict_mode'] == item['cached_metrics_data'][0]['metric_configuration']['strict_mode'] and metric_data['metric_configuration']['include_reason'] == item['cached_metrics_data'][0]['metric_configuration']['include_reason']:\n",
    "                            tag = True\n",
    "                            print(\"HAVE:\")\n",
    "                            print(x)\n",
    "                            break\n",
    "        if tag:\n",
    "            continue\n",
    "        test_case = LLMTestCase(\n",
    "            input= item['input'],\n",
    "            actual_output=item['actual_output'],\n",
    "            context=[item['expected_output']],\n",
    "        )\n",
    "        if item['cached_metrics_data'][0]['metric_metadata']['metric'] == 'Answer Relevancy':\n",
    "            metric = AnswerRelevancyMetric(\n",
    "                threshold=item['cached_metrics_data'][0]['metric_configuration']['threshold'],\n",
    "                model = evaluateModel,\n",
    "                include_reason=item['cached_metrics_data'][0]['metric_configuration']['include_reason']\n",
    "            )\n",
    "        elif item['cached_metrics_data'][0]['metric_metadata']['metric'] == 'Hallucination':\n",
    "            metric = HallucinationMetric(\n",
    "                threshold=item['cached_metrics_data'][0]['metric_configuration']['threshold'],\n",
    "                model = evaluateModel,\n",
    "                include_reason=item['cached_metrics_data'][0]['metric_configuration']['include_reason']\n",
    "            )\n",
    "        else:\n",
    "            print(\"unkonwn metric!\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            metric.measure(test_case)\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc())\n",
    "            errors_format_ins['id'] = item['id']\n",
    "            errors_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "            errors_format_ins['input'] = item['input']\n",
    "            errors_format_ins['actual_output'] = item['actual_output']\n",
    "            errors_format_ins['expected_output'] = item['expected_output']\n",
    "            errors_format_ins['cached_metrics_data'][0]['metric_metadata']['metric'] = metric.__name__\n",
    "            errors_format_ins['cached_metrics_data'][0]['metric_configuration'] = {'threshold':metric.threshold,'evaluation_model':metric.evaluation_model,'strict_mode':metric.strict_mode,'include_reason':metric.include_reason}\n",
    "            error['data'].append(errors_format_ins.copy())\n",
    "            print(errors_format_ins)\n",
    "            with open(errorFile,'w') as f:\n",
    "                json.dump(data_er,f,indent=4)\n",
    "            continue\n",
    "        metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "        metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "        metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "        metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "        \n",
    "        metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "        metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "        metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "        metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "        metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "        metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "        metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "\n",
    "        data_format_ins['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "        flag = True\n",
    "        for each in data_sv['data']:\n",
    "            if is_same_eval_item(each, data_format_ins):\n",
    "                each['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            data_sv['data'].append(data_format_ins.copy())\n",
    "            print(data_format_ins)\n",
    "        with open(saveFile,'w') as f:\n",
    "            json.dump(data_sv,f,indent=4)\n",
    "    \n",
    "    with open(errorFile,'w') as f:\n",
    "        json.dump(error,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VSD9ZM6ZPV4NVHR7Y2AQM",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdirs,files in os.walk(\"./data/KoLA/eval/error\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            joinEvalErrorToData('./data/KoLA/eval/error/'+file,'./data/KoLA/eval/save/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VSAT7N85H94262AKY18PQ",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_error = ['1-1_2_high_freq_ent_sample.json','1-2_1_low_freq_ent_sample.json', '1-3_r_1_simple_sample_sample.json', '2-5_DocRED_sample.json', '2-6_MAVEN_sample.json',  '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-4_kqapro_sample.json',  '3-6_r_KoRC++ood_sample.json',  '4-1_with_triples_sample.json', '4-2_r_with_triples_sample.json']\n",
    "for file in files_error[::-1]:\n",
    "    joinEvalErrorToData('./data/KoLA/eval/error/'+file,'./data/KoLA/eval/save/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VN69KWMEZ2KC0RHVSJDW4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionFind:\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "        self.parent = [i for i in range(n)]\n",
    "        self.size = n\n",
    "        self.keyset = [1]*n\n",
    "    def find(self,x):\n",
    "        if self.parent[x]!= x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self,x,y):\n",
    "        x = self.find(x)\n",
    "        y = self.find(y)\n",
    "        if x == y:\n",
    "            return False\n",
    "        if self.keyset[x] < self.keyset[y]:\n",
    "            x,y = y,x\n",
    "        self.parent[y] = x\n",
    "        self.keyset[x] += self.keyset[y]\n",
    "        self.size -= 1\n",
    "        return True\n",
    "    \n",
    "    def is_connected(self,x,y):\n",
    "        return self.find(x) == self.find(y)\n",
    "    def get_size(self,x):\n",
    "        return self.keyset[self.find(x)]\n",
    "    def get_size_all(self):\n",
    "        return self.size\n",
    "def merge_same_item(file_path:Union[str,Path])->None:\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id'] and  x['AnswerModel'] == item['AnswerModel'] :\n",
    "            return True\n",
    "        return False\n",
    "    with open(file_path,'r') as f:\n",
    "        data = json.load(f)\n",
    "    check = []\n",
    "    for item in data['data']:\n",
    "        if len(item[\"cached_metrics_data\"]) <2:\n",
    "            check.append(item)\n",
    "    check.sort(key=lambda x:(x['id'],x['AnswerModel']))\n",
    "    uf = UnionFind(len(check))\n",
    "    for i in range(len(check)):\n",
    "        for j in range(i+1,len(check)):\n",
    "            if is_same_eval_item(check[i],check[j]):\n",
    "                uf.union(i,j)\n",
    "    key_set = defaultdict(list)\n",
    "    for i in range(len(check)):\n",
    "        p = uf.find(i)\n",
    "        key_set[p].append(check[i])\n",
    "        \n",
    "    mergeList =  list(key_set.values())\n",
    "    for item in mergeList[:]:\n",
    "        if len(item)<2:\n",
    "            mergeList.remove(item)\n",
    "    for x,y in mergeList:\n",
    "        mergeItem = x.copy()\n",
    "        mergeItem['cached_metrics_data'].append(y['cached_metrics_data'][0])\n",
    "        data['data'].remove(x)\n",
    "        data['data'].remove(y)\n",
    "        data['data'].append(mergeItem)\n",
    "    with open(file_path,'w') as f:\n",
    "        json.dump(data,f,indent=4)\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3VQ5FSBVTMN9G8XNXZY7FX",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk('./data/KoLA/eval/save'):\n",
    "    for file in files:\n",
    "        merge_same_item('./data/KoLA/eval/save/'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01HXKR5W8PNS7HXZ48XCQEV4QF",
   "metadata": {},
   "source": [
    "### ADD `idx` To replace `id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 后来发现原始数据中的`id`存在问题，故使用`idx`进行替代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HXKR8WH38H5306AH99W6HGSH",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk(\"data/KoLA/origin\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(dir,file)) as f:\n",
    "                data = json.load(f)\n",
    "            instructions = data[\"adapter_spec\"][\"instructions\"]\n",
    "            prefix = file.replace(\".json\",\"\").replace(\"_sample\",\"\")\n",
    "            for i,ins in enumerate(data[\"request_states\"]):\n",
    "                ins[\"instance\"][\"idx\"] = prefix+\"==\"+(\"%02d\"%i)\n",
    "            with open(os.path.join(dir,file),'w') as f:\n",
    "                json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HXKR7W28SFE7REF9HA1Q67E5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_o = \"data/KoLA/origin\"\n",
    "dir_data = \".\\data\\\\KoLA\\\\save\"\n",
    "dir_eval = \"./data/KoLA/eval\\\\save\"\n",
    "dir_eval_error = \".\\\\data/KoLA/eval\\\\error\"\n",
    "fileList = ['1-1_2_high_freq_ent_sample.json', '1-2_1_low_freq_ent_sample.json', '1-3_r_1_simple_sample_sample.json', '2-1_COPEN++csj_sample.json', '2-2_COPEN++cpj_sample.json', '2-3_COPEN++cic_sample.json', '2-4_FewNERD++inter_sample.json', '2-4_FewNERD++intra_sample.json', '2-4_FewNERD++supervised_sample.json', '2-5_DocRED_sample.json', '2-6_MAVEN_sample.json', '2-7_MAVEN-ERE_sample.json', '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-3_musique_sample.json', '3-4_kqapro_sample.json', '3-5_KoRC++ood_sample.json', '3-6_r_KoRC++ood_sample.json', '4-1_without_triples_sample.json', '4-1_with_triples_sample.json', '4-2_r_without_triples_sample.json', '4-2_r_with_triples_sample.json']\n",
    "for file in fileList:\n",
    "    with open(os.path.join(dir_o,file)) as f:\n",
    "        data_origin = json.load(f)\n",
    "    # dir_data = dir_eval\n",
    "    # dir_data = dir_eval_error\n",
    "    if not os.path.exists(os.path.join(dir_data,file)):\n",
    "        continue\n",
    "    with open(os.path.join(dir_data,file)) as f:\n",
    "        data_save = json.load(f)\n",
    "    if data_save['fileName'] != file:\n",
    "        print(\"Not Match \"+file)\n",
    "        continue\n",
    "    instructions = data_origin[\"adapter_spec\"][\"instructions\"]\n",
    "    for ins in data_origin[\"request_states\"]:\n",
    "        inp = ins[\"instance\"][\"input\"][\"text\"]\n",
    "        idx = ins[\"instance\"][\"idx\"]\n",
    "        expected_output = ins[\"instance\"][\"references\"][0][\"output\"][\"text\"]\n",
    "        for item in data_save['data']:\n",
    "            if item[\"expected_output\"] == expected_output and item[\"input\"] == instructions+\"\\n\"+inp :\n",
    "                item[\"idx\"] = idx\n",
    "    with open(os.path.join(dir_data,file),'w') as f:\n",
    "        json.dump(data_save,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZFK2Z3WPY8PRF2NP148DV",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = {\"Answer Relevancy\":defaultdict(list),\"Hallucination\":defaultdict(list),}\n",
    "for dir,subdir,files in os.walk('data/KoLA/eval/save'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(dir,file)) as f:\n",
    "            data = json.load(f)\n",
    "        for item in data['data']:\n",
    "            for metric in item[\"cached_metrics_data\"]:\n",
    "                data_eval[metric[\"metric_metadata\"][\"metric\"]][item['AnswerModel']].append(metric[\"metric_metadata\"][\"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZFZCV3Y5K9VY970E5BZ3S",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZG6VBQET04FH0VJEHQ1PT",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_mean = {\"Answer Relevancy\":defaultdict(dict),\"Hallucination\":defaultdict(dict),}\n",
    "for metric,modelEvalItem in data_eval.items():\n",
    "    for model,evals in modelEvalItem.items():\n",
    "        data_eval_mean[metric][model][\"mean\"] = np.mean(evals)\n",
    "        data_eval_mean[metric][model][\"variance\"] = np.var(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZGBSEM5PJ2CR3C42WAM1E",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HXKVK8KYA4580A10VMAZPXXM",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_view = {}\n",
    "for dir,subdir,files in os.walk('data/KoLA/eval/save'):\n",
    "    for file in files:\n",
    "        data_id = defaultdict(dict)\n",
    "        with open(os.path.join(dir,file)) as f:\n",
    "            data = json.load(f)\n",
    "        for item in data['data']:\n",
    "            for metric in item[\"cached_metrics_data\"]:\n",
    "                if metric[\"metric_metadata\"][\"metric\"] not in data_id[item['AnswerModel']]:\n",
    "                    data_id[item['AnswerModel']][metric[\"metric_metadata\"][\"metric\"]] = []\n",
    "                data_id[item['AnswerModel']][metric[\"metric_metadata\"][\"metric\"]].append(metric[\"metric_metadata\"][\"score\"])\n",
    "        for key,value in data_id.items():\n",
    "            for k,v in value.items():\n",
    "                data_id[key][k] = np.mean(v)\n",
    "        data_view[file] = data_id.copy()\n",
    "\n",
    "data_view_ar = deepcopy(data_view)\n",
    "data_view_h = deepcopy(data_view)\n",
    "for key,value in data_view_ar.items():\n",
    "    for k,v in value.items():\n",
    "        data_view_ar[key][k] = v['Answer Relevancy'] if 'Answer Relevancy' in v else None\n",
    "        data_view_h[key][k] = v['Hallucination'] if 'Hallucination' in v else None\n",
    "        \n",
    "# pd.DataFrame(data_view).to_json(\"data/KoLA/results_view/model_file_dict_AR_H.json\",indent=4)\n",
    "# pd.DataFrame(data_view_ar).to_json(\"data/KoLA/results_view/model_file_AnswerRelevancy.json\",indent=4)\n",
    "# pd.DataFrame(data_view_h).to_json(\"data/KoLA/results_view/model_file_Hallucination.json\",indent=4)\n",
    "# pd.DataFrame(data_view).to_excel(\"data/KoLA/results_view/model_file_dict_AR_H.xlsx\")\n",
    "# pd.DataFrame(data_view_ar).to_excel(\"data/KoLA/results_view/model_file_AnswerRelevancy.xlsx\",)\n",
    "# pd.DataFrame(data_view_h).to_excel(\"data/KoLA/results_view/model_file_Hallucination.xlsx\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HXKVMA8MP8BCJGH4EWKX4ZQF",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HXKVMN4QF7JC6Q6ZW6FV1EE1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_view_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HXKVN28D5VC5G3EB7SHKDGNP",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_view_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATH DATA PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Field Name of MATH401 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX4025VX12NBKZ1JA9217W99",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {'fileName':\"math401.json\",'data':[]}\n",
    "with open('E:/Repository/math401-llm-main/math401.json') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        text = data['query']\n",
    "        if '**' in text:\n",
    "            text = text.replace('**','^')\n",
    "        item = {'input':text,'expected_output':data['response']}\n",
    "        question['data'].append(item.copy())\n",
    "with open('./data/math401/math401.json','w') as file:\n",
    "    json.dump(question,file,indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01HYEZSB1BM1QQ0XNAGEJ4443J",
   "metadata": {},
   "source": [
    "#### Add `class`,`id` Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYEZJAXC780D8KNPD80P7FRY",
   "metadata": {},
   "outputs": [],
   "source": [
    "classList = [\n",
    "    \"Euler Equation.\",\n",
    "    \"Add \\& Subtract of two integers within 10.\",\n",
    "    \"Add \\& Subtract of two integers within 100.\",\n",
    "    \"Add \\& Subtract of two integers within 1,000.\",\n",
    "    \"Add \\& Subtract of two integers within 1,000,000,000,000.\",\n",
    "    \"Add \\& Subtract of two integers within -10~10.\",\n",
    "    \"Add \\& Subtract of two decimal numbers within -100~100.\",\n",
    "    \"Multiply two integers within 100.\",\n",
    "    \"Multiply two decimal numbers within 10.\",\n",
    "    \"Multiply two integers within 100,000.\",\n",
    "    \"Division of two integers within 100.\",\n",
    "    \"Exponentiation of with integer base within 10 and integer exponent within 2~4.\",\n",
    "    \"Exponentiation of with a decimal number within 10 as the base and a decimal number within 2~4 as the exponent.\",\n",
    "    \"Add, Subtract \\& Multiply with one integer within 10 and a common irrational number (i.e. $e$ or $\\pi$).\",\n",
    "    \"Long arithmetic expressions with brackets, involved integers are all within 100 and operators contain add, subtract, multiply, and division.\",\n",
    "    \"Trigonometry functions including $\\sin$, $\\cos$, and $\\tan$. Inputs can be in the format of degrees and radians ($\\pi$ can also appear in the inputs).\",\n",
    "    \"Logarithm of integers within 1000 of different bases: $2,e,10$.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYEZJS3AKZJ6NAHECYM9GYBW",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/math401/math401.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for i,item in enumerate(data['data']):\n",
    "    item[\"id\"] = i\n",
    "    item[\"class\"] = classList[(i+24)//25]\n",
    "with open('./data/math401/math401.json','w') as f:\n",
    "    json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math50 Evaluation Samples Drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX40JAXPSCH3655C8NS17BQ8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/math401/math401.json') as f:\n",
    "    data = json.load(f)\n",
    "data_new  = {\"fileName\":\"math50.json\",'data':[]}\n",
    "data_new['data'].append(data['data'][0])\n",
    "data_new['data'].append(data['data'][1])\n",
    "for i,item in enumerate(data['data'][2:]):\n",
    "    if i % 8 == 0:\n",
    "        data_new['data'].append(item)\n",
    "with open('./data/math401/math50.json','w') as f:\n",
    "    json.dump(data_new,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mannual processing\n",
    "1. delete:\n",
    "    delete the following item from math50.json\n",
    "    <table>\n",
    "    <tr><td>\n",
    "        {\n",
    "            \"id\": 34,\n",
    "            \"input\": \"2+4=\",\n",
    "            \"expected_output\": \"6\"\n",
    "        }\n",
    "    </td></tr>\n",
    "    <tr><td>\n",
    "        {\n",
    "            \"id\": 306,\n",
    "            \"input\": \"0*2=\",\n",
    "            \"expected_output\": \"0.0000\"\n",
    "        }\n",
    "    </td></tr>\n",
    "    <tr><td>\n",
    "        {\n",
    "            \"id\": 266,\n",
    "            \"input\": \"6^2=\",\n",
    "            \"expected_output\": \"36\"\n",
    "        }\n",
    "    </td></tr>\n",
    "    </table>\n",
    "2. replace: \n",
    "    do the following replacing actions in math50.json\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>old</td>\n",
    "            <td>new</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                {\n",
    "                    \"id\": 378,\n",
    "                    \"input\": \"log 10(63)=\",\n",
    "                    \"expected_output\": \"1.7993\"\n",
    "                }\n",
    "            </td>\n",
    "            <td>\n",
    "                {\n",
    "                    \"id\": 376,\n",
    "                    \"input\": \"log 2(71)=\",\n",
    "                    \"expected_output\": \"6.1497\"\n",
    "                }\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "3. insert: \n",
    "    do the following inserting actions in math50.json\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td>insertItem</td>\n",
    "            <td>after</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                {\n",
    "                    \"id\": 352,\n",
    "                    \"input\": \"tan(-0.17\\u03c0)=\",\n",
    "                    \"expected_output\": \"-0.5774\"\n",
    "                }\n",
    "            </td>\n",
    "            <td>\n",
    "                {\n",
    "                    \"id\": 370,\n",
    "                    \"input\": \"cos(-300\\u00b0)=\",\n",
    "                    \"expected_output\": \"0.5000\"\n",
    "                }\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reset `id` of `math50.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYEZK798HT6KQ824AAHSZEPX",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/math401/math401.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open('./data/math401/math50.json','r') as f:\n",
    "    data_50 = json.load(f)\n",
    "for item in data_50['data']:\n",
    "    for idx in data['data']:\n",
    "        if item['input'] == idx['input']:\n",
    "            item['id'] = idx['id']\n",
    "            item['class'] = idx['class']\n",
    "            break\n",
    "with open('./data/math401/math50.json','w') as f:\n",
    "    json.dump(data_50,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZCH6ZSE8SZ6GN979AD82S",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_Reply_MATH(filepath:Union[str,Path],savePath:Union[str,Path],errorPath:Union[str,Path],isRecordInTime = True,answerModelList:Optional[List[str]] = None)->None:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        filepath (Union[str,Path]): the file path to the original evaluation task dataset\n",
    "        savePath (Union[str,Path]): the directory to save the results \n",
    "        errorPath (Union[str,Path]): the directory to save the error items which do no get the model reply at this attempt\n",
    "        isRecordInTime (bool, optional): if the value is True,will save the results in time ,else save the results after a model completes. Defaults to True.\n",
    "        answerModelList (List[str],optional): the list of model that will be evaluated,if the value is None,the default model list will be used. Defaults to None.  \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    questionList = data['data']\n",
    "    instructions = 'Only return the correct answer of the question.\\n'\n",
    "    if answerModelList is None:\n",
    "        answerModelList = [    \n",
    "            'baichuan2-7b-chat',\n",
    "            'baichuan2-13b-chat',\n",
    "            'baichuan2-13b-base',\n",
    "            'infini-megrez-7b', \n",
    "            'qwen-7b-chat',\n",
    "            'qwen-14b-chat',\n",
    "            'qwen-72b-chat',\n",
    "            'qwen-72b',\n",
    "            'qwen1.5-7b-chat',\n",
    "            'qwen1.5-14b-chat',\n",
    "            'qwen1.5-72b',\n",
    "            'qwen1.5-72b-chat',\n",
    "            # 'llama-2-70b', #这个模型对prompt有比较专业的要求，使得难以得到有效的回复\n",
    "            'llama-2-7b-chat',\n",
    "            'llama-2-13b-chat',\n",
    "            # 'llama-2-70b-chat' #这个模型可能访问量比较大，导致使用API一直超时\n",
    "        ]\n",
    "    for model in answerModelList:\n",
    "        error = {'fileName':model.title()+'.json','model':model,'data':[]}\n",
    "        save = {'fileName':model.title()+'.json','model':model,'data':[]}\n",
    "        if os.path.exists(os.path.join(savePath,model+'.json')):\n",
    "            with open(os.path.join(savePath,model+'.json'),'r',encoding='utf-8') as f:\n",
    "                save = json.load(f)\n",
    "        if os.path.exists(os.path.join(errorPath,model+'.json')):\n",
    "            with open(os.path.join(errorPath,model+'.json'),'r',encoding='utf-8') as f:\n",
    "                error = json.load(f)\n",
    "        for index,item in enumerate(questionList):\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                \"class\":'',\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'is_correct':-1,\n",
    "                'time':-1\n",
    "            }\n",
    "            if 'id' in item:\n",
    "                data_format_ins['id'] = item['id']\n",
    "            else:\n",
    "                data_format_ins['id'] = model+'-%04d'% index\n",
    "            if 'class' in item:\n",
    "                data_format_ins['class'] = item['class']\n",
    "            flag = False\n",
    "            for idx in save['data']:\n",
    "                if item['id'] == idx['id']:\n",
    "                    print(\"pass\\t\"+str(item['id'])+\"\\t\"+item['input'])\n",
    "                    flag = True\n",
    "                    break\n",
    "            for idx in error['data']:\n",
    "                if item['id'] == idx['id']:\n",
    "                    print(\"pass\\t\"+str(item['id'])+\"\\t\"+item['input'])\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            prompt = instructions+item['input']\n",
    "            data_format_ins['input'] = prompt\n",
    "            print(prompt)\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            data_format_ins['AnswerModel'] = model\n",
    "            print(model)\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                for idx in error['data']:\n",
    "                    if item['id'] == idx['id']:\n",
    "                        continue\n",
    "                error['data'].append({'id':data_format_ins['id'],\"AnswerModel\":model,\"input\":prompt,\"expected_output\":data_format_ins['expected_output'],'class':data_format_ins['class']})\n",
    "                continue\n",
    "            print(idx,f\"{delta:,}\",actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "            print('*'*70)\n",
    "            if isRecordInTime:\n",
    "                # =================================================================\n",
    "                with open(os.path.join(savePath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                    json.dump(save,out,indent=4)\n",
    "                # =================================================================\n",
    "                with open(os.path.join(errorPath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                    json.dump(error,out,indent=4)\n",
    "                # =================================================================\n",
    "        errorItemFinal = []\n",
    "        while error['data']:\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                \"class\":'',\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'is_correct':-1,\n",
    "                'time':-1\n",
    "            }\n",
    "            item = error['data'].pop()\n",
    "            data_format_ins['id'] = item['id']\n",
    "            model = item['AnswerModel']\n",
    "            data_format_ins['AnswerModel'] = model\n",
    "            prompt = item['input']\n",
    "            data_format_ins['input'] = prompt\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            data_format_ins['class'] = item['class']\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "                print('\\t'+str(idx)+'\\ttest')\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                errorItemFinal.append(item)\n",
    "                print(\"[error]:\\t\"+str(errorItemFinal[-1]))\n",
    "                continue\n",
    "            print(idx,delta,actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "        with open(os.path.join(savePath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "            json.dump(save,out,indent=4)\n",
    "        if errorItemFinal:\n",
    "            for i in errorItemFinal:\n",
    "                error['data'].append(i)\n",
    "            with open(os.path.join(errorPath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                json.dump(error,out,indent=4)\n",
    "        print('='*70)\n",
    "\n",
    "def joinErrorToData(errorFile:Union[str,Path],saveFile:Union[str,Path],originFile:Union[str,Path]='./data/math401/math401.json',maxReTry:int = 2)->None:\n",
    "    with open(errorFile) as f:\n",
    "        data_er = json.load(f)\n",
    "        \n",
    "    with open(originFile) as f:\n",
    "        data_o = json.load(f)\n",
    "    with open(saveFile,'r') as f:\n",
    "        data_sv = json.load(f)\n",
    "    if data_er['fileName'] != data_sv['fileName']:\n",
    "        print(\"The save_file does not match the error_file!\")\n",
    "        return\n",
    "    error = {'fileName' :data_er['fileName'],'model':data_er['model'],'data':[]}\n",
    "    while data_er['data']:\n",
    "        item = data_er['data'].pop()\n",
    "        data_format_ins = {\n",
    "            'id':0,\n",
    "            \"class\":'',\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'is_correct':-1,\n",
    "            'time':-1\n",
    "        }\n",
    "        model = item['AnswerModel']\n",
    "        data_format_ins['id'] = item['id']\n",
    "        if 'class' in item:\n",
    "            data_format_ins['class'] = item['class']\n",
    "        else:\n",
    "            data_format_ins['class'] = data_o['data'][item['id']]['class']\n",
    "        prompt = item['input']\n",
    "        data_format_ins['input'] = prompt\n",
    "        print(prompt)\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        data_format_ins['AnswerModel'] = model\n",
    "        print(model)\n",
    "        start = time.perf_counter_ns()\n",
    "        actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "        end = time.perf_counter_ns()\n",
    "        delta = end-start\n",
    "        idx = 0\n",
    "        while actual_output == \"Cannot connect to the model \"+model and idx<maxReTry:\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx += 1\n",
    "        if actual_output == \"Cannot connect to the model \"+model:\n",
    "            error['data'].append({'id':data_format_ins['id'],\"AnswerModel\":model,\"input\":prompt,\"expected_output\":data_format_ins['expected_output'],'class':data_format_ins['class']})\n",
    "            continue\n",
    "        print(idx,f\"{delta:,}\",actual_output,sep='\\t')\n",
    "        data_format_ins['actual_output'] = actual_output\n",
    "        data_format_ins['time'] = delta\n",
    "        data_sv['data'].append(data_format_ins.copy())\n",
    "        \n",
    "        print('*'*70)\n",
    "    \n",
    "    with open(saveFile,'w',encoding='utf-8') as out:\n",
    "        json.dump(data_sv,out,indent=4)\n",
    "\n",
    "    with open(errorFile,'w',encoding='utf-8') as out:\n",
    "        json.dump(error,out,indent=4)\n",
    "    print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Math50(drawn) Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZCH6Z2KY0MJ1QW9XHRHZE",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_LLM_Reply_MATH('./data/math401/math50.json','./data/math401/save','./data/math401/error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Math401(all) Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_LLM_Reply_MATH('./data/math401/math401.json','./data/math401/all/save','./data/math401/all/error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYESY8YV662D0AMAW02FH365",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinErrorToData(errorFile='data\\\\math401\\\\all\\\\error\\\\qwen-72b.json',saveFile='data/math401/all/save/qwen-72b.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYGPE16NXF0348V2ZJV5PHJQ",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinErrorToData(errorFile='data\\\\math401\\\\all\\\\error\\\\qwen1.5-72b.json',saveFile='data/math401/all/save/qwen1.5-72b.json',maxReTry=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYKBF1MKW10C91317GPG6ZN9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinErrorToData(errorFile='data\\\\math401\\\\all\\\\error\\\\qwen1.5-72b-chat.json',saveFile='data/math401/all/save/qwen1.5-72b-chat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYHJZQ4MJXN74AV83GHNG6VW",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinErrorToData(errorFile='data\\\\math401\\\\all\\\\error\\\\baichuan2-13b-chat.json',saveFile='data/math401/all/save/baichuan2-13b-chat.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Reverse Polish Notation Reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_Reply_MATH_RPN(filepath:Union[str,Path],savePath:Union[str,Path],errorPath:Union[str,Path],isRecordInTime = True,maxRetry:int = 2,answerModelList:Optional[List[str]] = None)->None:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        filepath (Union[str,Path]): the file path to the original evaluation task dataset\n",
    "        savePath (Union[str,Path]): the directory to save the results \n",
    "        errorPath (Union[str,Path]): the directory to save the error items which do no get the model reply at this attempt\n",
    "        isRecordInTime (bool, optional): if the value is True,will save the results in time ,else save the results after a model completes. Defaults to True.\n",
    "        answerModelList (List[str],optional): the list of model that will be evaluated,if the value is None,the default model list will be used. Defaults to None.  \n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    questionList = data['data']\n",
    "    if answerModelList is None:\n",
    "        answerModelList = [    \n",
    "            'baichuan2-7b-chat',\n",
    "            'baichuan2-13b-chat',\n",
    "            'baichuan2-13b-base',\n",
    "            'infini-megrez-7b', \n",
    "            'qwen-7b-chat',\n",
    "            'qwen-14b-chat',\n",
    "            'qwen-72b-chat',\n",
    "            'qwen-72b',\n",
    "            'qwen1.5-7b-chat',\n",
    "            'qwen1.5-14b-chat',\n",
    "            'qwen1.5-72b',\n",
    "            'qwen1.5-72b-chat',\n",
    "            # 'llama-2-70b', #这个模型对prompt有比较专业的要求，使得难以得到有效的回复\n",
    "            'llama-2-7b-chat',\n",
    "            'llama-2-13b-chat',\n",
    "            # 'llama-2-70b-chat' #这个模型可能访问量比较大，导致使用API一直超时\n",
    "        ]\n",
    "    for model in answerModelList:\n",
    "        error = {'fileName':model.title()+'.json','model':model,'data':[]}\n",
    "        save = {'fileName':model.title()+'.json','model':model,'data':[]}\n",
    "        if os.path.exists(os.path.join(savePath,model+'.json')):\n",
    "            with open(os.path.join(savePath,model+'.json'),'r',encoding='utf-8') as f:\n",
    "                save = json.load(f)\n",
    "        if os.path.exists(os.path.join(errorPath,model+'.json')):\n",
    "            with open(os.path.join(errorPath,model+'.json'),'r',encoding='utf-8') as f:\n",
    "                error = json.load(f)\n",
    "        for index,item in enumerate(questionList):\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                \"class\":'',\n",
    "                'AnswerModel':model,\n",
    "                'input':item['input'],\n",
    "                \"input_infix\":item['input_infix'],\n",
    "                \"instructions\":'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':item['expected_output'],\n",
    "                'is_correct':-1,\n",
    "                'time':-1\n",
    "            }\n",
    "            if 'instructions' in item:\n",
    "                instructions = item['instructions']\n",
    "            else:\n",
    "                instructions = 'Only return the correct answer of the question.'\n",
    "            data_format_ins['instructions'] = instructions\n",
    "            if 'id' in item:\n",
    "                data_format_ins['id'] = item['id']\n",
    "            else:\n",
    "                data_format_ins['id'] = model+'-%04d'% index\n",
    "            if 'class' in item:\n",
    "                data_format_ins['class'] = item['class']\n",
    "            flag = False\n",
    "            for idx in save['data']:\n",
    "                if item['id'] == idx['id']:\n",
    "                    print(\"pass\\t\"+str(item['id'])+\"\\t\"+item['input'])\n",
    "                    flag = True\n",
    "                    break\n",
    "            for idx in error['data']:\n",
    "                if item['id'] == idx['id']:\n",
    "                    print(\"pass\\t\"+str(item['id'])+\"\\t\"+item['input'])\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            prompt = instructions+'\\n'+item['input']\n",
    "            print(prompt)\n",
    "            print(model)\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<maxRetry:\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                for idx in error['data']:\n",
    "                    if item['id'] == idx['id']:\n",
    "                        continue\n",
    "                error['data'].append(data_format_ins.copy())\n",
    "                continue\n",
    "            print(idx,f\"{delta:,}\",actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "            print('*'*70)\n",
    "            if isRecordInTime:\n",
    "                # =================================================================\n",
    "                with open(os.path.join(savePath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                    json.dump(save,out,indent=4)\n",
    "                # =================================================================\n",
    "                with open(os.path.join(errorPath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                    json.dump(error,out,indent=4)\n",
    "                # =================================================================\n",
    "        errorItemFinal = []\n",
    "        while error['data']:\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                \"class\":'',\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                \"input_infix\":'',\n",
    "                \"instructions\":'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':'',\n",
    "                'is_correct':-1,\n",
    "                'time':-1\n",
    "            }\n",
    "            item = error['data'].pop()\n",
    "            data_format_ins['id'] = item['id']\n",
    "            model = item['AnswerModel']\n",
    "            data_format_ins['AnswerModel'] = model\n",
    "            prompt = instructions+\"\\n\"+item['input']\n",
    "            data_format_ins['input'] = item['input']\n",
    "            data_format_ins['input_infix'] = item['input_infix']\n",
    "            data_format_ins['instructions'] = item['instructions']\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            data_format_ins['class'] = item['class']\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx = 0\n",
    "            while actual_output == \"Cannot connect to the model \"+model and idx<maxRetry:\n",
    "                print('\\t'+str(idx)+'\\ttest')\n",
    "                start = time.perf_counter_ns()\n",
    "                actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "                end = time.perf_counter_ns()\n",
    "                delta = end-start\n",
    "                idx += 1\n",
    "            if actual_output == \"Cannot connect to the model \"+model:\n",
    "                errorItemFinal.append(item)\n",
    "                print(\"[error]:\\t\"+str(errorItemFinal[-1]))\n",
    "                continue\n",
    "            print(idx,f\"{delta:,}\",actual_output,sep='\\t')\n",
    "            data_format_ins['actual_output'] = actual_output\n",
    "            data_format_ins['time'] = delta\n",
    "            save['data'].append(data_format_ins.copy())\n",
    "        with open(os.path.join(savePath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "            json.dump(save,out,indent=4)\n",
    "        if errorItemFinal:\n",
    "            error['data'] = errorItemFinal\n",
    "            with open(os.path.join(errorPath,model+'.json'),'w',encoding='utf-8') as out:\n",
    "                json.dump(error,out,indent=4)\n",
    "        print('='*70)\n",
    "\n",
    "def joinErrorToDataRPN(errorFile:Union[str,Path],saveFile:Union[str,Path],originFile:Union[str,Path]='./data/math401/reverse_Polish_Notation.json',maxReTry:int = 2)->None:\n",
    "    with open(errorFile) as f:\n",
    "        data_er = json.load(f)\n",
    "    with open(originFile) as f:\n",
    "        data_o = json.load(f)\n",
    "    with open(saveFile,'r') as f:\n",
    "        data_sv = json.load(f)\n",
    "    if data_er['fileName'] != data_sv['fileName']:\n",
    "        print(\"The save_file does not match the error_file!\")\n",
    "        return\n",
    "    error = {'fileName' :data_er['fileName'],'model':data_er['model'],'data':[]}\n",
    "    model = data_er['model']\n",
    "    while data_er['data']:\n",
    "        item = data_er['data'].pop()\n",
    "        data_format_ins = {\n",
    "            'id':0,\n",
    "            \"class\":'',\n",
    "            'AnswerModel':model,\n",
    "            'input':item['input'],\n",
    "            \"input_infix\":item['input_infix'],\n",
    "            \"instructions\":'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':item['expected_output'],\n",
    "            'is_correct':-1,\n",
    "            'time':-1\n",
    "        }\n",
    "        if 'instructions' in item:\n",
    "            instructions = item['instructions']\n",
    "        else:\n",
    "            instructions = 'Only return the correct answer of the question.'\n",
    "        data_format_ins['instructions'] = instructions\n",
    "        model = item['AnswerModel']\n",
    "        data_format_ins['id'] = item['id']\n",
    "        if 'class' in item:\n",
    "            data_format_ins['class'] = item['class']\n",
    "        else:\n",
    "            data_format_ins['class'] = data_o['data'][item['id']]['class']\n",
    "        prompt = instructions+'\\n'+item['input']\n",
    "        print(prompt)\n",
    "        print(model)\n",
    "        start = time.perf_counter_ns()\n",
    "        actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "        end = time.perf_counter_ns()\n",
    "        delta = end-start\n",
    "        idx = 0\n",
    "        while actual_output == \"Cannot connect to the model \"+model and idx<maxReTry:\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,top_p=0)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx += 1\n",
    "        if actual_output == \"Cannot connect to the model \"+model:\n",
    "            error['data'].append(data_format_ins.copy())\n",
    "            continue\n",
    "        print(idx,f\"{delta:,}\",actual_output,sep='\\t')\n",
    "        data_format_ins['actual_output'] = actual_output\n",
    "        data_format_ins['time'] = delta\n",
    "        data_sv['data'].append(data_format_ins.copy())\n",
    "        \n",
    "        print('*'*70)\n",
    "    \n",
    "    with open(saveFile,'w',encoding='utf-8') as out:\n",
    "        json.dump(data_sv,out,indent=4)\n",
    "\n",
    "    with open(errorFile,'w',encoding='utf-8') as out:\n",
    "        json.dump(error,out,indent=4)\n",
    "    print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_LLM_Reply_MATH_RPN('./data/math401/reverse_Polish_Notation.json','./data/math401/reverse/save','./data/math401/reverse/error',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinErrorToDataRPN('./data/math401/reverse/error/baichuan2-13b-base.json','./data/math401/reverse/save/baichuan2-13b-base.json',maxReTry=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinErrorToDataRPN('./data/math401/reverse/error/llama-2-13b-chat.json','./data/math401/reverse/save/llama-2-13b-chat.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Reply Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZCH6ZVPSCC97BCK0RJAB8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_result(filePath:Union[str,Path])->None:\n",
    "    \"\"\"\n",
    "        the function is used to sort the results saved in the JSON `filepath` by `id` field\n",
    "        \n",
    "    Args:\n",
    "        `filePath` (Union[str,Path]): the JSON file path\n",
    "\n",
    "    Returns:\n",
    "        None: the result will override the original `filePath` \n",
    "    \"\"\"\n",
    "    with open(filePath,'r') as f:\n",
    "        data = json.load(f)\n",
    "    data['data'].sort(key = lambda x:x['id'])\n",
    "    with open(filePath,'w') as f:\n",
    "        json.dump(data,f,indent=4)\n",
    "\n",
    "def addField(file:Union[str,Path],field:str,default:Any)->None:\n",
    "    \"\"\"\n",
    "        the function is used to add a field into the `.json` `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the filename ,the file should be JSON file\n",
    "        `field` (str): the field name\n",
    "        `default` (Any): the default value of the added field,the value will add or subtract a Random from -500_000_000 to 500_000_000 if the field is `time`\n",
    "        \n",
    "    Returns:\n",
    "        None: the result will override the original `file` \n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if field not in item:\n",
    "            item[field] = default\n",
    "            if field == 'time':\n",
    "                item[field] += random.randint(-500_000_000,500_000_000)\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)\n",
    "\n",
    "def classValidation(filePath:Union[str,Path],orginFile:Union[str,Path] = './data/math401/math401.json') ->None:\n",
    "    \"\"\"\n",
    "        due to the unnoticed bug when implementing the `get_LLM_Reply_MATH` function to file `math401.json` directly,which was designed without the field `class` in the previous evaluation phase on `math50`,some mistakes occurred in the field `class` before the bug was fixed.\n",
    "    Args:\n",
    "        filePath (Union[str,Path]): the path to the file that is going to be validated\n",
    "        orginFile (Union[str,Path], optional): the path to the standard file. Defaults to `'./data/math401/math401.json'`.\n",
    "    \"\"\"\n",
    "    with open(filePath) as f:\n",
    "        data = json.load(f)\n",
    "    with open(orginFile) as f:\n",
    "        data_o = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if 'class' not in item or item['class'] != data_o['data'][item['id']]['class']:\n",
    "            print(item)\n",
    "            item['class'] = data_o['data'][item['id']]['class']\n",
    "    with open(filePath,'w',encoding='utf-8') as out:\n",
    "        json.dump(data,out,indent=4)\n",
    "\n",
    "def getNumberAnswer(text:str)->Union[float,None]:\n",
    "    \"\"\"\n",
    "        the function is used to extract the last number from the `text`\n",
    "\n",
    "    Args:\n",
    "        `text` (str): the text contains the number\n",
    "        \n",
    "    Returns:\n",
    "        Union[float,None]: the number in the `text`, or None if the `text` does not contain the number\n",
    "    \"\"\"\n",
    "    round = \"when round.* to \\d+ decimal places\"\n",
    "    text = re.sub(round, \"rounded to ## decimal places\",text)\n",
    "    remainer = \"with a remainder of \\d+\\.\"\n",
    "    text = re.sub(remainer, \"with a remainder of ###.\",text)\n",
    "    text = text.split('=')\n",
    "    if text:\n",
    "        text = text[-1]\n",
    "        regex = re.compile('([+-]?\\d+[,0-9]*[.]?[0-9]*)')\n",
    "        ret = regex.findall(text)\n",
    "        if ret:\n",
    "            return eval(ret[-1].replace(',',''))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def processAnswer(file:Union[str,Path]):\n",
    "    \"\"\"\n",
    "        the function is used to process the answer that the LLM returned , namely extracting the number from the `actual_output` and save as `extract_answer`, and transform `expected_output` from type `str` to `number[int,float]` \n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the filename of the file saving the answer created by the LLM\n",
    "        \n",
    "    Returns:\n",
    "        None : the result will be written into the original `file`\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if item['is_correct'] == -1:\n",
    "            item['extract_answer'] = getNumberAnswer(item['actual_output'])\n",
    "            item['expected_output'] = eval(item['expected_output'])\n",
    "            if item['extract_answer'] is None :\n",
    "                item['is_correct'] = 0\n",
    "            else:\n",
    "                item['is_correct'] =1 if abs(item['expected_output']-item['extract_answer']) < 1e-3 else 0\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MATH50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZCH6ZJC7571YKEH2ZZ02A",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "    for file in files:\n",
    "        sort_result(os.path.join(dir,file))\n",
    "        addField(os.path.join(dir,file),'extract_answer',None)\n",
    "        processAnswer(os.path.join(dir,file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01HYQTE9Y9AS6RHDRG7VWGVS3F",
   "metadata": {},
   "source": [
    "##### MATH401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYQTFA4GYETHWMFT0A7TM338",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/all/save'):\n",
    "    for file in files:\n",
    "        sort_result(os.path.join(dir,file))\n",
    "        addField(os.path.join(dir,file),'extract_answer',None)\n",
    "        processAnswer(os.path.join(dir,file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注:\n",
    "+ 由于某些模型在计算三角函数时返回的结果带有根号，如果人工计算这些特殊三角函数得到的数学准确值本身就是带有根号的结果，因而我们认为模型充分理解的这些三角表达式并认为这样的结果是正确答案。而所写的数值抽取函数是将最后一个数值字符串认为是计算结果且结果必须为浮点数或整数，所以我们人为地计算出这样的根号值并添加在`actual_output`字符串结尾，以更准确的计算模型对数学计算的准确率\n",
    "+ 虽然进行了根号转换，但有时返回的带根号结果本身就是错误的，所以不会误判而错误地提高准确率\n",
    "+ 根号的unicode编码为0x221A，所以在JSON文件中使用查找功能查找`\\u221a`进行人工修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/reverse/save'):\n",
    "    for file in files:\n",
    "        sort_result(os.path.join(dir,file))\n",
    "        addField(os.path.join(dir,file),'extract_answer',None)\n",
    "        processAnswer(os.path.join(dir,file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation And Results View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3ZCH6ZRF5AX3C8TJW3M7AB",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(file:Union[str,Path],return_tuple:bool = False)->Union[float,tuple[str,float]]:\n",
    "    \"\"\"\n",
    "        the function calculates the accuracy of the model answers on the `math401` dataset or sub-dataset saved in `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        `return_tuple` (bool, optional): if the value is True,the function will return a tuple (modelName,accuracy),else only return accuracy `[0,1]`. \n",
    "                    Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[float,tuple[str,float]]: if return_tuple is True,return a tuple containing both modelName and accuracy in format `(modelName:str,accuracy:float)`,else only return the accuracy in [0,1]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    correct = 0\n",
    "    for item in data['data']:\n",
    "        correct += item['is_correct']\n",
    "    accuracy =  correct/len(data['data'])\n",
    "    if return_tuple:\n",
    "        return (data['model'],accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_nan_ratio(file:Union[str,Path],return_tuple:bool = False,length:Optional[int] = None)->Union[float,tuple[str,float]]:\n",
    "    \"\"\"\n",
    "        the function calculates the no number ratio of the LLM answers on the `math401` dataset or sub-dataset saved in `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        `return_tuple` (bool, optional): if the value is True,the function will return a tuple (modelName,nan_ratio),else only return nan_ratio `[0,1]`. Defaults to False.\n",
    "        `length` (int,optional): the number of the evaluation task on the `math401` dataset, default is `None` ,which means the number of evaluation tasks on the `file` equals to the number in `math401` dataset,namely there is no task omitted. Defaults to None.\n",
    "    Returns:\n",
    "        Union[float,tuple[str,float]]: if return_tuple is True,return a tuple containing both modelName and nan_ratio in format `(modelName:str,nan_ratio:float)`,else only return the nan_ratio in [0,1]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    nan = 0\n",
    "    for item in data['data']:\n",
    "        if item['extract_answer'] is None:\n",
    "            nan += 1\n",
    "    if isinstance(length,int):\n",
    "        nan += length-len(data['data'])\n",
    "    else:\n",
    "        length = len(data['data'])\n",
    "    nan_ratio = nan/length\n",
    "    if return_tuple:\n",
    "        return (data['model'],nan_ratio)\n",
    "    return nan_ratio\n",
    "\n",
    "def RE(y_true:Union[int,float],y_pred:Union[int,float],)->float:\n",
    "    y_true = decimal.Decimal(y_true)\n",
    "    y_pred = decimal.Decimal(y_pred)\n",
    "    if abs(y_true - y_pred) < 1e-3:\n",
    "        return 0.0\n",
    "    return  float(min(10,abs(y_true-y_pred)/max(abs(y_true),1)))\n",
    "    \n",
    "    \n",
    "def calculate_RE(file:Union[str,Path],return_tuple:bool = False,isDict:bool = False,)->Union[List[float],Dict[int,float],tuple[str,List[float]],tuple[str,Dict[int,float]]]:\n",
    "    \"\"\"\n",
    "        the function calculates relative error(RE) between model answer and standard answer for each item in the file and return the results as a List or Dict.\n",
    "        \n",
    "    Args:\n",
    "        file (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        return_tuple (bool, optional): if the value is True,the function will return a tuple `(modelName:str,RE Collection:Dict[id:int,RE:float])`,else only return RE Collection:Union[List[float],Dict[id:int,RE:float]]. Defaults to False.\n",
    "        isDict (bool, optional): if the value is True,the function will return a `Dict[id:int,RE:float]`,else only return `List[float]`. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        Union[List[float],Dict[int,float],tuple[str,List[float]],tuple[str,Dict[int,float]]]: if return_tuple is True,return a tuple containing both modelName and REs in format `(modelName:str,RE Collection:Union[List[float],Dict[id:int,RE:float]])`,else only return the RE Collection,the format of RE Collection is Dict[id:int,RE:float] if is True ,else List[float]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    if isDict:\n",
    "        RE_set = {}\n",
    "    else:\n",
    "        RE_set = []\n",
    "    for item in data['data']:\n",
    "        ret = 0\n",
    "        y_true = item['expected_output']\n",
    "        y_pred = item['extract_answer']\n",
    "        if y_pred is None:\n",
    "            ret = 10\n",
    "        else:\n",
    "            ret = RE(y_true,y_pred)\n",
    "        if isDict:\n",
    "            RE_set[item['id']] = ret\n",
    "        else:\n",
    "            RE_set.append(ret)\n",
    "    if return_tuple:\n",
    "        return (data['model'],RE_set)\n",
    "    return RE_set\n",
    "\n",
    "def missingReplyNum(file:Union[str,Path],length:int = 401,return_tuple:bool = False)->Union[int,Tuple[str,int]]:\n",
    "    \"\"\"\n",
    "        the function returns the number of missing replies in `file`,namely the number of evaluation task cannot get the reply from the LLM\n",
    "    Args:\n",
    "        file (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        length (int, optional): the total number of the evaluation tasks. Defaults to 401.\n",
    "        return_tuple (bool, optional): if the value is True,the function will return a tuple (modelName,numberOfMissing:int),else only return the number of the missing. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[int,Tuple[str,int]]: if return_tuple is True,return a tuple containing both modelName and the number of the missing in format `(modelName:str,RE numOfMissing:int)`,else only return the number of the missing\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    num = length-len(data['data'])\n",
    "    if return_tuple:\n",
    "        return (data['model'],num)\n",
    "    return num\n",
    "\n",
    "def calculate_class(file:Union[str,Path],return_tuple:bool = False,saveDir:Union[Path,str] = None,saveType:str = \"JSON\",return_df:bool = False)->Union[pd.DataFrame,Dict[str,Dict[str,int]],tuple[str,pd.DataFrame],tuple[str,Dict[str,Dict[str,int]]]]:\n",
    "    \"\"\"\n",
    "        the function counts the number of each math401 class on [\"TRUE\":the correct answer, \"FALSE\":the wrong answer, \"NAN\":no answer or answer without number],and return the results or save the results in given saveDir.  \n",
    "    Args:\n",
    "        file (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        return_tuple (bool, optional): if the value is True,the function will return a tuple (modelName,numOfClass:Dict[\"TRUE\":Dict[class,int],\"FALSE\":Dict[class,int],\"NAN\":Dict[class,int]]),else only return numOfClass:Dict[\"TRUE\":Dict[class,int],\"FALSE\":Dict[class,int],\"NAN\":Dict[class,int]]. Defaults to False.\n",
    "        saveDir (Union[Path,str], optional): if `saveDir` is not None,the function will save the results under the `saveDir` in the file type of `saveType`. Defaults to None.\n",
    "        saveType (str, optional): the file type of the results will be saved. Defaults to \"JSON\",and must be one of type in [\"JSON\",\"XLSX\",\"EXCEL\",\"CSV\"].\n",
    "        return_df (bool, optional): if the value is True,the function will return the results in the format of pandas.DataFrame . Defaults to False.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: saveType must be 'json','csv','xlsx' or 'excel'\n",
    "\n",
    "    Returns:\n",
    "        Union[pd.DataFrame,Dict[str,Dict[str,int]],tuple[str,pd.DataFrame],tuple[str,Dict[str,Dict[str,int]]]]: return the results of counting the number of each class in the format of pandas.DataFrame if return_df is True ,else in the format of Dict[str,Dict[str,int]];if return_tuple is True ,returns the tuple(modelName:str,results) \n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "    ret = {\"TRUE\":defaultdict(int),\"FALSE\":defaultdict(int),\"NAN\":defaultdict(lambda :25,{\"Euler Equation.\":1})}\n",
    "    for item in data['data']:\n",
    "        if item['is_correct'] == 1:\n",
    "            ret[\"TRUE\"][item['class']] += 1\n",
    "            ret[\"FALSE\"][item['class']]\n",
    "        else:\n",
    "            ret[\"TRUE\"][item['class']]\n",
    "            ret[\"FALSE\"][item['class']] += 1\n",
    "        ret[\"NAN\"][item['class']] -= 1\n",
    "    ret = {k:dict(v) for k,v in ret.items()}\n",
    "    if saveDir and os.path.isdir(saveDir):\n",
    "        if saveType.lower() == 'json':\n",
    "            save = os.path.join(saveDir,data['model']+'.json')\n",
    "            with open(save,'w') as f:\n",
    "                json.dump({\"fileName\":Path(save).name,\"model\":data['model'],\"data\":ret},f,indent = 4)\n",
    "        elif saveType.lower() == 'csv':\n",
    "            save = os.path.join(saveDir,data['model']+'.csv')\n",
    "            pd.DataFrame(ret).to_csv(save,index_label=\"class\")\n",
    "        elif saveType.lower() == 'xlsx' or saveType.lower() == 'excel':\n",
    "            save = os.path.join(saveDir,data['model']+'.xlsx')\n",
    "            pd.DataFrame(ret).to_excel(save,index_label=\"class\")\n",
    "        else:\n",
    "            raise ValueError(\"saveType must be 'json','csv','xlsx' or 'excel'\")\n",
    "    if return_tuple and return_df:\n",
    "        return (data['model'],pd.DataFrame(ret))\n",
    "    if return_tuple:\n",
    "        return (data['model'],ret)\n",
    "    if return_df:\n",
    "        return pd.DataFrame(ret)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def deduplicated(file:Union[Path,str],equal:callable = None)->None:\n",
    "    if equal is None:\n",
    "        equal = lambda x,y:x['id'] == y['id']\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    ret = []\n",
    "    for item in data['data']:\n",
    "        if not ret :\n",
    "            ret.append(item.copy())\n",
    "        else:\n",
    "            flag = False\n",
    "            for idx in ret:\n",
    "                if equal(item,idx):\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                ret.append(item.copy())\n",
    "    data['data'] = ret\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYMC20KDYEPCE2CJ32PS52HY",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_output(directory:Union[str,Path],fileType:Optional[str],save_dir:Union[str,Path] = './data/math401/results/',isDict:bool = False,dataset:str = \"math401.json\",length:Optional[int]=None):\n",
    "    \"\"\"\n",
    "    the function outputs the evaluation results of the models in `directory` to a file in `save_dir`\n",
    "\n",
    "    Args:\n",
    "        directory (Union[str,Path]): the directory containing the reply of the models\n",
    "        fileType (Optional[str]): the file type to save evaluation results\n",
    "        save_dir (Union[str,Path], optional): the directory to save the output file. Defaults to './data/math401/results/'.\n",
    "        dataset (str, optional): the name of the dataset evaluated. Defaults to \"math401.json\".\n",
    "        length (Optional[int], optional): the total number of the evaluation tasks. Defaults to None.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: Save directory does not found in `save_dir`\n",
    "        ValueError: fileType must be `json`,`xlsx` or `df`\n",
    "    \"\"\"\n",
    "    if isDict:\n",
    "        field = \"RE_Dict\"\n",
    "    else:\n",
    "        field = \"RE_List\"\n",
    "    save = Path(dataset).stem\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise ValueError(f\"Save directory does not found in {save_dir}\")\n",
    "    if fileType.lower() == 'json':\n",
    "        data = {'eval':'math','eval_dataset':dataset,'data':[]}\n",
    "        for dir,subdir ,files in os.walk(directory):\n",
    "            for file in files:\n",
    "                tmp = calculate_accuracy(os.path.join(dir,file),return_tuple=True)\n",
    "                nan_ratio = calculate_nan_ratio(os.path.join(dir,file),length=length)\n",
    "                RE_Dict = calculate_RE(os.path.join(dir,file),isDict=isDict,)\n",
    "                missing = missingReplyNum(os.path.join(dir,file),length=length)\n",
    "                data['data'].append({'model':tmp[0],'Accuracy':tmp[1],'Nan_Ratio':nan_ratio,'missingReplyNum':missing,field:RE_Dict})\n",
    "        savePath = os.path.join(save_dir,save+'_eval_result.json')\n",
    "        with open(savePath,'w') as f:\n",
    "            json.dump(data,f,indent=4)\n",
    "        print(f\"Results saved in {savePath}\")\n",
    "    elif fileType.lower() == 'xlsx' or fileType.lower() == 'df':\n",
    "        data = {}\n",
    "        for dir,subdir ,files in os.walk(directory):\n",
    "            for file in files:\n",
    "                tmp = calculate_accuracy(os.path.join(dir,file),return_tuple=True)\n",
    "                nan_ratio = calculate_nan_ratio(os.path.join(dir,file),length=length)\n",
    "                RE_Dict = calculate_RE(os.path.join(dir,file),isDict=isDict)\n",
    "                missing = missingReplyNum(os.path.join(dir,file),length=length)\n",
    "                data[tmp[0]] = {'Accuracy':tmp[1],'Nan_Ratio':nan_ratio,'missingReplyNum':missing,field:RE_Dict}\n",
    "        df = pd.DataFrame(data).T.sort_index()\n",
    "        if fileType.lower() == 'df' :\n",
    "            savePath = os.path.join(save_dir,save+'_eval_result_df.json')\n",
    "            df.to_json(savePath,indent=4)\n",
    "            print(f\"Results saved in {savePath}\")\n",
    "        else:\n",
    "            savePath = os.path.join(save_dir,save+'_eval_result.xlsx')\n",
    "            df.to_excel(savePath)\n",
    "            print(f\"Results saved in {savePath}\")\n",
    "    else:\n",
    "        raise ValueError(\"fileType must be 'json','xlsx' or 'df'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./data/math401/reverse/save'):\n",
    "    deduplicated('./data/math401/reverse/save/'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MATH50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYQVHG7CESRVPA468VW6KCMC",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output('./data/math401/save','JSON',dataset='math50.json',length=50)\n",
    "results_output('./data/math401/save','df',dataset='math50.json',length=50)\n",
    "results_output('./data/math401/save','xlsx',dataset='math50.json',length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MATH401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HYQSBQE16W74T0PM9EDNV6XA",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output('./data/math401/all/save','JSON',isDict=True,length=401)\n",
    "results_output('./data/math401/all/save','df',isDict=True,length=401)\n",
    "results_output('./data/math401/all/save','xlsx',isDict=True,length=401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output('./data/math401/reverse/save','json',isDict=True,dataset='./data/math401/reverse_Polish_Notation.json',length=25)\n",
    "results_output('./data/math401/reverse/save','df',isDict=True,dataset='./data/math401/reverse_Polish_Notation.json',length=25)\n",
    "results_output('./data/math401/reverse/save','xlsx',isDict=True,dataset='./data/math401/reverse_Polish_Notation.json',length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk(\"data/math401/all/save\"):\n",
    "    for file in files:\n",
    "        calculate_class(os.path.join(dir,file),saveDir = \"./data/math401/all/classify/json/\",saveType='json',return_df=True)\n",
    "        calculate_class(os.path.join(dir,file),saveDir = \"./data/math401/all/classify/excel/\",saveType='xlsx',return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Evaluation Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EXZV4FD0TR4HP1ZQEX",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_timeout import *\n",
    "from evalplus.data import write_jsonl,get_human_eval_plus\n",
    "from evalplus.sanitize import main as sanitize_main\n",
    "from evalplus.syncheck import  main as syntax_check_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EX0HWYKP35M5Q9V5YT",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCompletion:\n",
    "    def __init__(self,index=0):\n",
    "        self.index = index\n",
    "    def GEN_SOLUTION(self,prompt,modelName:str = \"infini-megrez-7b\",INFINI_API_List = [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"],returnContent:bool = True,**kwargs):\n",
    "        url = \"https://cloud.infini-ai.com/maas/\"+modelName+\"/nvidia/chat/completions\"\n",
    "        payload = {\n",
    "            \"model\": modelName,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else 0.7,\n",
    "            \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else 1,\n",
    "            \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else -1,\n",
    "            \"n\": kwargs['n'] if 'n' in kwargs else 1,\n",
    "            \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else None,\n",
    "            \"stop\": kwargs['stop'] if 'stop' in kwargs else None,\n",
    "            \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else 0,\n",
    "            \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else 0\n",
    "        }\n",
    "        idx = 0\n",
    "        while idx < len(INFINI_API_List):\n",
    "            headers = {\n",
    "                    'Content-Type': \"application/json\",\n",
    "                    'Accept': \"*/*\",\n",
    "                    'Authorization': \"Bearer \"+INFINI_API_List[self.index%len(INFINI_API_List)],\n",
    "            } \n",
    "            # print(payload)\n",
    "            # print(headers)\n",
    "            response = requests.post(url, json=payload, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                data = response.json()\n",
    "                content = data['choices'][0]['message']['content']\n",
    "                print(content)\n",
    "                # if isinstance(content,str):\n",
    "                #     content = content.replace(',\\n}','\\n}')\n",
    "                #     content = content.replace(']\\n}',']}')\n",
    "                #     content = content.replace('\\\\','\\\\\\\\')\n",
    "                if returnContent:\n",
    "                    return content\n",
    "                try:\n",
    "                    content = json.loads(content)\n",
    "                except:\n",
    "                    pass\n",
    "                data['choices'][0]['message']['content'] = content\n",
    "                if isinstance(content,str):\n",
    "                    return content\n",
    "                \n",
    "                return json.dumps(data['choices'][0]['message']['content'])\n",
    "            else:\n",
    "                print(response.status_code)\n",
    "                try:\n",
    "                    print(response.json())\n",
    "                except:\n",
    "                    pass\n",
    "            self.index = (self.index + 1) % len(INFINI_API_List)\n",
    "            idx += 1\n",
    "        print((\"=\"*35)+'Error:\\t'+prompt+('='*35))\n",
    "        return \"Cannot connect to the model \"+modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EXCBF17PJM4VKFQH70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_human_eval_plus()\n",
    "dataset_copy  = dataset.copy()\n",
    "for i,key in enumerate(dataset.keys()):\n",
    "    if i % 6 != 0:\n",
    "        del dataset_copy[key]\n",
    "with open('./data/codeEval/data.json','w') as f:\n",
    "    json.dump(dataset_copy,f,ensure_ascii=False,indent=4)\n",
    "with open('./data/codeEval/data.jsonl','w') as fw:\n",
    "    for item in dataset_copy.keys():\n",
    "        fw.write(json.dumps(dataset_copy[item])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EXBF2YEW1QDA8TQT9Q",
   "metadata": {},
   "outputs": [],
   "source": [
    "answerModelList = [    \n",
    "    'baichuan2-7b-chat',\n",
    "    'baichuan2-13b-chat',\n",
    "    'baichuan2-13b-base',\n",
    "    'qwen-7b-chat',\n",
    "    'qwen-14b-chat',\n",
    "    'qwen-72b-chat',\n",
    "    'qwen-72b',\n",
    "    'qwen1.5-7b-chat',\n",
    "    'qwen1.5-14b-chat',\n",
    "    'qwen1.5-72b',\n",
    "    #'llama-2-70b-chat',#deprecated,can not connect to the model\n",
    "    'llama-2-70b',\n",
    "    'llama-2-7b-chat',\n",
    "    'llama-2-13b-chat',\n",
    "    'infini-megrez-7b',  \n",
    "]\n",
    "productor = LLMCompletion()\n",
    "with open('./data/codeEval/data.json','r') as f:\n",
    "    dataset = json.load(f)\n",
    "for modelName in answerModelList:\n",
    "    samples = [dict(task_id=task_id, solution=productor.GEN_SOLUTION(problem[\"prompt\"],modelName=modelName)) for task_id, problem in dataset.items()]\n",
    "    write_jsonl('./data/codeEval/code_raw/'+modelName+\".jsonl\", samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Sanitize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EX8V815J6TF6SBJCMS",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk('./data/codeEval/code_raw'):\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EX31E55E1H0A9DTSFA",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_code = ['baichuan2-13b-base.jsonl', 'baichuan2-13b-chat.jsonl', 'baichuan2-7b-chat.jsonl', 'infini-megrez-7b.jsonl',  'llama-2-13b-chat.jsonl', 'llama-2-70b-chat.jsonl', 'llama-2-70b.jsonl', 'llama-2-7b-chat.jsonl', 'qwen-14b-chat.jsonl', 'qwen-72b-chat.jsonl','qwen-72b.jsonl', 'qwen-7b-chat.jsonl', 'qwen1.5-14b-chat.jsonl', 'qwen1.5-72b.jsonl', 'qwen1.5-7b-chat.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3XSGT462SWQZGZR49206E5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUMANEVAL_OVERRIDE_PATH\"] = './data/codeEval/data.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3X89MPVMDBSTMHTCMJKM3R",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk('./data/codeEval/code_raw'):\n",
    "    for file in files:\n",
    "        sanitize_main(dir+'/'+file,\"humaneval\")\n",
    "        shutil.move(os.path.join(dir,file.replace(\".jsonl\",\"-sanitized.jsonl\")),os.path.join(Path(dir).parent/'code_sanitize',file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3XKZPQ0BXHA4RST6D6VQTM",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk('./data/codeEval/code_sanitize'):\n",
    "    for file in files:\n",
    "        syntax_check_main(dir+'/'+file,'humaneval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YA1NQY78HKVBAXMF4JRPS",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/codeEval/data.json','r') as f:\n",
    "    dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01HXKS9VFQ3KN29T3STA99Y0A9",
   "metadata": {},
   "source": [
    "#### Manual Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YAKGH1MKKQH58T8ZBVGVG",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"HumanEval/156\"]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EXG98TE4PE4DHFS658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'qwen1.5-7b-chat.jsonl'\n",
    "# data = []\n",
    "# with open('./data/codeEval/code_raw/'+file_name, 'r') as f:\n",
    "#     for line in f.readlines():\n",
    "#         data.append(json.loads(line))\n",
    "# data_sanitize = []\n",
    "# with open('./data/codeEval/code_sanitize/'+file_name,'r') as f:\n",
    "#     for line in f.readlines():\n",
    "#         data_sanitize.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3WB3EY82V5WTC0BYRV5G47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 27\n",
    "# print(data[i]['task_id'])\n",
    "# print(data[i]['solution'])\n",
    "# print('='*70)\n",
    "# print(data_sanitize[i]['task_id'])\n",
    "# print(data_sanitize[i]['solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YCE7W33ZAYD872NG6JDW0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(data_sanitize)):\n",
    "#     print(data_sanitize[i]['solution'])\n",
    "#     print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Execute And Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YM3K9KGDT3X29176Z0T05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(code:str,inputs:List,entry_point:str,record_time=True,timeout:float = 10)->List:\n",
    "    env = {}\n",
    "    n = len(inputs)\n",
    "    local = {}\n",
    "    try:\n",
    "        exec(code,None,local)\n",
    "        if len(local) != 1:\n",
    "            exec(code,env)\n",
    "        else:\n",
    "            env = local\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if record_time:\n",
    "            return ([\"SyntaxError\"]*n,[(1<<31)-1]*n)\n",
    "        return [\"SyntaxError\"]*n\n",
    "    if entry_point in env:\n",
    "        fn = env[entry_point]\n",
    "    else:\n",
    "        if record_time:\n",
    "            return ([\"NotImplemented\"]*n,[(1<<31)-1]*n)\n",
    "        return [\"NotImplemented\"]*n\n",
    "    @func_set_timeout(timeout)\n",
    "    def get(inp,record_time:bool=True):\n",
    "        try:\n",
    "            start = time.perf_counter_ns()\n",
    "            ret = fn(*inp) #if fn.__code__.co_argcount > 1 else fn(inp)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = (end-start)//1000_000\n",
    "        except Exception as e:\n",
    "            print(e,fn)\n",
    "            ret = \"SyntaxError\"\n",
    "            delta = (1<<31)-1\n",
    "        if isinstance(ret,NotImplementedError) or str(ret) == str(NotImplemented):\n",
    "            ret = \"NotImplemented\"\n",
    "        if record_time:\n",
    "            return (ret,delta)\n",
    "        return ret\n",
    "        \n",
    "    ret = []\n",
    "    rtime = []\n",
    "    for inp in inputs:\n",
    "        try:\n",
    "            res,delta = get(inp)\n",
    "            ret.append(res)\n",
    "            rtime.append(delta)\n",
    "        except FunctionTimedOut as e:\n",
    "            print(e)\n",
    "            ret.append(\"TimeLimitExceeding\")\n",
    "            rtime.append((1<<31)-1)\n",
    "    # try:\n",
    "    #     exec(code,None,local)\n",
    "    #     for key in local.keys:\n",
    "    #         del env[key]\n",
    "    # except:\n",
    "    #     print(\"del error\")\n",
    "    del env\n",
    "    if record_time:\n",
    "        return ret,rtime\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YMCRK85JVTQ96Q45HAW85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_execute(code:str,inputs:List,entry_point:str,record_time=True)->List:\n",
    "    env = {}\n",
    "    exec(code,None, env)\n",
    "    fn = env[entry_point]\n",
    "    ret = []\n",
    "    rtime = []\n",
    "    for i,inp in enumerate(inputs):\n",
    "        if record_time:\n",
    "            start = time.perf_counter_ns()\n",
    "            tmp = fn(*inp) #if fn.__code__.co_argcount > 1 else fn(inp)\n",
    "            end = time.perf_counter_ns()\n",
    "            ret.append(tmp)\n",
    "            rtime.append((end-start)//1000_000)\n",
    "        else:\n",
    "            tmp = fn(*inp) #if fn.__code__.co_argcount > 1 else fn(inp)\n",
    "            ret.append(tmp)\n",
    "    if record_time:\n",
    "        return ret,rtime\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YMNFT8NRMJBK1AAFMXE2W",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardExecute(dataset:Union[str,Path],inplace = False,is_force_override = False):\n",
    "    def trust_execute(code:str,inputs:List,entry_point:str,record_time=True)->List:\n",
    "        env = {}\n",
    "        exec(code,None, env)\n",
    "        fn = env[entry_point]\n",
    "        ret = []\n",
    "        rtime = []\n",
    "        for i,inp in enumerate(inputs):\n",
    "            if record_time:\n",
    "                start = time.perf_counter_ns()\n",
    "                ret.append(fn(*inp))\n",
    "                end = time.perf_counter_ns()\n",
    "                rtime.append((end-start)//1000_000)\n",
    "            else:\n",
    "                ret.append(fn(*inp))\n",
    "        if record_time:\n",
    "            return ret,rtime\n",
    "        return ret\n",
    "    if isinstance(dataset, str):\n",
    "        dataset = Path(dataset)\n",
    "    if dataset.suffix == \".json\":\n",
    "        with open(dataset,'r') as f:\n",
    "            data_origin = json.load(f)\n",
    "    elif dataset.suffix == '.jsonl':\n",
    "        with open(dataset,'r') as f:\n",
    "            data_origin = {}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                data_origin[item['task_id']] = item\n",
    "    else:\n",
    "        raise Exception(f\"{dataset.suffix} is not supported\")\n",
    "    \"\"\"            \n",
    "        {\"{{task_id}}\":{\n",
    "            \"task_id\":str,\n",
    "            \"prompt\":str,\n",
    "            \"entry_point\":str,\n",
    "            \"canonical_solution\":str[code],\n",
    "            \"base_input\":List[input],\n",
    "            \"plus_input\":List[input],\n",
    "            \"base\":List[output],\n",
    "            \"base_time\":List[float],\n",
    "            \"plus\":List[output],\n",
    "            \"plus_time\":List[float],\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    save = dataset.stem + \"_with_output.json\"\n",
    "    if os.path.exists(os.path.join(dataset.parent, save)) and not is_force_override:\n",
    "        print(\"The output file already exists\")\n",
    "        return \n",
    "    for task_id,problem in data_origin.items():\n",
    "        oracle = data_origin[task_id]\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = trust_execute(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"base_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = trust_execute(\n",
    "            problem[\"prompt\"] + problem[\"canonical_solution\"],\n",
    "            problem[\"plus_input\"],\n",
    "            problem[\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "    if not inplace:\n",
    "        with open(os.path.join(dataset.parent,save),'w') as f:\n",
    "            json.dump(data_origin,f,indent=4)\n",
    "    else:\n",
    "        with open(dataset,'w') as f:\n",
    "            json.dump(data_origin,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YQA52WQYV1294K8WXSWHB",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardExecute('./data/codeEval/data.json',is_force_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YMX7K59FP3DM5D8AP0B1D",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeExecute(file_path:Union[str,Path],dataset:Union[str,Path],exec_save_path:Union[str,Path] = None,is_force_override = False)->None:\n",
    "    if isinstance(dataset, str):\n",
    "        dataset = Path(dataset)\n",
    "    if dataset.suffix == \".json\":\n",
    "        with open(dataset,'r') as f:\n",
    "            data_origin = json.load(f)\n",
    "    elif dataset.suffix == '.jsonl':\n",
    "        with open(dataset,'r') as f:\n",
    "            \"\"\"            \n",
    "                {\"{{task_id}}\":{\n",
    "                    \"task_id\":str,\n",
    "                    \"prompt\":str,\n",
    "                    \"entry_point\":str,\n",
    "                    \"canonical_solution\":str[code],\n",
    "                    \"base_input\":List[input],\n",
    "                    \"plus_input\":List[input],\n",
    "                    \"base\":List[output],\n",
    "                    \"base_time\":List[float],\n",
    "                    \"plus\":List[output],\n",
    "                    \"plus_time\":List[float],\n",
    "                    }\n",
    "                }\n",
    "            \"\"\"\n",
    "            data_origin = {}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                data_origin[item['task_id']] = item\n",
    "    else:\n",
    "        raise Exception(f\"{dataset.suffix} is not supported\")\n",
    "    \n",
    "    if isinstance(file_path, str):\n",
    "        file_path = Path(file_path)\n",
    "    if file_path.suffix == \".json\":\n",
    "        with open(file_path,'r') as f:\n",
    "            data_eval = json.load(f)\n",
    "    elif file_path.suffix == '.jsonl':\n",
    "        with open(file_path,'r') as f:\n",
    "            # {\"task_id\":\"solution\",}\n",
    "            data_eval = {}\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                task_id = item['task_id']\n",
    "                data_eval[task_id] = item['solution'] if 'solution' in item else data_origin[task_id]['prompt']+item['completion']\n",
    "    else:\n",
    "        raise Exception(f\"{file_path.suffix} is not supported\")\n",
    "    \"\"\"\n",
    "        {\n",
    "            \"{{task_id}}\":{\n",
    "                \"info\":{\n",
    "                    \"task_id\":\"{{task_id}}\",\n",
    "                    \"prompt\":\"{{prompt}}\",\n",
    "                    \"entry_point\":\"{{entry_point}}\",\n",
    "                    \"canonical_solution\":\"{{canonical_solution}}\",\n",
    "                    \"base_input\":\"{{base_input}}\",\n",
    "                    \"plus_input\":\"{{plus_input}}\",\n",
    "                    \"atol\":float,\n",
    "                },\n",
    "                \"expected_output\":{\n",
    "                    \"base\":\"{{base}}\",\n",
    "                    \"base_time\":\"{{base_time}}\",\n",
    "                    \"plus\":\"{{plus}}\",\n",
    "                    \"plus_time\":\"{{plus_time}}\",\n",
    "                },\n",
    "                \"code_LLM\":\"{{solution}}\",\n",
    "                \"actual_output\":{\n",
    "                    \"base\":\"{{base}}\",\n",
    "                    \"base_time\":\"{{base_time}}\",\n",
    "                    \"plus\":\"{{plus}}\",\n",
    "                    \"plus_time\":\"{{plus_time}}\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    save = file_path.stem + \"_execute_results.json\"\n",
    "    if not exec_save_path:\n",
    "        parent = file_path.parent\n",
    "    else:\n",
    "        if isinstance(exec_save_path, str):\n",
    "            exec_save_path = Path(exec_save_path)\n",
    "        parent = exec_save_path if os.path.isdir(exec_save_path) else exec_save_path.parent\n",
    "    if os.path.exists(os.path.join(parent,save)) and not is_force_override:\n",
    "        print(\"The executeResults file already exists\")\n",
    "        return\n",
    "    for task_id,problem in data_origin.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = problem['base'],problem['base_time']\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = problem['plus'],problem['plus_time']\n",
    "        if task_id not in output:\n",
    "            output[task_id] = {}\n",
    "        output[task_id]['info'] = problem\n",
    "        output[task_id][\"expected_output\"] = oracle\n",
    "    for task_id,solution in data_eval.items():\n",
    "        oracle = {}\n",
    "        oracle[\"base\"], oracle[\"base_time\"] = execute(\n",
    "            solution,\n",
    "            data_origin[task_id][\"base_input\"],\n",
    "            data_origin[task_id][\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "        oracle[\"plus\"], oracle[\"plus_time\"] = execute(\n",
    "            solution,\n",
    "            data_origin[task_id][\"plus_input\"],\n",
    "            data_origin[task_id][\"entry_point\"],\n",
    "            record_time=True,\n",
    "        )\n",
    "        if task_id not in output:\n",
    "            output[task_id] = {}\n",
    "        output[task_id]['code_LLM'] = solution\n",
    "        output[task_id][\"actual_output\"] = oracle\n",
    "    with open(os.path.join(parent,save), \"w\") as f:\n",
    "        json.dump(output, f, indent=4,)\n",
    "    print(save)\n",
    "def is_floats(x) -> bool:\n",
    "    # check if it is float; List[float]; Tuple[float]\n",
    "    if isinstance(x, float):\n",
    "        return True\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return all(isinstance(i, float) for i in x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.dtype == np.float64 or x.dtype == np.float32\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YNAD1750FDWA6Z6PXPQCA",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeEvaluate(exec_file:Union[str,Path],eval_save_path:Union[str,Path] = None,is_force_override:bool = False):\n",
    "    def equal(exp:List,actual:List,atol = 0)->List[bool]:\n",
    "        ret_correct = []\n",
    "        if is_floats(exp):\n",
    "            if atol == 0:\n",
    "                atol = 1e-6\n",
    "            for i in range(len(exp)):\n",
    "                if isinstance(actual[i],str) or not np.isclose(exp[i],actual[i],atol=atol):\n",
    "                    ret_correct.append(False)\n",
    "                else:\n",
    "                    ret_correct.append(True)\n",
    "        else:\n",
    "            for i in range(len(exp)):\n",
    "                if actual[i] in [\"NotImplemented\",\"TimeLimitExceeding\",\"SyntaxError\"] or actual[i] != exp[i]:\n",
    "                    ret_correct.append(False)\n",
    "                else:\n",
    "                    ret_correct.append(True)\n",
    "        return ret_correct\n",
    "    if isinstance(exec_file, str):\n",
    "        exec_file = Path(exec_file)\n",
    "    if exec_file.suffix != \".json\":\n",
    "        raise Exception(f\"{exec_file.suffix} is not supported\")\n",
    "    with open(exec_file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    if not eval_save_path:\n",
    "        parent = exec_file.parent\n",
    "    else:\n",
    "        if isinstance(eval_save_path,str):\n",
    "            eval_save_path = Path(eval_save_path)\n",
    "        parent = eval_save_path if os.path.isdir(eval_save_path) else eval_save_path.parent\n",
    "            \n",
    "    save = exec_file.stem.replace('_execute_results','') + \"_eval_results.json\"\n",
    "    if os.path.exists(os.path.join(parent,save)) and not is_force_override:\n",
    "        print(\"The evalResults file already exists\")\n",
    "        return \n",
    "    eval_result = {}\n",
    "    \"\"\"\n",
    "        {\n",
    "            \"percision\":{\n",
    "                \"base\":float,\n",
    "                \"plus\":float,\n",
    "                \"mean\":float\n",
    "            },\n",
    "            \"AC\":int,\n",
    "            \"PASS\":float,\n",
    "            \"syntax_error_ratio\":{\n",
    "                \"base\":float,\n",
    "                \"plus\":float\n",
    "                \"mean\":float\n",
    "            },\n",
    "            \"accuracy_list\":{\n",
    "                \"base_accuracy_list\":List[float],\n",
    "                \"plus_accuracy_list\":List[float]\n",
    "            },\n",
    "            \"syntax_error_list\":{\n",
    "                \"base_syntax_error_list\":List[bool],\n",
    "                \"plus_syntax_error_list\":List[bool]\n",
    "            },\n",
    "            \"notImplemented_ratio_list\":List[float],\n",
    "            \"{{task_id}}\":{\n",
    "                \"base_eval\":List[bool],\n",
    "                \"base_num\":int,\n",
    "                \"base_AC\":int,\n",
    "                \"base_TLE\":int,\n",
    "                \"base_accuracy\":float,\n",
    "                \"base_exp_time\":List[time],\n",
    "                \"base_actual_time\":List[time],\n",
    "                \n",
    "                \"plus_eval\":List[bool],\n",
    "                \"plus_num\":int,\n",
    "                \"plus_AC\":int,\n",
    "                \"plus_TLE\":int,\n",
    "                \"plus_accuracy\":float,\n",
    "                \"plus_exp_time\":List[time],\n",
    "                \"plus_actual_time\":List[bool],\n",
    "                \n",
    "                \"accuracy\":float\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    eval_result['percision'] = {'base':0,'plus':0,\"mean\":0}\n",
    "    eval_result['AC'] = 0\n",
    "    eval_result['PASS'] = 0\n",
    "    AC_num = 0\n",
    "    eval_result['syntax_error_ratio'] = {'base':0,\"plus\":0,\"mean\":0}\n",
    "    base_accuracy_list = []\n",
    "    plus_accuracy_list = []\n",
    "    accuracy_list = {\"base_accuracy_list\":base_accuracy_list,\"plus_accuracy_list\":plus_accuracy_list}\n",
    "    eval_result['accuracy_list'] = accuracy_list\n",
    "    base_syntax_error_list = []\n",
    "    plus_syntax_error_list = []\n",
    "    eval_result['syntax_error_list'] ={ \"base_syntax_error_list\":base_syntax_error_list,\"plus_syntax_error_list\":plus_syntax_error_list}\n",
    "    notImplemented_list = []\n",
    "    eval_result['notImplemented_ratio_list'] = notImplemented_list\n",
    "    for task_id,eval in data.items():\n",
    "        oracle = {}\n",
    "        if 'atol' in eval['info']:\n",
    "            atol = eval['info']['atol']\n",
    "        else:\n",
    "            atol = 0\n",
    "        base_exp = eval['expected_output']['base']\n",
    "        base_actual = eval['actual_output']['base']\n",
    "        base_correct = equal(base_exp, base_actual,atol)\n",
    "        \n",
    "        oracle['base_num'] = len(base_correct)\n",
    "        oracle['base_AC'] = sum(base_correct)\n",
    "        oracle['base_TLE'] = base_actual.count(\"TimeLimitExceeding\")\n",
    "        oracle['base_SE'] = base_actual.count(\"SyntaxError\")\n",
    "        oracle['base_NoImplemented'] = base_actual.count(\"NotImplemented\")\n",
    "        oracle['base_accuracy'] = np.mean(base_correct)\n",
    "        base_accuracy_list.append(oracle['base_accuracy'])\n",
    "        base_syntax_error_list.append(oracle['base_SE']/oracle['base_num'])\n",
    "        \n",
    "        plus_exp = eval['expected_output']['plus']\n",
    "        plus_actual = eval['actual_output']['plus']\n",
    "        plus_correct = equal(plus_exp,plus_actual,atol)\n",
    "        \n",
    "        oracle['plus_num'] = len(plus_correct)\n",
    "        oracle['plus_AC'] = sum(plus_correct)\n",
    "        oracle['plus_TLE'] = plus_actual.count(\"TimeLimitExceeding\")\n",
    "        oracle['plus_SE'] = plus_actual.count(\"SyntaxError\")\n",
    "        oracle['plus_NoImplemented'] = plus_actual.count(\"NotImplemented\")\n",
    "        oracle['plus_accuracy'] = np.mean(plus_correct)\n",
    "        plus_accuracy_list.append(oracle['plus_accuracy'])\n",
    "        plus_syntax_error_list.append(oracle['plus_SE']/oracle['plus_num'])\n",
    "        \n",
    "        oracle['accuracy'] = np.mean(base_correct+plus_correct)\n",
    "        if oracle['accuracy'] == 1:\n",
    "            AC_num += 1\n",
    "        \n",
    "        oracle['base_exp_time'] = eval['expected_output']['base_time']\n",
    "        oracle['base_actual_time'] = eval['actual_output']['base_time']\n",
    "        oracle['plus_exp_time'] = eval['expected_output']['plus_time']\n",
    "        oracle['plus_actual_time'] = eval['actual_output']['plus_time']\n",
    "        \n",
    "        notImplemented_list.append((oracle['base_NoImplemented']+oracle['plus_NoImplemented'])/(oracle['base_num']+oracle['plus_num']))\n",
    "        \n",
    "        eval_result[task_id] = oracle\n",
    "    eval_result['percision']['base'] = np.mean(base_accuracy_list)\n",
    "    eval_result['percision']['plus'] = np.mean(plus_accuracy_list)\n",
    "    eval_result['percision']['mean'] = np.mean(base_accuracy_list+plus_accuracy_list)\n",
    "    eval_result['syntax_error_ratio']['base'] = np.mean(base_syntax_error_list)\n",
    "    eval_result['syntax_error_ratio']['plus'] = np.mean(plus_syntax_error_list)\n",
    "    eval_result['syntax_error_ratio']['mean'] = np.mean(base_syntax_error_list+plus_syntax_error_list)\n",
    "    eval_result['AC'] = AC_num\n",
    "    eval_result['PASS'] = AC_num/len(data)\n",
    "    with open(os.path.join(parent,save), \"w\") as f:\n",
    "        json.dump(eval_result, f, indent=4,)\n",
    "    print(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YS9AV023N5CM459JXJA01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir,files in os.walk('./data/codeEval/code_sanitize'):\n",
    "    for file in files:\n",
    "        codeExecute(os.path.join(dir,file),\"./data/codeEval/data_with_output.json\",exec_save_path = './data/codeEval/code_execute',is_force_override=True)\n",
    "        codeEvaluate(os.path.join('./data/codeEval/code_execute',file.replace(\".jsonl\",\"_execute_results.json\")),eval_save_path = './data/codeEval/code_eval',is_force_override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YTWH6GBQ5MRPSTGZQMX5V",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = {'accuracy':{},\"pass\":{},\"AC\":{}}\n",
    "for dir,subdir,files in os.walk('./data/codeEval/code_eval'):\n",
    "    for file in files:\n",
    "        model = file.replace('_eval_results.json','')\n",
    "        with open(os.path.join(dir,file)) as f:\n",
    "            data = json.load(f)\n",
    "        data_eval['accuracy'][model] = data['percision']['mean']\n",
    "        data_eval['pass'][model] = data['PASS']\n",
    "        data_eval['AC'][model] = data['AC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01HX3YV38AKTXAE08GHFR9Y1SG",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_eval).sort_values(by=\"AC\",ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

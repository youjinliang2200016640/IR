{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,re,os,datetime,time,string\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import langchain\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional,Dict, Mapping, Union\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import collections\n",
    "import traceback,random\n",
    "import pandas as pd\n",
    "import evalplus.sanitize,evalplus.syncheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_API = \"sk-c7cssl4bkglsrwf2\"\n",
    "INFINI_API_2 = \"sk-c7erk6qaqhkz5t72\"\n",
    "INFINI_API_List = [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = {\n",
    "    'llama-3-70b-instruct':\"Llama3系列是由Meta开发的Llama系列全新的第三代版本，包含一系列预训练和指令调优的文本生成式模型。Llama3基于优化后的Transformer架构，预训练过程中使用了超过15T tokens的数据，调优后的模型使用SFT和RLHF，以更好地贴合人类对可用性和安全性的偏好。Llama3-70b-Instruct是此系列里，700亿参数的指令调优的模型，针对对话场景用例进行了优化，并在常见的行业基准测试中超越了许多可用的开源聊天模型。Llama3-70b-Instruct支持模型上下文至8k tokens，该模型的数据的知识截止日期为2023年12月。\",\n",
    "    'llama-3-8b-instruct':\"Llama3系列是由Meta开发的Llama系列全新的第三代版本，包含一系列预训练和指令调优的文本生成式模型。Llama3基于优化后的Transformer架构，预训练过程中使用了超过15T tokens的数据，调优后的模型使用SFT和RLHF，以更好地贴合人类对可用性和安全性的偏好。Llama3-8b-Instruct是此系列里，80亿参数的指令调优的模型，针对对话场景用例进行了优化，并在常见的行业基准测试中超越了许多可用的开源聊天模型。Llama3-8b-Instruct支持模型上下文至8k tokens，该模型的数据的知识截止日期为2023年3月。\",\n",
    "    'chatglm3':\"ChatGLM3是智谱AI与清华KEG实验室发布的闭源模型，支持 8K 上下文，经过海量中英标识符的预训练与人类偏好对齐训练，相比一代模型在 MMLU、C-Eval、GSM8K 分别取得了16%、36%、280%的提升，并登顶中文任务榜单C-Eval。适用于对知识量、推理能力、创造力要求较高的场景，比如广告文案、小说写作、知识类写作、代码生成等。\",\n",
    "    'chatglm2-6b':\"ChatGLM2-6b 是由智谱开发的 ChatGLM 系列的第二代版本，支持中英双语的60亿参数规模的开源模型。在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，在 MMLU、C-Eval、GSM8K、BBH等主流学术数据集上，都得到了显著的性能提升，并通过基于 FlashAttention 技术，将对话模型的上下文长度（Context Length）提升至 8k tokens，允许更多轮次的对话。\",\n",
    "    'chatglm2-6b-32k':\"ChatGLM2-6b 是由智谱开发的 ChatGLM 系列的第二代版本，支持中英双语的60亿参数规模的开源模型。相较于ChatGLM2-6B，ChatGLM2-6b-32k支持更长的模型上下文至32k tokens。\",\n",
    "    'infini-megrez-7b':\"由无问芯穹公司自主研发的70亿参数大语言模型。在逻辑推理、对话能力等方面有优秀的性能表现。配合无问芯穹自研高效推理引擎，同时支持Nvidia和AMD的GPU，具备更快的推理速度，在性能表现方面更上一层楼。\",\n",
    "    'llama-2-7b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-7b-chat是其中70亿的主流参数大小的模型，适用于chat场景，更擅长英文相关的内容。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-13b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-7b-chat是其中70亿的主流参数大小的模型，适用于chat场景，更擅长英文相关的内容。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-70b-chat':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-70b-chat是其中700亿参数的大模型，适用于chat场景，更擅长英文相关的内容，相较该系列里其他规模的的模型，有更强的综合能力。模型支持 4k tokens上下文。\",\n",
    "    'llama-2-70b':\"Llama2是由Meta开发并开源的大型语言模型（LLM）系列，这是一组从70亿到700亿参数不同规模、经过预训练和微调的生成式文本模型。架构层面，Llama2是一个使用优化型转换器架构的自动回归语言模型。调整后的版本使用有监督的微调（SFT）和带有人类反馈的强化学习（RLHF）以对齐人类对有用性和安全性的偏好。Llama2较Llama系列在多种学术数据集上有着更加不俗的表现，为大量其他模型提供了设计和开发的思路。Llama2-70b-base是其中700亿参数的基础大模型，适用于通用语言任务场景，更擅长英文相关的内容，相较该系列里其他规模的的模型，有更强的综合能力。模型支持 4k tokens上下文。\",\n",
    "    'baichuan2-7b-chat':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-7b-chat是130亿参数规模用于对话的模型，在C-Eval、MMLU、CMMLU等主流评测数据集上都有不俗的表现。该基模型支持4k tokens上下文。\",\n",
    "    'baichuan2-13b-chat':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-13b-chat是130亿参数规模用于对话的模型，在C-Eval、MMLU、CMMLU等主流评测数据集上都有不俗的表现。该基模型支持8k tokens上下文。\",\n",
    "    'baichuan2-13b-base':\"Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan2-13b-base是130亿参数规模的基础模型，适用于通用对话和文本续写，较chat模型更适合于复杂场景的微调后使用。该基模型支持4k tokens上下文。\",\n",
    "    'chatglm3-6b':\"ChatGLM3-6b 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源模型。ChatGLM3采用了全新设计的 Prompt 格式，并原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。模型支持 8k tokens上下文。\",\n",
    "    'chatglm3-6b-32k':\"ChatGLM3-6b 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源模型。相较于ChatGLM之前系列的模型，ChatGLM3采用了更多样的训练数据，并原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。ChatGLM3-6b-32k在ChatGLM3-6b 基础上进一步强化了对于长文本的理解能力，能够更好的处理最多32k tokens长度的上下文。\",\n",
    "    'chatglm3-6b-base':\"ChatGLM3-6b-base 是由智谱开发的 ChatGLM 系列最新一代的60亿参数规模的开源的基础模型。ChatGLM3-6B-Base 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。基础模型更适合于复杂场景的微调后使用，该基模型支持32k tokens上下文。\",\n",
    "    'qwen-7b-chat':\"通义千问-7B-chat（Qwen-7B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的70亿参数规模的大语言模型。相较于Qwen-7B-Base模型，Qwen-7B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 8k tokens上下文。\",\n",
    "    'qwen-14b-chat':\"通义千问-14B-chat（Qwen-14B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的140亿参数规模的大语言模型。相较于Qwen-14B-Base模型，Qwen-14B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 8k tokens上下文。\",\n",
    "    'qwen-72b-chat':\"通义千问-72B-chat（Qwen-72B-chat）是阿里云研发的基于Transformer，在超大规模的预训练数据上进行训练得到的720亿参数规模的大语言模型。相较于Qwen-72B-Base模型，Qwen-72B-chat是针对于对话场景以及一些常见的智能对话需求指令对齐的AI助手模型，在更多文本相关的问答场景上有更好的指令跟随能力。模型支持 32k tokens上下文。\",\n",
    "    'qwen-72b':\"通义千问-72B（Qwen-72B）是阿里云研发的通义千问大模型系列的720亿参数规模的模型。Qwen-72B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。模型支持 32k tokens上下文。\",\n",
    "    'qwen1.5-7b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-7b-chat是其中专用于chat场景的70亿参数的主流大小模型。\",\n",
    "    'qwen1.5-14b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-14b-chat是其中专用于chat场景的140亿参数的主流大小模型。\",\n",
    "    'qwen1.5-72b-chat':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-72b-chat是其中专用于chat场景的720亿参数的大模型。\",\n",
    "    'qwen1.5-72b':\"Qwen1.5系列是Qwen2的Beta版本，是一个基于Transformer的仅解码语言模型，在海量数据上进行预训练。与之前发布的Qwen系列版本相比，Qwen1.5系列base与chat模型均能支持多种语言，在整体聊天和基础能力上都得到了提升，并且支持32k tokens上下文。Qwen1.5-72b-base是其中的720亿参数的基础大模型，适合多种场景的使用。\",\n",
    "}\n",
    "answerModelList = [    \n",
    "    'llama-3-8b-instruct',\n",
    "    # 'chatglm3',\n",
    "    # 'chatglm2-6b',\n",
    "    # 'chatglm2-6b-32k',\n",
    "    'infini-megrez-7b',\n",
    "    'llama-2-7b-chat',\n",
    "    'llama-2-13b-chat',\n",
    "    'llama-2-70b-chat',\n",
    "    'llama-2-70b',\n",
    "    'baichuan2-7b-chat',\n",
    "    'baichuan2-13b-chat',\n",
    "    'baichuan2-13b-base',\n",
    "    # 'chatglm3-6b',\n",
    "    # 'chatglm3-6b-32k',\n",
    "    # 'chatglm3-6b-base',\n",
    "    'qwen-7b-chat',\n",
    "    'qwen-14b-chat',\n",
    "    'qwen-72b-chat',\n",
    "    'qwen-72b',\n",
    "    'qwen1.5-7b-chat',\n",
    "    'qwen1.5-14b-chat',\n",
    "    'qwen1.5-72b',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLMCompletions(prompt,modelName:str = \"infini-megrez-7b\",INFINI_API = \"sk-c7cssl4bkglsrwf2\",returnContent:bool = True,**kwargs):\n",
    "    url = \"https://cloud.infini-ai.com/maas/\"+modelName+\"/nvidia/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"string\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else 0.7,\n",
    "        \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else 1,\n",
    "        \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else -1,\n",
    "        \"n\": kwargs['n'] if 'n' in kwargs else 1,\n",
    "        \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else None,\n",
    "        \"stop\": kwargs['stop'] if 'stop' in kwargs else None,\n",
    "        \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else 0,\n",
    "        \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else 0\n",
    "    }\n",
    "    headers = {\n",
    "            'Content-Type': \"application/json\",\n",
    "            'Accept': \"*/*\",\n",
    "            'Authorization': \"Bearer \"+INFINI_API,\n",
    "    } \n",
    "    response = requests.post(url, json=payload, headers=headers,)\n",
    "    if response.status_code == 200:\n",
    "        response.encoding = 'utf-8'\n",
    "        data = response.json()\n",
    "        content = data['choices'][0]['message']['content']\n",
    "        \n",
    "        content = content.replace(',\\n}','\\n}')\n",
    "        if returnContent:\n",
    "            return content\n",
    "        try:\n",
    "            content = json.loads(content)\n",
    "        except:\n",
    "            content = content.replace('\\n','')\n",
    "        data['choices'][0]['message']['content'] = content\n",
    "        \n",
    "        return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        return \"Cannot connect to the model \"+modelName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_ins = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'time':-1\n",
    "}\n",
    "def joinErrorToData(errorFile,saveFile):\n",
    "    with open(errorFile, 'r') as ef:\n",
    "        data_ef = json.load(ef)\n",
    "    fileName = data_ef['fileName']\n",
    "    with open(saveFile) as sf:\n",
    "        data_sf = json.load(sf)\n",
    "    if fileName != data_sf['fileName']:\n",
    "        print('FileName not match')\n",
    "        return \n",
    "    if not data_ef['data']:\n",
    "        print('The Errors of this ErrorFile all have been solved!')\n",
    "        return \n",
    "    errorItem = []\n",
    "    for item in data_ef['data']:\n",
    "        data_format_ins['id'] = item['id']\n",
    "        model = item['AnswerModel']\n",
    "        data_format_ins['AnswerModel'] = model\n",
    "        prompt = item['input']\n",
    "        data_format_ins['input'] = prompt\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        start = time.perf_counter_ns()\n",
    "        actual_output =  LLMCompletions(prompt,modelName=model,INFINI_API=INFINI_API_2)\n",
    "        end = time.perf_counter_ns()\n",
    "        delta = end-start\n",
    "        idx = 0\n",
    "        while actual_output == \"Cannot connect to the model \"+model and idx<2:\n",
    "            start = time.perf_counter_ns()\n",
    "            actual_output =  LLMCompletions(prompt,modelName=model,INFINI_API=INFINI_API_2)\n",
    "            end = time.perf_counter_ns()\n",
    "            delta = end-start\n",
    "            idx += 1\n",
    "        if actual_output == \"Cannot connect to the model \"+model:\n",
    "            errorItem.append(item)\n",
    "            print(\"[error]:\\t\"+str(errorItem[-1]))\n",
    "            continue\n",
    "        print(actual_output)\n",
    "        data_format_ins['time'] = delta\n",
    "        data_format_ins['actual_output'] = actual_output\n",
    "        data_sf['data'].append(data_format_ins.copy())\n",
    "    data_ef['data'] = errorItem\n",
    "    with open(saveFile,'w') as saveF:\n",
    "        json.dump(data_sf,saveF)\n",
    "    with open(errorFile,'w') as error:\n",
    "        json.dump(data_ef,error)\n",
    "    if errorItem:\n",
    "        print(\"There are still some errors! \")\n",
    "    else:\n",
    "        print('The Errors of this ErrorFile all have been solved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDeprecatedModel(filePath):\n",
    "    with open(filePath) as f:\n",
    "        data = json.load(f)\n",
    "    new_data = []\n",
    "    for item in data['data']:\n",
    "        if item['AnswerModel']  in answerModelList:\n",
    "            new_data.append(item)\n",
    "    data['data'] = new_data[:]\n",
    "    with open(filePath,'w') as f:\n",
    "        json.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeDeprecatedModel('./data/1-2_1_low_freq_ent_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joinErrorToData('./data/4-2_r_with_triples_sampleError.json','./data/4-2_r_with_triples_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qapairs = []\n",
    "# model_input_id = []\n",
    "# with open('./data/1-1_2_high_freq_ent_sample.json') as saveF:\n",
    "#     data = json.load(saveF)\n",
    "# with open('E:/Repository/KoLA/Sample_Data/1-1_2_high_freq_ent_sample.json') as file:\n",
    "#     data_o = json.load(file)\n",
    "# instructions = data_o['adapter_spec']['instructions']\n",
    "# for item in data_o['request_states']:\n",
    "#     qapairs.append((item['instance']['id'],instructions+'\\n'+item['instance']['input']['text']))\n",
    "# for item in data['data']:\n",
    "#     model_input_id.append([item['AnswerModel'],item['input'],item['id']])\n",
    "# check = [ False for i in range(len(model_input_id))]\n",
    "# for model in answerModelList:\n",
    "#     for id,text in qapairs:\n",
    "#         for i,item in enumerate(model_input_id):\n",
    "#             if model == item[0] and text == item[1] and id == item[2]:\n",
    "#                 check[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model,text,id in model_input_id:\n",
    "#     diff[model] += 1\n",
    "#     diff[text] += 1\n",
    "#     diff[id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### deprecated ChatLLM,CustomLLM\n",
    "# class ChatLLM(LLM):\n",
    "#     modelName = \"qwen1.5-72b-chat\"\n",
    "#     INFINI_API = \"sk-c7cssl4bkglsrwf2\"\n",
    "#     temperature = 0.8\n",
    "#     top_p = 1\n",
    "#     top_k = -1\n",
    "#     n = 1\n",
    "#     max_tokens:int = None\n",
    "#     stop:Optional[List[str]] = None\n",
    "#     presence_penalty = 0\n",
    "#     frequency_penalty = 0\n",
    "    \n",
    "#     headers = {\n",
    "#             'Content-Type': \"application/json\",\n",
    "#             'Accept': \"*/*\",\n",
    "#             'Authorization': \"Bearer \"+INFINI_API,\n",
    "#     }   \n",
    "#     @property\n",
    "#     def _llm_type(self)->str:\n",
    "#         return \"ChatLLM\"\n",
    "#     @property\n",
    "#     def _identifying_params(self)->Mapping[str,Any]:\n",
    "#         _param_dict = {\n",
    "#             \"modelName\":ChatLLM().modelName,\n",
    "#             \"INFINI_API\":ChatLLM().INFINI_API,\n",
    "#             \"stream\":bool(ChatLLM().stream),\n",
    "#             \"temperature\":ChatLLM().temperature,\n",
    "#             \"top_p\":ChatLLM().top_p,\n",
    "#             \"top_k\":ChatLLM().top_k,\n",
    "#             \"n\":ChatLLM().n,\n",
    "#             \"max_tokens\":ChatLLM().max_tokens,\n",
    "#             \"stop\":ChatLLM().stop,\n",
    "#             \"presence_penalty\":ChatLLM().presence_penalty,\n",
    "#             \"frequency_penalty\":ChatLLM().frequency_penalty,\n",
    "#         }\n",
    "#         return _param_dict\n",
    "#     @classmethod  \n",
    "#     def _call(self, prompt: str, stop: Optional[List[str]]= None,  **kwargs: Any) -> str:\n",
    "#         url = \"https://cloud.infini-ai.com/maas/\"+ChatLLM().modelName+\"/nvidia/chat/completions\"\n",
    "#         payload = {\n",
    "#             \"model\": \"string\",\n",
    "#             \"messages\": [\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": prompt\n",
    "#                 }\n",
    "#             ],\n",
    "#             \"stream\": False,\n",
    "#             \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else ChatLLM().temperature,\n",
    "#             \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else ChatLLM().top_p,\n",
    "#             \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else ChatLLM().top_k,\n",
    "#             \"n\": kwargs['n'] if 'n' in kwargs else ChatLLM().n,\n",
    "#             \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else ChatLLM().max_tokens,\n",
    "#             \"stop\": stop if stop else ChatLLM().stop,\n",
    "#             \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else ChatLLM().presence_penalty,\n",
    "#             \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else ChatLLM().frequency_penalty\n",
    "#         }\n",
    "#         response = requests.post(url, json=payload, headers=ChatLLM().headers)\n",
    "#         if response.status_code == 200:\n",
    "#             response.encoding = 'utf-8'\n",
    "#             data = response.json()\n",
    "#             content = data['choices'][0]['message']['content']\n",
    "#             if isinstance(content,str):    \n",
    "#                 content = content.replace(',\\n}','\\n}')\n",
    "#                 content = content.replace(']\\n}',']}')\n",
    "#                 content = content.replace('\\\\','\\\\\\\\')\n",
    "#                 flag = False\n",
    "#                 if 'statements' in content:\n",
    "#                     regex = re.compile('\\\"statements\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "#                     flag = True\n",
    "#                 elif 'verdicts' in content:\n",
    "#                     regex = re.compile('\\\"verdicts\\\":\\s+\\[.*\\]\\}', re.DOTALL) \n",
    "#                     flag = True\n",
    "#                 if flag:\n",
    "#                     matchStr =regex.search(content)\n",
    "#                     if matchStr:\n",
    "#                         content = '{'+matchStr.group()\n",
    "#             try:\n",
    "#                 content = json.loads(content)\n",
    "#             except:\n",
    "#                 pass\n",
    "#             data['choices'][0]['message']['content'] = content\n",
    "                \n",
    "#             return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "#         else:\n",
    "#             return \"Cannot connect to the model \"+ChatLLM().modelName\n",
    "#     def setParameter(self,**kwargs):\n",
    "#         self.temperature = kwargs[\"temperature\"] if \"temperature\" in kwargs else ChatLLM().temperature\n",
    "#         self.top_p = kwargs['top_p'] if 'top_p' in  kwargs else ChatLLM().top_p\n",
    "#         self.top_k = kwargs['top_k'] if 'top_k' in  kwargs else ChatLLM().top_k\n",
    "#         self.n = kwargs['n'] if 'n' in kwargs else ChatLLM().n\n",
    "#         self.max_tokens = kwargs['max_tokens'] if 'max_tokens' in kwargs else ChatLLM().max_tokens\n",
    "#         self.stop = kwargs['stop'] if 'stop' in kwargs else ChatLLM().stop\n",
    "#         self.presence_penalty = kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else ChatLLM().presence_penalty\n",
    "#         self.frequency_penalty = kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else ChatLLM().frequency_penalty\n",
    "\n",
    "# class CustomLLM(DeepEvalBaseLLM):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model\n",
    "#     ):\n",
    "#         self.model = model\n",
    "\n",
    "#     def load_model(self):\n",
    "#         return self.model\n",
    "\n",
    "#     def generate(self, prompt: str) -> str:\n",
    "#         chat_model = self.load_model()\n",
    "#         return chat_model.invoke(prompt)\n",
    "#     async def a_generate(self, prompt: str) -> str:\n",
    "#         chat_model = self.load_model()\n",
    "#         res = await chat_model.ainvoke(prompt)\n",
    "#         return res\n",
    "\n",
    "#     def get_model_name(self):\n",
    "#         return \"CustomLLM\"\n",
    "\n",
    "# custom_model = ChatLLM()\n",
    "\n",
    "# evaluateModel = CustomLLM(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatLLM(LLM):\n",
    "    @property\n",
    "    def modelName(self)->str:\n",
    "        return \"qwen1.5-72b-chat\"\n",
    "    @property\n",
    "    def INFINI_API_List(self)->List[str]:\n",
    "        return [\"sk-c7cssl4bkglsrwf2\", \"sk-c7erk6qaqhkz5t72\",\"sk-c7etq7veyeie4dn2\"]\n",
    "    @property\n",
    "    def temperature(self)->float:\n",
    "        return 0.7\n",
    "    @property\n",
    "    def top_p(self)->float:\n",
    "        return 0.1\n",
    "    @property\n",
    "    def top_k(self)->int:\n",
    "        return -1\n",
    "    @property\n",
    "    def n(self)->int:\n",
    "        return 1\n",
    "    @property\n",
    "    def max_tokens(self)->int:\n",
    "        return None\n",
    "    @property\n",
    "    def stop(self)->Optional[List[str]]:\n",
    "        return None\n",
    "    @property\n",
    "    def presence_penalty(self)->float:\n",
    "        return 0\n",
    "    @property\n",
    "    def frequency_penalty(self)->float:\n",
    "        return 0\n",
    "    def getHeader(self,index_api):  \n",
    "        headers = {\n",
    "            'Content-Type': \"application/json\",\n",
    "            'Accept': \"*/*\",\n",
    "            'Authorization': \"Bearer \"+self.INFINI_API_List[index_api%len(self.INFINI_API_List)],\n",
    "        }\n",
    "        return headers\n",
    "    @property\n",
    "    def _llm_type(self)->str:\n",
    "        return \"ChatLLM\"\n",
    "    @property\n",
    "    def _identifying_params(self)->Mapping[str,Any]:\n",
    "        _param_dict = {\n",
    "            \"modelName\":self.modelName,\n",
    "            \"INFINI_API\":self.getHeader(self.__fields__['index_api'] if 'index_api' in self.__fields__ else 0),\n",
    "            \"stream\":bool(self.stream),\n",
    "            \"temperature\":self.temperature,\n",
    "            \"top_p\":self.top_p,\n",
    "            \"top_k\":self.top_k,\n",
    "            \"n\":self.n,\n",
    "            \"max_tokens\":self.max_tokens,\n",
    "            \"stop\":self.stop,\n",
    "            \"presence_penalty\":self.presence_penalty,\n",
    "            \"frequency_penalty\":self.frequency_penalty,\n",
    "        }\n",
    "        return _param_dict\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]= None, run_manager= None,**kwargs: Any) -> str:\n",
    "        url = \"https://cloud.infini-ai.com/maas/\"+str(self.modelName)+\"/nvidia/chat/completions\"\n",
    "        payload = {\n",
    "            \"model\": \"string\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"temperature\": kwargs[\"temperature\"] if \"temperature\" in kwargs else self.temperature,\n",
    "            \"top_p\": kwargs[\"top_p\"] if 'top_p' in kwargs else self.top_p,\n",
    "            \"top_k\": kwargs['top_k'] if 'top_k' in kwargs else self.top_k,\n",
    "            \"n\": kwargs['n'] if 'n' in kwargs else self.n,\n",
    "            \"max_tokens\": kwargs['max_tokens'] if 'max_tokens' in kwargs else self.max_tokens,\n",
    "            \"stop\": stop if stop else self.stop,\n",
    "            \"presence_penalty\": kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else self.presence_penalty,\n",
    "            \"frequency_penalty\": kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else self.frequency_penalty\n",
    "        }\n",
    "        index = 0\n",
    "        if 'index_api' not in self.__fields__:\n",
    "            self.__fields__['index_api'] = -1\n",
    "        index_api = self.__fields__['index_api']+1\n",
    "        length = len(self.INFINI_API_List)\n",
    "        while index < length:\n",
    "            response = requests.post(url, json=payload, headers=self.getHeader(index_api))\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                data = response.json()\n",
    "                print(\"response json success\")\n",
    "                content = data['choices'][0]['message']['content']\n",
    "                if isinstance(content,str):   \n",
    "                    content = content.replace(',\\n}','\\n}')\n",
    "                    content = content.replace(']\\n}',']}')\n",
    "                    if 'statements' in content:\n",
    "                        regex = re.compile('\\\"statements\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "                        matchStr =regex.search(content)\n",
    "                        if matchStr:\n",
    "                            content = '{'+matchStr.group()\n",
    "                    elif 'verdicts' in content:\n",
    "                        regex = re.compile('\\\"verdicts\\\":\\s+\\[.*\\]\\}',re.DOTALL)\n",
    "                        matchStr =regex.search(content)\n",
    "                        if matchStr is not None:\n",
    "                            content ='{' +matchStr.group()\n",
    "                            regex = re.compile(\"\\\"reason\\\":(.*?)\\}\",re.DOTALL)\n",
    "                            matchStr = regex.findall(content)\n",
    "                            for string in matchStr:\n",
    "                                tmp = string.strip()[1:-1].replace('\"','\\\\\\\"')\n",
    "                                tmp = '\\\"'+tmp+\"\\\"\"\n",
    "                                content = content.replace(string,tmp)\n",
    "                if isinstance(content,str):\n",
    "                    return content\n",
    "                data['choices'][0]['message']['content'] = content\n",
    "                return json.dumps(data['choices'][0]['message']['content'])\n",
    "\n",
    "            index += 1\n",
    "            index_api =  (index_api+1)%length\n",
    "            self.__fields__['index_api'] = index_api\n",
    "            print(response.status_code)\n",
    "            try:\n",
    "                print(response.json())\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "        return \"Cannot connect to the model \"+self.modelName\n",
    "    def setParameter(self,**kwargs):\n",
    "        self.temperature = kwargs[\"temperature\"] if \"temperature\" in kwargs else self.temperature\n",
    "        self.top_p = kwargs['top_p'] if 'top_p' in  kwargs else self.top_p\n",
    "        self.top_k = kwargs['top_k'] if 'top_k' in  kwargs else self.top_k\n",
    "        self.n = kwargs['n'] if 'n' in kwargs else self.n\n",
    "        self.max_tokens = kwargs['max_tokens'] if 'max_tokens' in kwargs else self.max_tokens\n",
    "        self.stop = kwargs['stop'] if 'stop' in kwargs else self.stop\n",
    "        self.presence_penalty = kwargs[\"presence_penalty\"]  if 'presence_penalty' in kwargs else self.presence_penalty\n",
    "        self.frequency_penalty = kwargs['frequency_penalty'] if 'frequency_penalty' in kwargs else self.frequency_penalty\n",
    "    \n",
    "\n",
    "class CustomLLM(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        global path\n",
    "        chat_model = self.load_model()\n",
    "        ret = chat_model.invoke(prompt)\n",
    "        idx = 0\n",
    "        if ret == \"Cannot connect to the model \"+self.get_model_name() and idx<2:\n",
    "            time.sleep(5)\n",
    "            ret = chat_model.invoke(prompt)\n",
    "            idx += 1\n",
    "        return ret\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        try:\n",
    "            return self.model.modelName\n",
    "        except:\n",
    "            return \"CustomLLM\"\n",
    "custom_model = ChatLLM()\n",
    "evaluateModel = CustomLLM(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\\"verdicts\\\": [{\\\"verdict\\\":\\\"yes\\\"} , {\\\"verdict\\\":\\\"no\\\",\\\"reason\\\":\\\"123\\\"}]\"\n",
    "regex = re.compile(\"\\{\\s*\\\"verdict\\\"\\s*:.*?\\}\")\n",
    "matchStr = regex.findall(content)\n",
    "matchStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_format_example = {\n",
    "    'metric_metadata':{\n",
    "        'metric':None,\n",
    "        'threshold':0,\n",
    "        'success':True,\n",
    "        'score':0.8,\n",
    "        'reason':'',\n",
    "        'strictMode': False,\n",
    "        'evaluationModel': 'CustomLLM',\n",
    "        'evaluationCost': 0\n",
    "    },\n",
    "    'metric_configuration': {\n",
    "        'threshold': 0.5,\n",
    "        'evaluation_model': 'CustomLLM',\n",
    "        'strict_mode': False,\n",
    "        'include_reason': True\n",
    "    }\n",
    "}\n",
    "\n",
    "data_format_example = {\n",
    "    'id':0,\n",
    "    'AnswerModel':'',\n",
    "    'input':'',\n",
    "    'actual_output':'',\n",
    "    'expected_output':None,\n",
    "    'retrieval_context':None,\n",
    "    'cached_metrics_data':[\n",
    "        {\n",
    "            'metric_metadata':{\n",
    "                'metric':None,\n",
    "                'success':True,\n",
    "                'score':0.8,\n",
    "                'reason':'',\n",
    "                'statements':'',\n",
    "                'verdicts':'',\n",
    "                'evaluationCost': 0\n",
    "            },\n",
    "            'metric_configuration': {\n",
    "                'threshold': 0.5,\n",
    "                'evaluation_model': 'CustomLLM',\n",
    "                'strict_mode': False,\n",
    "                'include_reason': True\n",
    "            }\n",
    "        },\n",
    "        metrics_format_example\n",
    "    ]\n",
    "}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirName,subDirName ,fileNames in os.walk('./data'):\n",
    "    print(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = ['1-3_r_1_simple_sample_sample.json', '2-1_COPEN++csj_sample.json', '2-2_COPEN++cpj_sample.json', '2-3_COPEN++cic_sample.json', '2-4_FewNERD++inter_sample.json', '2-4_FewNERD++intra_sample.json', '2-4_FewNERD++supervised_sample.json', '2-5_DocRED_sample.json', '2-6_MAVEN_sample.json', '2-7_MAVEN-ERE_sample.json', '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-3_musique_sample.json', '3-4_kqapro_sample.json', '3-5_KoRC++ood_sample.json', '3-6_r_KoRC++ood_sample.json', '4-1_without_triples_sample.json', '4-1_with_triples_sample.json', '4-2_r_without_triples_sample.json', '4-2_r_with_triples_sample.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric,FaithfulnessMetric,HallucinationMetric,BaseMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics  = [\n",
    "    AnswerRelevancyMetric(\n",
    "        threshold=0.5,\n",
    "        model=evaluateModel,\n",
    "        include_reason=True\n",
    "    ),\n",
    "    HallucinationMetric(\n",
    "        threshold=0.5,\n",
    "        model=evaluateModel,\n",
    "        include_reason=True\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./eval/error/2-1_COPEN++csj_sample.json') as f:\n",
    "#     data = json.load(f)\n",
    "# data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_case = LLMTestCase(input=data['data'][0]['input'],\n",
    "#                         actual_output=data['data'][0]['actual_output'],\n",
    "#                         context=[data['data'][0]['expected_output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics[0].measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(filename:Union[str,Path],save_file:Union[str,Path],error_file:Union[str,Path],force_save:bool = False,metrics:List[BaseMetric] = [AnswerRelevancyMetric(threshold=0.5,model=evaluateModel,include_reason=True),HallucinationMetric(threshold=0.5,model=evaluateModel,include_reason=True)]):\n",
    "    \"\"\"_summary_\n",
    "        the function is used to evaluate the LLM output saved in `filename` by the metric in `metrics`,the successful eval results will be saved into `save_file` and the error item will be saved into `error_file`\n",
    "    \n",
    "    Args:\n",
    "        `filename` (Union[str,Path]): the filename saves the LLM generation results\n",
    "        `save_file` (Union[str,Path]): the filename will save the evaluate results\n",
    "        `error_file` (Union[str,Path]): the filename will save the error eval item\n",
    "        `force_save` (bool, optional): if the value is `True`,function will rerun all eval item in `filename` and directly override the `save_file` and `error_file`. \n",
    "                    Defaults to False.\n",
    "        `metrics` (List[BaseMetric], optional): a list of evaluation metrics. \n",
    "                    Defaults to [AnswerRelevancyMetric(threshold=0.5,model=evaluateModel,include_reason=True),HallucinationMetric(threshold=0.5,model=evaluateModel,include_reason=True)].\n",
    "    \"\"\"\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id'] and x['input'] == item['input'] and x['actual_output'] == item['actual_output'] and x['AnswerModel'] == item['AnswerModel'] and x['expected_output'] == item['expected_output']:\n",
    "            return True\n",
    "        return False\n",
    "    with open(filename,'r') as f:\n",
    "        data = json.load(f)\n",
    "    save,error = dict(),dict()\n",
    "    if  Path(save_file).is_file() and not force_save:\n",
    "        with open(save_file) as f:\n",
    "            save = json.load(f)\n",
    "        if 'fileName' in save and  save['fileName'] != data['fileName']:\n",
    "            print(\"The save_file does not match the file name!\")\n",
    "            return\n",
    "    else:\n",
    "        save = {'fileName':data['fileName'],'class':data['class'],'data':[]}\n",
    "\n",
    "    if Path(error_file).is_file() and not force_save:\n",
    "        with open(error_file) as f:\n",
    "            error = json.load(f)\n",
    "        if 'fileName' in error and  error['fileName'] != data['fileName']:\n",
    "            print(\"The error_file does not match the file name!\")\n",
    "            return\n",
    "    else:\n",
    "        error = {'fileName' :data['fileName'],'class':data['class'],'data':[]}\n",
    "    for metric in metrics:\n",
    "        for item in data['data']:\n",
    "            data_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'retrieval_context':None,\n",
    "                'cached_metrics_data':[]\n",
    "            }\n",
    "            metrics_format_ins = {\n",
    "                'metric_metadata':{\n",
    "                    'metric':None,\n",
    "                    'success':True,\n",
    "                    'score':0.8,\n",
    "                    'reason':'',\n",
    "                    'statements':'',\n",
    "                    'verdicts':'',\n",
    "                    'evaluationCost': 0\n",
    "                },\n",
    "                'metric_configuration': {\n",
    "                    'threshold': 0.5,\n",
    "                    'evaluation_model': 'CustomLLM',\n",
    "                    'strict_mode': False,\n",
    "                    'include_reason': True\n",
    "                }\n",
    "            }\n",
    "            errors_format_ins = {\n",
    "                'id':0,\n",
    "                'AnswerModel':'',\n",
    "                'input':'',\n",
    "                'actual_output':'',\n",
    "                'expected_output':None,\n",
    "                'retrieval_context':None,\n",
    "                'cached_metrics_data':[\n",
    "                    {            \n",
    "                        'metric_metadata':{\n",
    "                            'metric':None,\n",
    "                        },\n",
    "                        'metric_configuration': {\n",
    "                            'threshold': 0.5,\n",
    "                            'evaluation_model': 'CustomLLM',\n",
    "                            'strict_mode': False,\n",
    "                            'include_reason': True\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            data_format_ins['id'] = item['id']\n",
    "            data_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "            data_format_ins['input'] = item['input']\n",
    "            data_format_ins['actual_output'] = item['actual_output']\n",
    "            data_format_ins['expected_output'] = item['expected_output']\n",
    "            tag = False\n",
    "            for x in save['data']:\n",
    "                if is_same_eval_item(item,x):\n",
    "                    for metric_data in x['cached_metrics_data']:\n",
    "                        if metric_data['metric_metadata']['metric'] == metric.__name__:\n",
    "                            if metric_data['metric_configuration']['threshold'] == metric.threshold and metric_data['metric_configuration']['evaluation_model'] == metric.evaluation_model and metric_data['metric_configuration']['strict_mode'] == metric.strict_mode and metric_data['metric_configuration']['include_reason'] == metric.include_reason:\n",
    "                                tag = True\n",
    "                                print(\"HAVE:\")\n",
    "                                print(x)\n",
    "                                break\n",
    "            if tag:\n",
    "                continue\n",
    "            test_case = LLMTestCase(\n",
    "                input= item['input'],\n",
    "                actual_output=item['actual_output'],\n",
    "                context=[item['expected_output']],\n",
    "            )\n",
    "            try:\n",
    "                metric.measure(test_case)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                path.append(e)\n",
    "                errors_format_ins['id'] = item['id']\n",
    "                errors_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "                errors_format_ins['input'] = item['input']\n",
    "                errors_format_ins['actual_output'] = item['actual_output']\n",
    "                errors_format_ins['expected_output'] = item['expected_output']\n",
    "                errors_format_ins['cached_metrics_data'][0]['metric_metadata']['metric'] = metric.__name__\n",
    "                errors_format_ins['cached_metrics_data'][0]['metric_configuration'] = {'threshold':metric.threshold,'evaluation_model':metric.evaluation_model,'strict_mode':metric.strict_mode,'include_reason':metric.include_reason}\n",
    "                error['data'].append(errors_format_ins.copy())\n",
    "                print(errors_format_ins)\n",
    "                with open(error_file,'w') as f:\n",
    "                    json.dump(error,f,indent=4)\n",
    "                continue\n",
    "            metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "            metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "            metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "            metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "            \n",
    "            metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "            metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "            metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "            metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "            metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "            metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "            metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "\n",
    "            data_format_ins['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "            flag = True\n",
    "            for each in save['data']:\n",
    "                if is_same_eval_item(each, data_format_ins):\n",
    "                    each['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                save['data'].append(data_format_ins.copy())\n",
    "                print(data_format_ins)\n",
    "            with open(save_file,'w') as f:\n",
    "                json.dump(save,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinEvalErrorToData(errorFile:Union[str,Path],saveFile:Union[str,Path])->None:\n",
    "    \"\"\"_summary_\n",
    "        the function is used to rerun the error item in the `errorFile` and append the results into the `saveFile` \n",
    "        \n",
    "    Args:\n",
    "        `errorFile` (Union[str,Path]): the JSON file saves the error item in the before running\n",
    "        `saveFile` (Union[str,Path]): the JSON file saves the pass result\n",
    "        \n",
    "    Returns:\n",
    "        None: the result will override the original file \n",
    "    \"\"\"\n",
    "    def is_same_eval_item(x,item):\n",
    "        if x['id'] == item['id'] and x['input'] == item['input'] and x['actual_output'] == item['actual_output'] and x['AnswerModel'] == item['AnswerModel'] and x['expected_output'] == item['expected_output']:\n",
    "            return True\n",
    "        return False\n",
    "    with open(errorFile,'r') as f:\n",
    "        data_er = json.load(f)\n",
    "    with open(saveFile,'r') as f:\n",
    "        data_sv = json.load(f)\n",
    "    if data_er['fileName'] != data_sv['fileName']:\n",
    "        print(\"The save_file does not match the error_file!\")\n",
    "        return\n",
    "    error = {'fileName' :data_er['fileName'],'class':data_er['class'],'data':[]}\n",
    "    while data_er['data']:\n",
    "        item = data_er['data'].pop()\n",
    "        data_format_ins = {\n",
    "            'id':0,\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'context':None,\n",
    "            'retrieval_context':None,\n",
    "            'cached_metrics_data':[]\n",
    "        }\n",
    "        metrics_format_ins = {\n",
    "            'metric_metadata':{\n",
    "                'metric':None,\n",
    "                'success':True,\n",
    "                'score':0.8,\n",
    "                'reason':'',\n",
    "                'statements':'',\n",
    "                'verdicts':'',\n",
    "                'evaluationCost': 0\n",
    "            },\n",
    "            'metric_configuration': {\n",
    "                'threshold': 0.5,\n",
    "                'evaluation_model': 'CustomLLM',\n",
    "                'strict_mode': False,\n",
    "                'include_reason': True\n",
    "            }\n",
    "        }\n",
    "        errors_format_ins = {\n",
    "            'id':0,\n",
    "            'AnswerModel':'',\n",
    "            'input':'',\n",
    "            'actual_output':'',\n",
    "            'expected_output':None,\n",
    "            'retrieval_context':None,\n",
    "            'cached_metrics_data':[\n",
    "                {            \n",
    "                    'metric_metadata':{\n",
    "                        'metric':None,\n",
    "                    },\n",
    "                    'metric_configuration': {\n",
    "                        'threshold': 0.5,\n",
    "                        'evaluation_model': 'CustomLLM',\n",
    "                        'strict_mode': False,\n",
    "                        'include_reason': True\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        data_format_ins['id'] = item['id']\n",
    "        data_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "        data_format_ins['input'] = item['input']\n",
    "        data_format_ins['actual_output'] = item['actual_output']\n",
    "        data_format_ins['expected_output'] = item['expected_output']\n",
    "        \n",
    "        tag = False\n",
    "        for x in data_sv['data']:\n",
    "            if is_same_eval_item(item,x):\n",
    "                for metric_data in x['cached_metrics_data']:\n",
    "                    if metric_data['metric_metadata']['metric'] == item['cached_metrics_data'][0]['metric_metadata']['metric']:\n",
    "                        if metric_data['metric_configuration']['threshold'] == item['cached_metrics_data'][0]['metric_configuration']['threshold'] and metric_data['metric_configuration']['evaluation_model'] == item['cached_metrics_data'][0]['metric_configuration']['evaluation_model'] and metric_data['metric_configuration']['strict_mode'] == item['cached_metrics_data'][0]['metric_configuration']['strict_mode'] and metric_data['metric_configuration']['include_reason'] == item['cached_metrics_data'][0]['metric_configuration']['include_reason']:\n",
    "                            tag = True\n",
    "                            print(\"HAVE:\")\n",
    "                            print(x)\n",
    "                            break\n",
    "        if tag:\n",
    "            continue\n",
    "        test_case = LLMTestCase(\n",
    "            input= item['input'],\n",
    "            actual_output=item['actual_output'],\n",
    "            context=[item['expected_output']],\n",
    "        )\n",
    "        if item['cached_metrics_data'][0]['metric_metadata']['metric'] == 'Answer Relevancy':\n",
    "            metric = AnswerRelevancyMetric(\n",
    "                threshold=item['cached_metrics_data'][0]['metric_configuration']['threshold'],\n",
    "                model = evaluateModel,\n",
    "                include_reason=item['cached_metrics_data'][0]['metric_configuration']['include_reason']\n",
    "            )\n",
    "        elif item['cached_metrics_data'][0]['metric_metadata']['metric'] == 'Hallucination':\n",
    "            metric = HallucinationMetric(\n",
    "                threshold=item['cached_metrics_data'][0]['metric_configuration']['threshold'],\n",
    "                model = evaluateModel,\n",
    "                include_reason=item['cached_metrics_data'][0]['metric_configuration']['include_reason']\n",
    "            )\n",
    "        else:\n",
    "            print(\"unkonwn metric!\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            metric.measure(test_case)\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc())\n",
    "            errors_format_ins['id'] = item['id']\n",
    "            errors_format_ins['AnswerModel'] = item['AnswerModel']\n",
    "            errors_format_ins['input'] = item['input']\n",
    "            errors_format_ins['actual_output'] = item['actual_output']\n",
    "            errors_format_ins['expected_output'] = item['expected_output']\n",
    "            errors_format_ins['cached_metrics_data'][0]['metric_metadata']['metric'] = metric.__name__\n",
    "            errors_format_ins['cached_metrics_data'][0]['metric_configuration'] = {'threshold':metric.threshold,'evaluation_model':metric.evaluation_model,'strict_mode':metric.strict_mode,'include_reason':metric.include_reason}\n",
    "            error['data'].append(errors_format_ins.copy())\n",
    "            print(errors_format_ins)\n",
    "            with open(errorFile,'w') as f:\n",
    "                json.dump(data_er,f,indent=4)\n",
    "            continue\n",
    "        metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "        metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "        metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "        metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "        \n",
    "        metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "        metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "        metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "        metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "        metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "        metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "        metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "\n",
    "        data_format_ins['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "        flag = True\n",
    "        for each in data_sv['data']:\n",
    "            if is_same_eval_item(each, data_format_ins):\n",
    "                each['cached_metrics_data'].append(metrics_format_ins.copy())\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            data_sv['data'].append(data_format_ins.copy())\n",
    "            print(data_format_ins)\n",
    "        with open(saveFile,'w') as f:\n",
    "            json.dump(data_sv,f,indent=4)\n",
    "    \n",
    "    with open(errorFile,'w') as f:\n",
    "        json.dump(error,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eval/error/2-5_DocRED_sample.json') as f:\n",
    "    tmp = json.load(f)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./eval/error/2-4_FewNERD++supervised_sample.json','r') as f:\n",
    "#     data = json.load(f)\n",
    "# for item in data['data']:\n",
    "#     item['actual_output'] = item['actual_output'].replace(\"[/INST]\",'')\n",
    "#     item['actual_output'] = item['actual_output'].replace(\"[INST]\",'')\n",
    "# with open('./eval/error/2-4_FewNERD++supervised_sample.json','w') as f:\n",
    "#     json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">504\n",
       "</pre>\n"
      ],
      "text/plain": [
       "504\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">504\n",
       "</pre>\n"
      ],
      "text/plain": [
       "504\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">504\n",
       "</pre>\n"
      ],
      "text/plain": [
       "504\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">504\n",
       "</pre>\n"
      ],
      "text/plain": [
       "504\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'id': 'with_triples_sample_0', 'AnswerModel': 'baichuan2-13b-chat', 'input': 'Complete the following generated texts and make sure to contain all the events provided.\\n### \\n#### Known Events\\n##### relation: uncovered\\n Speaker: Alston & Bird\\n Message: no facts to show that U . S . Soccer knew of the 1992 Incident when it hired Mr . Berhalter \\n Receiver: Gregg Berhalter\\n##### relation: harm\\n Agent: Gregg Berhalter\\n Cause: others \\n Location: United States\\n##### relation: injured\\n Agent: Gregg Berhalter\\n Cause: Rosalind\\n Location: the University of North Carolina \\n##### relation: preparing\\n Agent: The next coach \\n Content: the U . S . team for the 2026 World Cup \\n Upcoming activity: World Cup\\n Location: United States\\n##### relation: guided\\n Agent: Berhalter\\n Patient: United States\\n Method: men ’ s national team coach \\n##### relation: knew\\n Agent: USSF\\n Patient: the 1992 Incident \\n Location: United States\\n##### relation: show\\n Speaker: Alston & Bird\\n Message: similar incidents occurred at any point in the last 31 years \\n Location: United States\\n##### relation: complain\\n Agent: Rosalind\\n Patient: Gregg Berhalter\\n Reason: kicking her while they were dating \\n##### relation: hired\\n Employer: USSF\\n Patient: Gregg Berhalter\\n Job content: guided the United States for four years , leading a young squad to two regional championships and a place in the World Cup \\n Position: men ’ s national team coach \\n##### relation: show\\n Speaker: Alston & Bird\\n Message: the 1992 Incident has any nexus to the present or to the workplace \\n Location: United States\\n##### relation: begin\\n Agent: The next coach \\n Patient: preparing the U . S . team for the 2026 World Cup \\n Location: United States\\n##### relation: lost\\n Agent: United States\\n Patient: Netherlands\\n Content: group play \\n##### relation: report\\n Agent: Alston & Bird\\n Patient: Gregg Berhalter\\n Behavior: Berhalter had not committed any additional acts of violence against his now - wife since 1992 and hadn ’ t violated any disclosure rules during his hiring process \\n##### relation: occurred\\n Agent: similar incidents \\n##### relation: claim\\n Message: Mr . Berhalter presents a risk of harm to others \\n Location: United States\\n##### relation: told\\n Message Sender: Rosalind\\n Message Receiver: Alston & Bird\\n Message: she was not injured and didn ’ t complain to the university \\n Location: United States\\n##### relation: support\\n Agent: no facts \\n Patient: a claim \\n Location: United States\\n Content: Mr . Berhalter presents a risk of harm to others \\n##### relation: finished\\n Agent: Netherlands\\n Patient: United States\\n Symbol event: lost to the Netherlands in the round of 16 \\n##### relation: said\\n Speaker: Investigators \\n Message: uncovered no facts to show that U . S . Soccer knew of the 1992 Incident when it hired Mr . Berhalter \\n Details: no facts to show that similar incidents occurred at any point in the last 31 years \\n Location: United States\\n##### relation: championships\\n#### Generated Context\\n Details of a sordid rift between two prominent U . S . soccer families — one that included allegations of domestic abuse against men ’ s national team coach Gregg Berhalter and parental complaints about Gio Reyna ’ s playing time at the 2022 World Cup — continued to spill out Monday when the findings of an independent investigation were released .The three - month probe , conducted by the Alston & Bird law firm , concluded Berhalter had not committed any additional acts of violence against his now - wife since 1992 and hadn ’ t violated any disclosure rules during his hiring process .He had previously admitted to kicking her while they were dating as students at the University of North Carolina .With that finding , the U . S . Soccer Federation said Berhalter would remain a candidate to return as coach .His contract expired Dec . 31 .“ Given the investigators ’ conclusion that there is no legal impediment to employing him , Gregg Berhalter remains a candidate to serve as head coach ,” the USSF said in a statement .The governing body said it will not hire a coach until it appoints a new sporting director , a position that oversees all national teams .', 'actual_output': '### \\n#### Known Events\\n##### relation: uncovered\\n Speaker: Alston & Bird\\n Message: no facts to show that U . S . Soccer knew of the 1992 Incident when it hired Mr . Berhalter \\n Receiver: Gregg Berhalter\\n##### relation: harm\\n Agent: Gregg Berhalter\\n Cause: others \\n Location: United States\\n##### relation: injured\\n Agent: Gregg Berhalter\\n Cause: Rosalind\\n Location: the University of North Carolina \\n##### relation: preparing\\n Agent: The next coach \\n Content: the U . S . team for the 2026 World Cup \\n Upcoming activity: World Cup\\n Location: United States\\n##### relation: guided\\n Agent: Berhalter\\n Patient: United States\\n Method: men ’ s national team coach \\n##### relation: knew\\n Agent: USSF\\n Patient: the 1992 Incident \\n Location: United States\\n##### relation: show\\n Speaker: Alston & Bird\\n Message: similar incidents occurred at any point in the last 31 years \\n Location: United States\\n##### relation: complain\\n Agent: Rosalind\\n Patient: Gregg Berhalter\\n Reason: kicking her while they were dating \\n##### relation: hired\\n Employer: USSF\\n Patient: Gregg Berhalter\\n Job content: guided the United States for four years , leading a young squad to two regional championships and a place in the World Cup \\n Position: men ’ s national team coach \\n##### relation: show\\n Speaker: Alston & Bird\\n Message: the 1992 Incident has any nexus to the present or to the workplace \\n Location: United States\\n##### relation: begin\\n Agent: The next coach \\n Patient: preparing the U . S . team for the 2026 World Cup \\n Location: United States\\n##### relation: lost\\n Agent: United States\\n Patient: Netherlands\\n Content: group play \\n##### relation: report\\n Agent: Alston & Bird\\n Patient: Gregg Berhalter\\n Behavior: Berhalter had not committed any additional acts of violence against his now - wife since 1992 and hadn ’ t violated any disclosure rules during his hiring process \\n##### relation: occurred\\n Agent: similar incidents \\n##### relation: claim\\n Message: Mr . Berhalter presents a risk of harm to others \\n Location: United States\\n##### relation: told\\n Message Sender: Rosalind\\n Message Receiver: Alston & Bird\\n Message: she was not injured and didn ’ t complain to the university \\n Location: United States\\n##### relation: support\\n Agent: no facts \\n Patient: a claim \\n Location: United States\\n Content: Mr . Berhalter presents a risk of harm to others \\n##### relation: finished\\n Agent: Netherlands\\n Patient: United States\\n Symbol event: lost to the Netherlands in the round of 16 \\n##### relation: said\\n Speaker: Investigators \\n Message: uncovered no facts to show that U . S . Soccer knew of the 1992 Incident when it hired Mr . Berhalter \\n Details: no facts to show that similar incidents occurred at any point in the last 31 years \\n Location: United States\\n##### relation: championships\\n Details of a sordid rift between two prominent U . S . soccer families — one that included allegations of domestic abuse against men ’ s national team coach Gregg Berhalter and parental complaints about Gio Reyna ’ s playing time at the 2022 World Cup — continued to spill out Monday when the findings of an independent investigation were released .The three - month probe , conducted by the Alston & Bird law firm , concluded Berhalter had not committed any additional acts of violence against his now - wife since 1992 and hadn ’ t violated any disclosure rules during his hiring process .He had previously admitted to kicking her while they were dating as students at the University of North Carolina .With that finding , the U . S . Soccer Federation said Berhalter would remain a candidate to return as coach .His contract expired Dec . 31 .“ Given the investigators ’ conclusion that there is no legal impediment to employing him , Gregg Berhalter remains a candidate to serve as head coach ,” the USSF said in a statement .The governing body said it will not hire a coach until it appoints a new sporting director , a position that oversees all national teams . ', 'expected_output': 'Earnie Stewart left the job last month .Anthony Hudson , a World Cup assistant , is the interim coach .The next coach will begin preparing the U . S . team for the 2026 World Cup , which will take place in the United States , Mexico and Canada .Berhalter guided the United States for four years , leading a young squad to two regional championships and a place in the World Cup , where it finished second in group play and lost to the Netherlands in the round of 16 .Investigators said in the report that they “ uncovered no facts to show that U . S . Soccer knew of the 1992 Incident when it hired Mr . Berhalter ; no facts to show that similar incidents occurred at any point in the last 31 years ; no facts to show that the 1992 Incident has any nexus to the present or to the workplace ; and no facts to support a claim that Mr . Berhalter presents a risk of harm to others .” Berhalter ’ s wife , Rosalind , told investigators she was not injured and didn ’ t complain to the university .', 'retrieval_context': None, 'cached_metrics_data': [{'metric_metadata': {'metric': 'Answer Relevancy'}, 'metric_configuration': {'threshold': 0.5, 'evaluation_model': 'qwen1.5-72b-chat', 'strict_mode': False, 'include_reason': True}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\site-packages\\deepeval\\metrics\\utils.py\", line 57, in trimAndLoadJson\n",
      "    return json.loads(jsonStr)\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\游锦亮\\AppData\\Local\\Temp\\ipykernel_42008\\194931906.py\", line 104, in joinEvalErrorToData\n",
      "    metric.measure(test_case)\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py\", line 60, in measure\n",
      "    loop.run_until_complete(\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\asyncio\\futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\asyncio\\tasks.py\", line 256, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py\", line 87, in a_measure\n",
      "    self.statements: List[str] = await self._a_generate_statements(\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py\", line 189, in _a_generate_statements\n",
      "    data = trimAndLoadJson(res, self)\n",
      "  File \"d:\\Anaconda\\envs\\vllm\\lib\\site-packages\\deepeval\\metrics\\utils.py\", line 62, in trimAndLoadJson\n",
      "    raise ValueError(error_str)\n",
      "ValueError: Evaluation LLM outputted an invalid JSON. Please use a better evaluation model.\n"
     ]
    }
   ],
   "source": [
    "files_error = ['2-6_MAVEN_sample.json', '2-7_MAVEN-ERE_sample.json', '2-8_r_DocRED_sample.json', '3-1_hotpotqa_sample.json', '3-2_2wikimultihopqa_sample.json', '3-3_musique_sample.json', '3-4_kqapro_sample.json', '3-5_KoRC++ood_sample.json', '3-6_r_KoRC++ood_sample.json', '4-1_with_triples_sample.json', '4-2_r_with_triples_sample.json']\n",
    "for file in files_error:\n",
    "    joinEvalErrorToData('./eval/error/'+file,'./eval/save/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joinEvalErrorToData('./eval/error/2-5_DocRED_sample.json','./eval/save/2-5_DocRED_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(\"\\\"reason\\\":(.*?)\\}\",re.DOTALL)\n",
    "matchStr = regex.findall(path[1])\n",
    "# matchStr = regex.sub(lambda x:\"\\\"reason\\\":\\\"\"+x.group(0).strip().replace(\"\\\"reason\\\":\",\"\").strip()[1:-1].replace('\"','\\\\\\\"'),path[1])\n",
    "matchStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchStr[2].strip()[1:-1].replace('\"','\\\\\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['{\"verdicts\": [\\n        {\\n            \"verdict\": \"idk\"\\n        },\\n        {\\n            \"verdict\": \"no\",\\n            \"reason\": \"The statement \\'I am unable to provide an answer to your question\\' acknowledges the inability to answer but doesn\\'t provide any relevant information about the similarity with Rose.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'it is not clear what \"Rose\" refers to\\' is relevant as it points out the need for clarification to determine similarity.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'Could you please provide more context or information?\\' is relevant as it directly asks for additional details to establish the similarity with Rose.\"\\n        }\\n    ]}',\n",
    " '{\"verdicts\": [\\n        {\\n            \"verdict\": \"no\",\\n            \"reason\": \"The statement \\'I\\'m sorry,\\' does not provide any information or similarity comparison with Rose.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'I need more context to answer your question.\\' is relevant as it acknowledges the need for additional information to determine similarity with Rose.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'Could you please clarify what \"Rose\" refers to?\\' is relevant as it seeks to understand the specific context or entity named Rose for a proper comparison.\"\\n        },\\n        {\\n            \"verdict\": \"yes\",\\n            \"reason\": \"The statement \\'What you are looking for in terms of a conceptually similar entity?\\' is relevant as it asks for the criteria or aspects to consider when identifying a conceptually similar entity to Rose.\"\\n        }\\n    ]}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_answer_kilt(answers, text) -> bool:\n",
    "    text = normalize_kilt(text)\n",
    "    for single_answer in answers:\n",
    "        single_answer = normalize_kilt(single_answer)\n",
    "        if single_answer in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# answer normalization\n",
    "def normalize_kilt(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in fileNames:\n",
    "#     evaluate_file('./data/'+file,'./eval/save/'+file,'./eval/error/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eval/save/2-1_COPEN++csj_sample.json') as f:\n",
    "    data_view = json.load(f)\n",
    "noEvalItem = []\n",
    "for item in data_view['data']:\n",
    "    if len(item['cached_metrics_data']) <2:\n",
    "        noEvalItem.append(item)\n",
    "noEvalItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = noEvalItem[0]\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input= item['input'],\n",
    "    actual_output=item['actual_output'],\n",
    "    context=[item['expected_output']],\n",
    ")\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model = evaluateModel,\n",
    "    include_reason=True,\n",
    ")\n",
    "metric.measure(test_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in [noEvalItem[0]]:\n",
    "    metrics_format_ins = {\n",
    "        'metric_metadata':{\n",
    "            'metric':None,\n",
    "            'success':True,\n",
    "            'score':0.8,\n",
    "            'reason':'',\n",
    "            'statements':'',\n",
    "            'verdicts':'',\n",
    "            'evaluationCost': 0\n",
    "        },\n",
    "        'metric_configuration': {\n",
    "            'threshold': 0.5,\n",
    "            'evaluation_model': 'CustomLLM',\n",
    "            'strict_mode': False,\n",
    "            'include_reason': True\n",
    "        }\n",
    "    }\n",
    "    test_case = LLMTestCase(\n",
    "        input= item['input'],\n",
    "        actual_output=item['actual_output'],\n",
    "        context=[item['expected_output']],\n",
    "    )\n",
    "    metric = AnswerRelevancyMetric(\n",
    "        threshold=0.5,\n",
    "        model = evaluateModel,\n",
    "        include_reason=True,async_mode=False\n",
    "    )\n",
    "    metric.measure(test_case)\n",
    "    \n",
    "    metrics_format_ins['metric_metadata']['metric'] = metric.__name__\n",
    "    metrics_format_ins['metric_metadata']['score'] = metric.score\n",
    "    metrics_format_ins['metric_metadata']['success'] = metric.is_successful()\n",
    "    metrics_format_ins['metric_metadata']['reason'] = metric.reason\n",
    "    \n",
    "    metrics_format_ins['metric_metadata']['statements'] = getattr(metric,'statements','')\n",
    "    metrics_format_ins['metric_metadata']['verdicts'] = str(getattr(metric,'verdicts',''))\n",
    "    metrics_format_ins['metric_metadata']['evaluationCost'] = metric.evaluation_cost\n",
    "\n",
    "    metrics_format_ins['metric_configuration']['threshold'] = metric.threshold\n",
    "    metrics_format_ins['metric_configuration']['strict_mode'] = metric.strict_mode\n",
    "    metrics_format_ins['metric_configuration']['evaluation_model'] = metric.evaluation_model\n",
    "    metrics_format_ins['metric_configuration']['include_reason'] = metric.include_reason\n",
    "    \n",
    "    print(metrics_format_ins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATH PROCESS AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/math401/math50.json') as f:\n",
    "#     data = json.load(f)\n",
    "# data_new  = {'fileName': data['fileName'],'data':[]}\n",
    "# for item in data['data']:\n",
    "#     data_format = {'id':item['id'],'input':item['input'],'expected_output':item['expected_output']}\n",
    "#     data_new['data'].append(data_format.copy())\n",
    "# with open('./data/math401/math50.json','w') as f:\n",
    "#     json.dump(data_new,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/math401/math50.json','r') as f:\n",
    "    data  = json.load(f)\n",
    "data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/math401/save/infini-megrez-7b.json','r') as f:\n",
    "#     data_401 = json.load(f)\n",
    "#     data_401_new = {'fileName': data_401['fileName'],'data':[]}\n",
    "#     for item in data_401['data']:\n",
    "#         for x in data['data']:\n",
    "#             if item['input'] =='Only return the correct answer of the question.\\n'+ x['input']:\n",
    "#                 item['id'] = x['id']\n",
    "#                 item['is_correct'] = -1\n",
    "#                 data_401_new['data'].append(item.copy())\n",
    "# with open('./data/math401/save/infini-megrez-7b.json','w') as f:\n",
    "#     json.dump(data_401_new,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_result(filePath:Union[str,Path])->None:\n",
    "    \"\"\"\n",
    "    _summary_\n",
    "        the function is used to sort the results saved in the JSON `filepath` by `id` field\n",
    "        \n",
    "    Args:\n",
    "        `filePath` (Union[str,Path]): the JSON file path\n",
    "\n",
    "    Returns:\n",
    "        None: the result will override the original `filePath` \n",
    "    \"\"\"\n",
    "    with open(filePath,'r') as f:\n",
    "        data = json.load(f)\n",
    "    data['data'].sort(key = lambda x:x['id'])\n",
    "    with open(filePath,'w') as f:\n",
    "        json.dump(data,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addField(file:Union[str,Path],field:str,default:Any)->None:\n",
    "    \"\"\"\n",
    "    _summary_\n",
    "        the function is used to add a field into the `.json` `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the filename ,the file should be JSON file\n",
    "        `field` (str): the field name\n",
    "        `default` (Any): the default value of the added field,the value will add or subtract a Random from -500_000_000 to 500_000_000 if the field is `time`\n",
    "        \n",
    "    Returns:\n",
    "        None: the result will override the original `file` \n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if field not in item:\n",
    "            item[field] = default\n",
    "            if field == 'time':\n",
    "                item[field] += random.randint(-500_000_000,500_000_000)\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "    for file in files:\n",
    "        addField(os.path.join(dir,file),'extract_answer',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberAnswer(text:str)->Union[float,None]:\n",
    "    \"\"\"_summary_\n",
    "        the function is used to extract the number from the text\n",
    "\n",
    "    Args:\n",
    "        `text` (str): the text contains the number\n",
    "        \n",
    "    Returns:\n",
    "        Union[float,None]: the number in the `text`, or None if the `text` does not contain the number\n",
    "    \"\"\"\n",
    "    text = text.split('=')\n",
    "    if text:\n",
    "        text = text[-1]\n",
    "        regex = re.compile('([+-]?\\d+[,0-9]*[.]?[0-9]*)')\n",
    "        ret = regex.findall(text)\n",
    "        if ret:\n",
    "            return eval(ret[-1].replace(',',''))\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAnswer(file:Union[str,Path]):\n",
    "    \"\"\"\n",
    "    _summary_:\n",
    "        the function is used to process the answer that the LLM returned , namely extracting the number from the `actual_output` and save as `extract_answer`, and transform `expected_output` from str to number[int,float] \n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the filename of the file saving the answer created by the LLM\n",
    "        \n",
    "    Returns:\n",
    "        None : the result will be written into the original `file`\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data['data']:\n",
    "        if item['is_correct'] == -1:\n",
    "            item['extract_answer'] = getNumberAnswer(item['actual_output'])\n",
    "            item['expected_output'] = eval(item['expected_output'])\n",
    "            if item['extract_answer'] is None :\n",
    "                item['is_correct'] = 0\n",
    "            else:\n",
    "                item['is_correct'] =1 if abs(item['expected_output']-item['extract_answer']) < 1e-3 else 0\n",
    "    with open(file,'w') as f:\n",
    "        json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "    for file in files:\n",
    "        processAnswer(os.path.join(dir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "#     for file in files:\n",
    "#         with open(os.path.join(dir,file),'r') as f:\n",
    "#             data = json.load(f)\n",
    "#         for item in data['data']:\n",
    "\n",
    "#             if item['is_correct']:\n",
    "#                 item['is_correct'] = 1\n",
    "#             else:\n",
    "#                 item['is_correct'] = 0\n",
    "#         with open(os.path.join(dir,file),'w') as f:\n",
    "#             json.dump(data,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(file:Union[str,Path],return_tuple:bool = False)->Union[float,tuple[str,float]]:\n",
    "    \"\"\"_summary_\n",
    "        the function calculates the accuracy of the model answers on the `math50` dataset saved in `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        `return_tuple` (bool, optional): if the value is True,the function will return a tuple (modelName,accuracy),else only return accuracy `[0,1]`. \n",
    "                    Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[float,tuple[str,float]]: if return_tuple is True,return a tuple containing both modelName and accuracy in format `(modelName:str,accuracy:float)`,else only return the accuracy in [0,1]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    correct = 0\n",
    "    for item in data['data']:\n",
    "        correct += item['is_correct']\n",
    "    accuracy =  correct/len(data['data'])\n",
    "    if return_tuple:\n",
    "        return (data['model'],accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_nan_ratio(file:Union[str,Path],return_tuple:bool = False)->Union[float,tuple[str,float]]:\n",
    "    \"\"\"_summary_\n",
    "        the function calculates the no number ratio of the LLM answers on the `math50` dataset saved in `file`\n",
    "\n",
    "    Args:\n",
    "        `file` (Union[str,Path]): the path to the JSON `file` containing a model answer results\n",
    "        `return_tuple` (bool, optional): if the value is True,the function will return a tuple (modelName,nan_ratio),else only return nan_ratio `[0,1]`. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[float,tuple[str,float]]: if return_tuple is True,return a tuple containing both modelName and nan_ratio in format `(modelName:str,nan_ratio:float)`,else only return the nan_ratio in [0,1]\n",
    "    \"\"\"\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    nan = 0\n",
    "    for item in data['data']:\n",
    "        if item['extract_answer'] is None:\n",
    "            nan += 1\n",
    "    nan_ratio = nan/len(data['data'])\n",
    "    if return_tuple:\n",
    "        return (data['model'],nan_ratio)\n",
    "    return nan_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RE(y_true,y_pred):\n",
    "    return min(10,abs(y_true-y_pred)/max(abs(y_true),1))\n",
    "def calculate_RE(file:Union[str,Path],return_tuple:bool = False)->Union[List[float],tuple[str,List[float]]]:\n",
    "    with open(file,'r') as f:\n",
    "        data = json.load(f)\n",
    "    RE_list = []\n",
    "    for item in data['data']:\n",
    "        ret = 0\n",
    "        y_true = item['expected_output']\n",
    "        y_pred = item['extract_answer']\n",
    "        if y_pred is None:\n",
    "            ret = 10\n",
    "        else:\n",
    "            ret = RE(y_true,y_pred)\n",
    "        RE_list.append(ret)\n",
    "    if return_tuple:\n",
    "        return (data['model'],RE_list)\n",
    "    return RE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = {'eval':'math','eval_dataset':'math50.json','data':[]}\n",
    "# for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "#     for file in files:\n",
    "#         tmp = calculate_accuracy(os.path.join(dir,file),return_tuple=True)\n",
    "#         nan_ratio = calculate_nan_ratio(os.path.join(dir,file))\n",
    "#         RE_List = calculate_RE(os.path.join(dir,file))\n",
    "#         data_list['data'].append({'model':tmp[0],'Accuracy':tmp[1],'Nan_Ratio':nan_ratio,'RE_List':RE_List[:]})\n",
    "# with open('./data/math401/math50_eval_result.json','w') as f:\n",
    "#     json.dump(data_list,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# for dir,subdir ,files in os.walk('./data/math401/save'):\n",
    "#     for file in files:\n",
    "#         tmp = calculate_accuracy(os.path.join(dir,file),return_tuple=True)\n",
    "#         nan_ratio = calculate_nan_ratio(os.path.join(dir,file))\n",
    "#         RE_List = calculate_RE(os.path.join(dir,file))\n",
    "#         data[tmp[0]] = {'Accuracy':tmp[1],'Nan_Ratio':nan_ratio,'RE_List':RE_List[:]}\n",
    "# df = pd.DataFrame(data).T.sort_index()\n",
    "# df.to_json('./data/math401/math50_eval_result_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
